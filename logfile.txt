Script started on 2023-12-07 16:46:59+00:00 [TERM="xterm-256color" TTY="/dev/pts/0" COLUMNS="114" LINES="33"]
]0;paperspace@psybffhvzv5t: ~/Desktop[01;32mpaperspace@psybffhvzv5t[00m:[01;34m~/Desktop[00m$ AZFUSE_TSV_USE_FUSE=1 python -m generativeimage2text.inference -p "{'type': 'test_git_inference_single_image', \
> 'image_path': 'aux_data/images/1.jpg', \
> 'model_name': 'GIT_BASE', \
> 'prefix': '', \
> }"
2023-12-07 16:47:50,703.703 7917:inference.py:318   <module>(): param:
{'image_path': 'aux_data/images/1.jpg',
 'model_name': 'GIT_BASE',
 'prefix': '',
 'type': 'test_git_inference_single_image'}
2023-12-07 16:47:50,704.704 7917:common.py:234 limited_retry_agent(): fails with 
[Errno 2] No such file or directory: 'aux_data/configs/azfuse.yaml': tried 1/10-th time
Traceback (most recent call last):
  File "/home/paperspace/.local/lib/python3.9/site-packages/azfuse/common.py", line 232, in limited_retry_agent
    return func(*args, **kwargs)
FileNotFoundError: [Errno 2] No such file or directory: 'aux_data/configs/azfuse.yaml'
2023-12-07 16:47:52,824.824 7917:common.py:234 limited_retry_agent(): fails with 
[Errno 2] No such file or directory: 'aux_data/configs/azfuse.yaml': tried 2/10-th time
Traceback (most recent call last):
  File "/home/paperspace/.local/lib/python3.9/site-packages/azfuse/common.py", line 232, in limited_retry_agent
    return func(*args, **kwargs)
FileNotFoundError: [Errno 2] No such file or directory: 'aux_data/configs/azfuse.yaml'
2023-12-07 16:47:54,712.712 7917:common.py:234 limited_retry_agent(): fails with 
[Errno 2] No such file or directory: 'aux_data/configs/azfuse.yaml': tried 3/10-th time
Traceback (most recent call last):
  File "/home/paperspace/.local/lib/python3.9/site-packages/azfuse/common.py", line 232, in limited_retry_agent
    return func(*args, **kwargs)
FileNotFoundError: [Errno 2] No such file or directory: 'aux_data/configs/azfuse.yaml'
2023-12-07 16:47:55,640.640 7917:common.py:234 limited_retry_agent(): fails with 
[Errno 2] No such file or directory: 'aux_data/configs/azfuse.yaml': tried 4/10-th time
Traceback (most recent call last):
  File "/home/paperspace/.local/lib/python3.9/site-packages/azfuse/common.py", line 232, in limited_retry_agent
    return func(*args, **kwargs)
FileNotFoundError: [Errno 2] No such file or directory: 'aux_data/configs/azfuse.yaml'
2023-12-07 16:47:56,763.763 7917:common.py:234 limited_retry_agent(): fails with 
[Errno 2] No such file or directory: 'aux_data/configs/azfuse.yaml': tried 5/10-th time
Traceback (most recent call last):
  File "/home/paperspace/.local/lib/python3.9/site-packages/azfuse/common.py", line 232, in limited_retry_agent
    return func(*args, **kwargs)
FileNotFoundError: [Errno 2] No such file or directory: 'aux_data/configs/azfuse.yaml'
2023-12-07 16:48:00,071.071 7917:common.py:234 limited_retry_agent(): fails with 
[Errno 2] No such file or directory: 'aux_data/configs/azfuse.yaml': tried 6/10-th time
Traceback (most recent call last):
  File "/home/paperspace/.local/lib/python3.9/site-packages/azfuse/common.py", line 232, in limited_retry_agent
    return func(*args, **kwargs)
FileNotFoundError: [Errno 2] No such file or directory: 'aux_data/configs/azfuse.yaml'
2023-12-07 16:48:00,787.787 7917:common.py:234 limited_retry_agent(): fails with 
[Errno 2] No such file or directory: 'aux_data/configs/azfuse.yaml': tried 7/10-th time
Traceback (most recent call last):
  File "/home/paperspace/.local/lib/python3.9/site-packages/azfuse/common.py", line 232, in limited_retry_agent
    return func(*args, **kwargs)
FileNotFoundError: [Errno 2] No such file or directory: 'aux_data/configs/azfuse.yaml'
2023-12-07 16:48:02,816.816 7917:common.py:234 limited_retry_agent(): fails with 
[Errno 2] No such file or directory: 'aux_data/configs/azfuse.yaml': tried 8/10-th time
Traceback (most recent call last):
  File "/home/paperspace/.local/lib/python3.9/site-packages/azfuse/common.py", line 232, in limited_retry_agent
    return func(*args, **kwargs)
FileNotFoundError: [Errno 2] No such file or directory: 'aux_data/configs/azfuse.yaml'
2023-12-07 16:48:03,102.102 7917:common.py:234 limited_retry_agent(): fails with 
[Errno 2] No such file or directory: 'aux_data/configs/azfuse.yaml': tried 9/10-th time
Traceback (most recent call last):
  File "/home/paperspace/.local/lib/python3.9/site-packages/azfuse/common.py", line 232, in limited_retry_agent
    return func(*args, **kwargs)
FileNotFoundError: [Errno 2] No such file or directory: 'aux_data/configs/azfuse.yaml'
2023-12-07 16:48:04,698.698 7917:common.py:234 limited_retry_agent(): fails with 
[Errno 2] No such file or directory: 'aux_data/configs/azfuse.yaml': tried 10/10-th time
Traceback (most recent call last):
  File "/home/paperspace/.local/lib/python3.9/site-packages/azfuse/common.py", line 232, in limited_retry_agent
    return func(*args, **kwargs)
FileNotFoundError: [Errno 2] No such file or directory: 'aux_data/configs/azfuse.yaml'
Traceback (most recent call last):
  File "/usr/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/usr/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/paperspace/Desktop/GenerativeImage2Text/generativeimage2text/inference.py", line 321, in <module>
    locals()[function_name](**kwargs)
  File "/home/paperspace/Desktop/GenerativeImage2Text/generativeimage2text/inference.py", line 69, in test_git_inference_single_image
    if File.isfile(f'aux_data/models/{model_name}/parameter.yaml'):
  File "/home/paperspace/.local/lib/python3.9/site-packages/azfuse/azfuse.py", line 39, in isfile
    cls.ensure_initialized()
  File "/home/paperspace/.local/lib/python3.9/site-packages/azfuse/azfuse.py", line 25, in ensure_initialized
    cls.fuser = create_cloud_fuse()
  File "/home/paperspace/.local/lib/python3.9/site-packages/azfuse/cloud_storage.py", line 157, in create_cloud_fuse
    config = load_from_yaml_file(fname)
  File "/home/paperspace/.local/lib/python3.9/site-packages/azfuse/common.py", line 287, in load_from_yaml_file
    with exclusive_open_to_read(file_name, 'r') as fp:
  File "/home/paperspace/.local/lib/python3.9/site-packages/azfuse/common.py", line 140, in exclusive_open_to_read
    fp = limited_retry_agent(10, io.open, fname, mode)
  File "/home/paperspace/.local/lib/python3.9/site-packages/azfuse/common.py", line 232, in limited_retry_agent
    return func(*args, **kwargs)
FileNotFoundError: [Errno 2] No such file or directory: 'aux_data/configs/azfuse.yaml'
]0;paperspace@psybffhvzv5t: ~/Desktop[01;32mpaperspace@psybffhvzv5t[00m:[01;34m~/Desktop[00m$ cd GenerativeImage2Text/
]0;paperspace@psybffhvzv5t: ~/Desktop/GenerativeImage2Text[01;32mpaperspace@psybffhvzv5t[00m:[01;34m~/Desktop/GenerativeImage2Text[00m$ cd GenerativeImage2Text/[KAZFUSE_TSV_USE_FUSE=1 python -m generativeimage2text.inference -p "{'type': 'test_git_inference_single_image', \
> 'image_path': 'aux_data/images/1.jpg', \
> 'model_name': 'GIT_BASE', \
> 'prefix': '', \
> }"
2023-12-07 16:48:41,774.774 8155:inference.py:318   <module>(): param:
{'image_path': 'aux_data/images/1.jpg',
 'model_name': 'GIT_BASE',
 'prefix': '',
 'type': 'test_git_inference_single_image'}
/home/paperspace/Desktop/GenerativeImage2Text/generativeimage2text/inference.py:119: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.
  Resize(crop_size, interpolation=Image.BICUBIC),
/home/paperspace/.local/lib/python3.9/site-packages/torchvision/transforms/transforms.py:332: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.
  warnings.warn(
2023-12-07 16:48:49,583.583 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.class_embedding                                         will be loaded from image_encoder.class_embedding                                         of shape (768,)
2023-12-07 16:48:49,583.583 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.conv1.weight                                            will be loaded from image_encoder.conv1.weight                                            of shape (768, 3, 16, 16)
2023-12-07 16:48:49,583.583 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.ln_post.bias                                            will be loaded from image_encoder.ln_post.bias                                            of shape (768,)
2023-12-07 16:48:49,583.583 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.ln_post.weight                                          will be loaded from image_encoder.ln_post.weight                                          of shape (768,)
2023-12-07 16:48:49,583.583 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.ln_pre.bias                                             will be loaded from image_encoder.ln_pre.bias                                             of shape (768,)
2023-12-07 16:48:49,584.584 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.ln_pre.weight                                           will be loaded from image_encoder.ln_pre.weight                                           of shape (768,)
2023-12-07 16:48:49,584.584 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.positional_embedding                                    will be loaded from image_encoder.positional_embedding                                    of shape (197, 768)
2023-12-07 16:48:49,584.584 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.proj                                                    will be loaded from image_encoder.proj                                                    of shape (768, 512)
2023-12-07 16:48:49,584.584 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.0.attn.in_proj_bias               will be loaded from image_encoder.transformer.resblocks.0.attn.in_proj_bias               of shape (2304,)
2023-12-07 16:48:49,584.584 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.0.attn.in_proj_weight             will be loaded from image_encoder.transformer.resblocks.0.attn.in_proj_weight             of shape (2304, 768)
2023-12-07 16:48:49,584.584 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.0.attn.out_proj.bias              will be loaded from image_encoder.transformer.resblocks.0.attn.out_proj.bias              of shape (768,)
2023-12-07 16:48:49,584.584 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.0.attn.out_proj.weight            will be loaded from image_encoder.transformer.resblocks.0.attn.out_proj.weight            of shape (768, 768)
2023-12-07 16:48:49,584.584 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.0.ln_1.bias                       will be loaded from image_encoder.transformer.resblocks.0.ln_1.bias                       of shape (768,)
2023-12-07 16:48:49,584.584 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.0.ln_1.weight                     will be loaded from image_encoder.transformer.resblocks.0.ln_1.weight                     of shape (768,)
2023-12-07 16:48:49,584.584 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.0.ln_2.bias                       will be loaded from image_encoder.transformer.resblocks.0.ln_2.bias                       of shape (768,)
2023-12-07 16:48:49,584.584 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.0.ln_2.weight                     will be loaded from image_encoder.transformer.resblocks.0.ln_2.weight                     of shape (768,)
2023-12-07 16:48:49,584.584 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.0.mlp.c_fc.bias                   will be loaded from image_encoder.transformer.resblocks.0.mlp.c_fc.bias                   of shape (3072,)
2023-12-07 16:48:49,584.584 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.0.mlp.c_fc.weight                 will be loaded from image_encoder.transformer.resblocks.0.mlp.c_fc.weight                 of shape (3072, 768)
2023-12-07 16:48:49,584.584 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.0.mlp.c_proj.bias                 will be loaded from image_encoder.transformer.resblocks.0.mlp.c_proj.bias                 of shape (768,)
2023-12-07 16:48:49,584.584 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.0.mlp.c_proj.weight               will be loaded from image_encoder.transformer.resblocks.0.mlp.c_proj.weight               of shape (768, 3072)
2023-12-07 16:48:49,584.584 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.1.attn.in_proj_bias               will be loaded from image_encoder.transformer.resblocks.1.attn.in_proj_bias               of shape (2304,)
2023-12-07 16:48:49,584.584 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.1.attn.in_proj_weight             will be loaded from image_encoder.transformer.resblocks.1.attn.in_proj_weight             of shape (2304, 768)
2023-12-07 16:48:49,584.584 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.1.attn.out_proj.bias              will be loaded from image_encoder.transformer.resblocks.1.attn.out_proj.bias              of shape (768,)
2023-12-07 16:48:49,584.584 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.1.attn.out_proj.weight            will be loaded from image_encoder.transformer.resblocks.1.attn.out_proj.weight            of shape (768, 768)
2023-12-07 16:48:49,584.584 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.1.ln_1.bias                       will be loaded from image_encoder.transformer.resblocks.1.ln_1.bias                       of shape (768,)
2023-12-07 16:48:49,584.584 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.1.ln_1.weight                     will be loaded from image_encoder.transformer.resblocks.1.ln_1.weight                     of shape (768,)
2023-12-07 16:48:49,585.585 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.1.ln_2.bias                       will be loaded from image_encoder.transformer.resblocks.1.ln_2.bias                       of shape (768,)
2023-12-07 16:48:49,585.585 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.1.ln_2.weight                     will be loaded from image_encoder.transformer.resblocks.1.ln_2.weight                     of shape (768,)
2023-12-07 16:48:49,585.585 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.1.mlp.c_fc.bias                   will be loaded from image_encoder.transformer.resblocks.1.mlp.c_fc.bias                   of shape (3072,)
2023-12-07 16:48:49,585.585 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.1.mlp.c_fc.weight                 will be loaded from image_encoder.transformer.resblocks.1.mlp.c_fc.weight                 of shape (3072, 768)
2023-12-07 16:48:49,585.585 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.1.mlp.c_proj.bias                 will be loaded from image_encoder.transformer.resblocks.1.mlp.c_proj.bias                 of shape (768,)
2023-12-07 16:48:49,585.585 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.1.mlp.c_proj.weight               will be loaded from image_encoder.transformer.resblocks.1.mlp.c_proj.weight               of shape (768, 3072)
2023-12-07 16:48:49,585.585 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.10.attn.in_proj_bias              will be loaded from image_encoder.transformer.resblocks.10.attn.in_proj_bias              of shape (2304,)
2023-12-07 16:48:49,585.585 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.10.attn.in_proj_weight            will be loaded from image_encoder.transformer.resblocks.10.attn.in_proj_weight            of shape (2304, 768)
2023-12-07 16:48:49,585.585 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.10.attn.out_proj.bias             will be loaded from image_encoder.transformer.resblocks.10.attn.out_proj.bias             of shape (768,)
2023-12-07 16:48:49,585.585 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.10.attn.out_proj.weight           will be loaded from image_encoder.transformer.resblocks.10.attn.out_proj.weight           of shape (768, 768)
2023-12-07 16:48:49,585.585 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.10.ln_1.bias                      will be loaded from image_encoder.transformer.resblocks.10.ln_1.bias                      of shape (768,)
2023-12-07 16:48:49,585.585 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.10.ln_1.weight                    will be loaded from image_encoder.transformer.resblocks.10.ln_1.weight                    of shape (768,)
2023-12-07 16:48:49,585.585 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.10.ln_2.bias                      will be loaded from image_encoder.transformer.resblocks.10.ln_2.bias                      of shape (768,)
2023-12-07 16:48:49,585.585 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.10.ln_2.weight                    will be loaded from image_encoder.transformer.resblocks.10.ln_2.weight                    of shape (768,)
2023-12-07 16:48:49,585.585 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.10.mlp.c_fc.bias                  will be loaded from image_encoder.transformer.resblocks.10.mlp.c_fc.bias                  of shape (3072,)
2023-12-07 16:48:49,585.585 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.10.mlp.c_fc.weight                will be loaded from image_encoder.transformer.resblocks.10.mlp.c_fc.weight                of shape (3072, 768)
2023-12-07 16:48:49,585.585 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.10.mlp.c_proj.bias                will be loaded from image_encoder.transformer.resblocks.10.mlp.c_proj.bias                of shape (768,)
2023-12-07 16:48:49,585.585 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.10.mlp.c_proj.weight              will be loaded from image_encoder.transformer.resblocks.10.mlp.c_proj.weight              of shape (768, 3072)
2023-12-07 16:48:49,585.585 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.11.attn.in_proj_bias              will be loaded from image_encoder.transformer.resblocks.11.attn.in_proj_bias              of shape (2304,)
2023-12-07 16:48:49,585.585 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.11.attn.in_proj_weight            will be loaded from image_encoder.transformer.resblocks.11.attn.in_proj_weight            of shape (2304, 768)
2023-12-07 16:48:49,585.585 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.11.attn.out_proj.bias             will be loaded from image_encoder.transformer.resblocks.11.attn.out_proj.bias             of shape (768,)
2023-12-07 16:48:49,585.585 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.11.attn.out_proj.weight           will be loaded from image_encoder.transformer.resblocks.11.attn.out_proj.weight           of shape (768, 768)
2023-12-07 16:48:49,586.586 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.11.ln_1.bias                      will be loaded from image_encoder.transformer.resblocks.11.ln_1.bias                      of shape (768,)
2023-12-07 16:48:49,586.586 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.11.ln_1.weight                    will be loaded from image_encoder.transformer.resblocks.11.ln_1.weight                    of shape (768,)
2023-12-07 16:48:49,586.586 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.11.ln_2.bias                      will be loaded from image_encoder.transformer.resblocks.11.ln_2.bias                      of shape (768,)
2023-12-07 16:48:49,586.586 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.11.ln_2.weight                    will be loaded from image_encoder.transformer.resblocks.11.ln_2.weight                    of shape (768,)
2023-12-07 16:48:49,586.586 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.11.mlp.c_fc.bias                  will be loaded from image_encoder.transformer.resblocks.11.mlp.c_fc.bias                  of shape (3072,)
2023-12-07 16:48:49,586.586 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.11.mlp.c_fc.weight                will be loaded from image_encoder.transformer.resblocks.11.mlp.c_fc.weight                of shape (3072, 768)
2023-12-07 16:48:49,586.586 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.11.mlp.c_proj.bias                will be loaded from image_encoder.transformer.resblocks.11.mlp.c_proj.bias                of shape (768,)
2023-12-07 16:48:49,586.586 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.11.mlp.c_proj.weight              will be loaded from image_encoder.transformer.resblocks.11.mlp.c_proj.weight              of shape (768, 3072)
2023-12-07 16:48:49,586.586 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.2.attn.in_proj_bias               will be loaded from image_encoder.transformer.resblocks.2.attn.in_proj_bias               of shape (2304,)
2023-12-07 16:48:49,586.586 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.2.attn.in_proj_weight             will be loaded from image_encoder.transformer.resblocks.2.attn.in_proj_weight             of shape (2304, 768)
2023-12-07 16:48:49,586.586 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.2.attn.out_proj.bias              will be loaded from image_encoder.transformer.resblocks.2.attn.out_proj.bias              of shape (768,)
2023-12-07 16:48:49,586.586 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.2.attn.out_proj.weight            will be loaded from image_encoder.transformer.resblocks.2.attn.out_proj.weight            of shape (768, 768)
2023-12-07 16:48:49,586.586 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.2.ln_1.bias                       will be loaded from image_encoder.transformer.resblocks.2.ln_1.bias                       of shape (768,)
2023-12-07 16:48:49,586.586 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.2.ln_1.weight                     will be loaded from image_encoder.transformer.resblocks.2.ln_1.weight                     of shape (768,)
2023-12-07 16:48:49,586.586 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.2.ln_2.bias                       will be loaded from image_encoder.transformer.resblocks.2.ln_2.bias                       of shape (768,)
2023-12-07 16:48:49,586.586 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.2.ln_2.weight                     will be loaded from image_encoder.transformer.resblocks.2.ln_2.weight                     of shape (768,)
2023-12-07 16:48:49,586.586 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.2.mlp.c_fc.bias                   will be loaded from image_encoder.transformer.resblocks.2.mlp.c_fc.bias                   of shape (3072,)
2023-12-07 16:48:49,586.586 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.2.mlp.c_fc.weight                 will be loaded from image_encoder.transformer.resblocks.2.mlp.c_fc.weight                 of shape (3072, 768)
2023-12-07 16:48:49,586.586 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.2.mlp.c_proj.bias                 will be loaded from image_encoder.transformer.resblocks.2.mlp.c_proj.bias                 of shape (768,)
2023-12-07 16:48:49,586.586 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.2.mlp.c_proj.weight               will be loaded from image_encoder.transformer.resblocks.2.mlp.c_proj.weight               of shape (768, 3072)
2023-12-07 16:48:49,586.586 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.3.attn.in_proj_bias               will be loaded from image_encoder.transformer.resblocks.3.attn.in_proj_bias               of shape (2304,)
2023-12-07 16:48:49,587.587 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.3.attn.in_proj_weight             will be loaded from image_encoder.transformer.resblocks.3.attn.in_proj_weight             of shape (2304, 768)
2023-12-07 16:48:49,587.587 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.3.attn.out_proj.bias              will be loaded from image_encoder.transformer.resblocks.3.attn.out_proj.bias              of shape (768,)
2023-12-07 16:48:49,587.587 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.3.attn.out_proj.weight            will be loaded from image_encoder.transformer.resblocks.3.attn.out_proj.weight            of shape (768, 768)
2023-12-07 16:48:49,587.587 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.3.ln_1.bias                       will be loaded from image_encoder.transformer.resblocks.3.ln_1.bias                       of shape (768,)
2023-12-07 16:48:49,587.587 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.3.ln_1.weight                     will be loaded from image_encoder.transformer.resblocks.3.ln_1.weight                     of shape (768,)
2023-12-07 16:48:49,587.587 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.3.ln_2.bias                       will be loaded from image_encoder.transformer.resblocks.3.ln_2.bias                       of shape (768,)
2023-12-07 16:48:49,587.587 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.3.ln_2.weight                     will be loaded from image_encoder.transformer.resblocks.3.ln_2.weight                     of shape (768,)
2023-12-07 16:48:49,587.587 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.3.mlp.c_fc.bias                   will be loaded from image_encoder.transformer.resblocks.3.mlp.c_fc.bias                   of shape (3072,)
2023-12-07 16:48:49,587.587 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.3.mlp.c_fc.weight                 will be loaded from image_encoder.transformer.resblocks.3.mlp.c_fc.weight                 of shape (3072, 768)
2023-12-07 16:48:49,587.587 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.3.mlp.c_proj.bias                 will be loaded from image_encoder.transformer.resblocks.3.mlp.c_proj.bias                 of shape (768,)
2023-12-07 16:48:49,587.587 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.3.mlp.c_proj.weight               will be loaded from image_encoder.transformer.resblocks.3.mlp.c_proj.weight               of shape (768, 3072)
2023-12-07 16:48:49,587.587 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.4.attn.in_proj_bias               will be loaded from image_encoder.transformer.resblocks.4.attn.in_proj_bias               of shape (2304,)
2023-12-07 16:48:49,587.587 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.4.attn.in_proj_weight             will be loaded from image_encoder.transformer.resblocks.4.attn.in_proj_weight             of shape (2304, 768)
2023-12-07 16:48:49,587.587 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.4.attn.out_proj.bias              will be loaded from image_encoder.transformer.resblocks.4.attn.out_proj.bias              of shape (768,)
2023-12-07 16:48:49,587.587 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.4.attn.out_proj.weight            will be loaded from image_encoder.transformer.resblocks.4.attn.out_proj.weight            of shape (768, 768)
2023-12-07 16:48:49,587.587 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.4.ln_1.bias                       will be loaded from image_encoder.transformer.resblocks.4.ln_1.bias                       of shape (768,)
2023-12-07 16:48:49,587.587 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.4.ln_1.weight                     will be loaded from image_encoder.transformer.resblocks.4.ln_1.weight                     of shape (768,)
2023-12-07 16:48:49,587.587 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.4.ln_2.bias                       will be loaded from image_encoder.transformer.resblocks.4.ln_2.bias                       of shape (768,)
2023-12-07 16:48:49,587.587 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.4.ln_2.weight                     will be loaded from image_encoder.transformer.resblocks.4.ln_2.weight                     of shape (768,)
2023-12-07 16:48:49,587.587 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.4.mlp.c_fc.bias                   will be loaded from image_encoder.transformer.resblocks.4.mlp.c_fc.bias                   of shape (3072,)
2023-12-07 16:48:49,587.587 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.4.mlp.c_fc.weight                 will be loaded from image_encoder.transformer.resblocks.4.mlp.c_fc.weight                 of shape (3072, 768)
2023-12-07 16:48:49,587.587 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.4.mlp.c_proj.bias                 will be loaded from image_encoder.transformer.resblocks.4.mlp.c_proj.bias                 of shape (768,)
2023-12-07 16:48:49,587.587 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.4.mlp.c_proj.weight               will be loaded from image_encoder.transformer.resblocks.4.mlp.c_proj.weight               of shape (768, 3072)
2023-12-07 16:48:49,587.587 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.5.attn.in_proj_bias               will be loaded from image_encoder.transformer.resblocks.5.attn.in_proj_bias               of shape (2304,)
2023-12-07 16:48:49,588.588 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.5.attn.in_proj_weight             will be loaded from image_encoder.transformer.resblocks.5.attn.in_proj_weight             of shape (2304, 768)
2023-12-07 16:48:49,588.588 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.5.attn.out_proj.bias              will be loaded from image_encoder.transformer.resblocks.5.attn.out_proj.bias              of shape (768,)
2023-12-07 16:48:49,588.588 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.5.attn.out_proj.weight            will be loaded from image_encoder.transformer.resblocks.5.attn.out_proj.weight            of shape (768, 768)
2023-12-07 16:48:49,588.588 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.5.ln_1.bias                       will be loaded from image_encoder.transformer.resblocks.5.ln_1.bias                       of shape (768,)
2023-12-07 16:48:49,588.588 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.5.ln_1.weight                     will be loaded from image_encoder.transformer.resblocks.5.ln_1.weight                     of shape (768,)
2023-12-07 16:48:49,588.588 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.5.ln_2.bias                       will be loaded from image_encoder.transformer.resblocks.5.ln_2.bias                       of shape (768,)
2023-12-07 16:48:49,588.588 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.5.ln_2.weight                     will be loaded from image_encoder.transformer.resblocks.5.ln_2.weight                     of shape (768,)
2023-12-07 16:48:49,588.588 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.5.mlp.c_fc.bias                   will be loaded from image_encoder.transformer.resblocks.5.mlp.c_fc.bias                   of shape (3072,)
2023-12-07 16:48:49,588.588 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.5.mlp.c_fc.weight                 will be loaded from image_encoder.transformer.resblocks.5.mlp.c_fc.weight                 of shape (3072, 768)
2023-12-07 16:48:49,588.588 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.5.mlp.c_proj.bias                 will be loaded from image_encoder.transformer.resblocks.5.mlp.c_proj.bias                 of shape (768,)
2023-12-07 16:48:49,588.588 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.5.mlp.c_proj.weight               will be loaded from image_encoder.transformer.resblocks.5.mlp.c_proj.weight               of shape (768, 3072)
2023-12-07 16:48:49,588.588 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.6.attn.in_proj_bias               will be loaded from image_encoder.transformer.resblocks.6.attn.in_proj_bias               of shape (2304,)
2023-12-07 16:48:49,588.588 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.6.attn.in_proj_weight             will be loaded from image_encoder.transformer.resblocks.6.attn.in_proj_weight             of shape (2304, 768)
2023-12-07 16:48:49,588.588 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.6.attn.out_proj.bias              will be loaded from image_encoder.transformer.resblocks.6.attn.out_proj.bias              of shape (768,)
2023-12-07 16:48:49,588.588 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.6.attn.out_proj.weight            will be loaded from image_encoder.transformer.resblocks.6.attn.out_proj.weight            of shape (768, 768)
2023-12-07 16:48:49,588.588 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.6.ln_1.bias                       will be loaded from image_encoder.transformer.resblocks.6.ln_1.bias                       of shape (768,)
2023-12-07 16:48:49,588.588 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.6.ln_1.weight                     will be loaded from image_encoder.transformer.resblocks.6.ln_1.weight                     of shape (768,)
2023-12-07 16:48:49,588.588 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.6.ln_2.bias                       will be loaded from image_encoder.transformer.resblocks.6.ln_2.bias                       of shape (768,)
2023-12-07 16:48:49,588.588 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.6.ln_2.weight                     will be loaded from image_encoder.transformer.resblocks.6.ln_2.weight                     of shape (768,)
2023-12-07 16:48:49,588.588 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.6.mlp.c_fc.bias                   will be loaded from image_encoder.transformer.resblocks.6.mlp.c_fc.bias                   of shape (3072,)
2023-12-07 16:48:49,588.588 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.6.mlp.c_fc.weight                 will be loaded from image_encoder.transformer.resblocks.6.mlp.c_fc.weight                 of shape (3072, 768)
2023-12-07 16:48:49,588.588 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.6.mlp.c_proj.bias                 will be loaded from image_encoder.transformer.resblocks.6.mlp.c_proj.bias                 of shape (768,)
2023-12-07 16:48:49,588.588 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.6.mlp.c_proj.weight               will be loaded from image_encoder.transformer.resblocks.6.mlp.c_proj.weight               of shape (768, 3072)
2023-12-07 16:48:49,588.588 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.7.attn.in_proj_bias               will be loaded from image_encoder.transformer.resblocks.7.attn.in_proj_bias               of shape (2304,)
2023-12-07 16:48:49,589.589 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.7.attn.in_proj_weight             will be loaded from image_encoder.transformer.resblocks.7.attn.in_proj_weight             of shape (2304, 768)
2023-12-07 16:48:49,589.589 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.7.attn.out_proj.bias              will be loaded from image_encoder.transformer.resblocks.7.attn.out_proj.bias              of shape (768,)
2023-12-07 16:48:49,589.589 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.7.attn.out_proj.weight            will be loaded from image_encoder.transformer.resblocks.7.attn.out_proj.weight            of shape (768, 768)
2023-12-07 16:48:49,589.589 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.7.ln_1.bias                       will be loaded from image_encoder.transformer.resblocks.7.ln_1.bias                       of shape (768,)
2023-12-07 16:48:49,589.589 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.7.ln_1.weight                     will be loaded from image_encoder.transformer.resblocks.7.ln_1.weight                     of shape (768,)
2023-12-07 16:48:49,589.589 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.7.ln_2.bias                       will be loaded from image_encoder.transformer.resblocks.7.ln_2.bias                       of shape (768,)
2023-12-07 16:48:49,589.589 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.7.ln_2.weight                     will be loaded from image_encoder.transformer.resblocks.7.ln_2.weight                     of shape (768,)
2023-12-07 16:48:49,589.589 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.7.mlp.c_fc.bias                   will be loaded from image_encoder.transformer.resblocks.7.mlp.c_fc.bias                   of shape (3072,)
2023-12-07 16:48:49,589.589 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.7.mlp.c_fc.weight                 will be loaded from image_encoder.transformer.resblocks.7.mlp.c_fc.weight                 of shape (3072, 768)
2023-12-07 16:48:49,589.589 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.7.mlp.c_proj.bias                 will be loaded from image_encoder.transformer.resblocks.7.mlp.c_proj.bias                 of shape (768,)
2023-12-07 16:48:49,589.589 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.7.mlp.c_proj.weight               will be loaded from image_encoder.transformer.resblocks.7.mlp.c_proj.weight               of shape (768, 3072)
2023-12-07 16:48:49,589.589 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.8.attn.in_proj_bias               will be loaded from image_encoder.transformer.resblocks.8.attn.in_proj_bias               of shape (2304,)
2023-12-07 16:48:49,589.589 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.8.attn.in_proj_weight             will be loaded from image_encoder.transformer.resblocks.8.attn.in_proj_weight             of shape (2304, 768)
2023-12-07 16:48:49,589.589 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.8.attn.out_proj.bias              will be loaded from image_encoder.transformer.resblocks.8.attn.out_proj.bias              of shape (768,)
2023-12-07 16:48:49,589.589 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.8.attn.out_proj.weight            will be loaded from image_encoder.transformer.resblocks.8.attn.out_proj.weight            of shape (768, 768)
2023-12-07 16:48:49,589.589 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.8.ln_1.bias                       will be loaded from image_encoder.transformer.resblocks.8.ln_1.bias                       of shape (768,)
2023-12-07 16:48:49,589.589 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.8.ln_1.weight                     will be loaded from image_encoder.transformer.resblocks.8.ln_1.weight                     of shape (768,)
2023-12-07 16:48:49,589.589 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.8.ln_2.bias                       will be loaded from image_encoder.transformer.resblocks.8.ln_2.bias                       of shape (768,)
2023-12-07 16:48:49,589.589 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.8.ln_2.weight                     will be loaded from image_encoder.transformer.resblocks.8.ln_2.weight                     of shape (768,)
2023-12-07 16:48:49,589.589 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.8.mlp.c_fc.bias                   will be loaded from image_encoder.transformer.resblocks.8.mlp.c_fc.bias                   of shape (3072,)
2023-12-07 16:48:49,589.589 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.8.mlp.c_fc.weight                 will be loaded from image_encoder.transformer.resblocks.8.mlp.c_fc.weight                 of shape (3072, 768)
2023-12-07 16:48:49,589.589 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.8.mlp.c_proj.bias                 will be loaded from image_encoder.transformer.resblocks.8.mlp.c_proj.bias                 of shape (768,)
2023-12-07 16:48:49,589.589 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.8.mlp.c_proj.weight               will be loaded from image_encoder.transformer.resblocks.8.mlp.c_proj.weight               of shape (768, 3072)
2023-12-07 16:48:49,589.589 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.9.attn.in_proj_bias               will be loaded from image_encoder.transformer.resblocks.9.attn.in_proj_bias               of shape (2304,)
2023-12-07 16:48:49,590.590 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.9.attn.in_proj_weight             will be loaded from image_encoder.transformer.resblocks.9.attn.in_proj_weight             of shape (2304, 768)
2023-12-07 16:48:49,590.590 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.9.attn.out_proj.bias              will be loaded from image_encoder.transformer.resblocks.9.attn.out_proj.bias              of shape (768,)
2023-12-07 16:48:49,590.590 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.9.attn.out_proj.weight            will be loaded from image_encoder.transformer.resblocks.9.attn.out_proj.weight            of shape (768, 768)
2023-12-07 16:48:49,590.590 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.9.ln_1.bias                       will be loaded from image_encoder.transformer.resblocks.9.ln_1.bias                       of shape (768,)
2023-12-07 16:48:49,590.590 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.9.ln_1.weight                     will be loaded from image_encoder.transformer.resblocks.9.ln_1.weight                     of shape (768,)
2023-12-07 16:48:49,590.590 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.9.ln_2.bias                       will be loaded from image_encoder.transformer.resblocks.9.ln_2.bias                       of shape (768,)
2023-12-07 16:48:49,590.590 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.9.ln_2.weight                     will be loaded from image_encoder.transformer.resblocks.9.ln_2.weight                     of shape (768,)
2023-12-07 16:48:49,590.590 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.9.mlp.c_fc.bias                   will be loaded from image_encoder.transformer.resblocks.9.mlp.c_fc.bias                   of shape (3072,)
2023-12-07 16:48:49,590.590 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.9.mlp.c_fc.weight                 will be loaded from image_encoder.transformer.resblocks.9.mlp.c_fc.weight                 of shape (3072, 768)
2023-12-07 16:48:49,590.590 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.9.mlp.c_proj.bias                 will be loaded from image_encoder.transformer.resblocks.9.mlp.c_proj.bias                 of shape (768,)
2023-12-07 16:48:49,590.590 8155:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.9.mlp.c_proj.weight               will be loaded from image_encoder.transformer.resblocks.9.mlp.c_proj.weight               of shape (768, 3072)
2023-12-07 16:48:49,590.590 8155:torch_common.py:128 align_and_update_state_dicts(): textual.embedding.layer_norm.bias                                     will be loaded from textual.embedding.layer_norm.bias                                     of shape (768,)
2023-12-07 16:48:49,590.590 8155:torch_common.py:128 align_and_update_state_dicts(): textual.embedding.layer_norm.weight                                   will be loaded from textual.embedding.layer_norm.weight                                   of shape (768,)
2023-12-07 16:48:49,590.590 8155:torch_common.py:128 align_and_update_state_dicts(): textual.embedding.positions.weight                                    will be loaded from textual.embedding.positions.weight                                    of shape (1024, 768)
2023-12-07 16:48:49,590.590 8155:torch_common.py:128 align_and_update_state_dicts(): textual.embedding.words.weight                                        will be loaded from textual.embedding.words.weight                                        of shape (30522, 768)
2023-12-07 16:48:49,590.590 8155:torch_common.py:128 align_and_update_state_dicts(): textual.output.bias                                                   will be loaded from textual.output.bias                                                   of shape (30522,)
2023-12-07 16:48:49,590.590 8155:torch_common.py:128 align_and_update_state_dicts(): textual.output.weight                                                 will be loaded from textual.output.weight                                                 of shape (30522, 768)
2023-12-07 16:48:49,590.590 8155:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.attention.output.LayerNorm.bias   will be loaded from textual.transformer.encoder.layer.0.attention.output.LayerNorm.bias   of shape (768,)
2023-12-07 16:48:49,590.590 8155:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.attention.output.LayerNorm.weight will be loaded from textual.transformer.encoder.layer.0.attention.output.LayerNorm.weight of shape (768,)
2023-12-07 16:48:49,590.590 8155:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.attention.output.dense.bias       will be loaded from textual.transformer.encoder.layer.0.attention.output.dense.bias       of shape (768,)
2023-12-07 16:48:49,590.590 8155:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.attention.output.dense.weight     will be loaded from textual.transformer.encoder.layer.0.attention.output.dense.weight     of shape (768, 768)
2023-12-07 16:48:49,590.590 8155:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.attention.self.key.bias           will be loaded from textual.transformer.encoder.layer.0.attention.self.key.bias           of shape (768,)
2023-12-07 16:48:49,590.590 8155:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.attention.self.key.weight         will be loaded from textual.transformer.encoder.layer.0.attention.self.key.weight         of shape (768, 768)
2023-12-07 16:48:49,591.591 8155:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.attention.self.query.bias         will be loaded from textual.transformer.encoder.layer.0.attention.self.query.bias         of shape (768,)
2023-12-07 16:48:49,591.591 8155:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.attention.self.query.weight       will be loaded from textual.transformer.encoder.layer.0.attention.self.query.weight       of shape (768, 768)
2023-12-07 16:48:49,591.591 8155:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.attention.self.value.bias         will be loaded from textual.transformer.encoder.layer.0.attention.self.value.bias         of shape (768,)
2023-12-07 16:48:49,591.591 8155:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.attention.self.value.weight       will be loaded from textual.transformer.encoder.layer.0.attention.self.value.weight       of shape (768, 768)
2023-12-07 16:48:49,591.591 8155:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.intermediate.dense.bias           will be loaded from textual.transformer.encoder.layer.0.intermediate.dense.bias           of shape (3072,)
2023-12-07 16:48:49,591.591 8155:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.intermediate.dense.weight         will be loaded from textual.transformer.encoder.layer.0.intermediate.dense.weight         of shape (3072, 768)
2023-12-07 16:48:49,591.591 8155:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.output.LayerNorm.bias             will be loaded from textual.transformer.encoder.layer.0.output.LayerNorm.bias             of shape (768,)
2023-12-07 16:48:49,591.591 8155:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.output.LayerNorm.weight           will be loaded from textual.transformer.encoder.layer.0.output.LayerNorm.weight           of shape (768,)
2023-12-07 16:48:49,591.591 8155:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.output.dense.bias                 will be loaded from textual.transformer.encoder.layer.0.output.dense.bias                 of shape (768,)
2023-12-07 16:48:49,591.591 8155:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.output.dense.weight               will be loaded from textual.transformer.encoder.layer.0.output.dense.weight               of shape (768, 3072)
2023-12-07 16:48:49,591.591 8155:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.attention.output.LayerNorm.bias   will be loaded from textual.transformer.encoder.layer.1.attention.output.LayerNorm.bias   of shape (768,)
2023-12-07 16:48:49,591.591 8155:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.attention.output.LayerNorm.weight will be loaded from textual.transformer.encoder.layer.1.attention.output.LayerNorm.weight of shape (768,)
2023-12-07 16:48:49,591.591 8155:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.attention.output.dense.bias       will be loaded from textual.transformer.encoder.layer.1.attention.output.dense.bias       of shape (768,)
2023-12-07 16:48:49,591.591 8155:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.attention.output.dense.weight     will be loaded from textual.transformer.encoder.layer.1.attention.output.dense.weight     of shape (768, 768)
2023-12-07 16:48:49,591.591 8155:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.attention.self.key.bias           will be loaded from textual.transformer.encoder.layer.1.attention.self.key.bias           of shape (768,)
2023-12-07 16:48:49,591.591 8155:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.attention.self.key.weight         will be loaded from textual.transformer.encoder.layer.1.attention.self.key.weight         of shape (768, 768)
2023-12-07 16:48:49,591.591 8155:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.attention.self.query.bias         will be loaded from textual.transformer.encoder.layer.1.attention.self.query.bias         of shape (768,)
2023-12-07 16:48:49,591.591 8155:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.attention.self.query.weight       will be loaded from textual.transformer.encoder.layer.1.attention.self.query.weight       of shape (768, 768)
2023-12-07 16:48:49,591.591 8155:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.attention.self.value.bias         will be loaded from textual.transformer.encoder.layer.1.attention.self.value.bias         of shape (768,)
2023-12-07 16:48:49,591.591 8155:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.attention.self.value.weight       will be loaded from textual.transformer.encoder.layer.1.attention.self.value.weight       of shape (768, 768)
2023-12-07 16:48:49,591.591 8155:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.intermediate.dense.bias           will be loaded from textual.transformer.encoder.layer.1.intermediate.dense.bias           of shape (3072,)
2023-12-07 16:48:49,591.591 8155:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.intermediate.dense.weight         will be loaded from textual.transformer.encoder.layer.1.intermediate.dense.weight         of shape (3072, 768)
2023-12-07 16:48:49,591.591 8155:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.output.LayerNorm.bias             will be loaded from textual.transformer.encoder.layer.1.output.LayerNorm.bias             of shape (768,)
2023-12-07 16:48:49,592.592 8155:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.output.LayerNorm.weight           will be loaded from textual.transformer.encoder.layer.1.output.LayerNorm.weight           of shape (768,)
2023-12-07 16:48:49,592.592 8155:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.output.dense.bias                 will be loaded from textual.transformer.encoder.layer.1.output.dense.bias                 of shape (768,)
2023-12-07 16:48:49,592.592 8155:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.output.dense.weight               will be loaded from textual.transformer.encoder.layer.1.output.dense.weight               of shape (768, 3072)
2023-12-07 16:48:49,592.592 8155:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.attention.output.LayerNorm.bias   will be loaded from textual.transformer.encoder.layer.2.attention.output.LayerNorm.bias   of shape (768,)
2023-12-07 16:48:49,592.592 8155:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.attention.output.LayerNorm.weight will be loaded from textual.transformer.encoder.layer.2.attention.output.LayerNorm.weight of shape (768,)
2023-12-07 16:48:49,592.592 8155:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.attention.output.dense.bias       will be loaded from textual.transformer.encoder.layer.2.attention.output.dense.bias       of shape (768,)
2023-12-07 16:48:49,592.592 8155:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.attention.output.dense.weight     will be loaded from textual.transformer.encoder.layer.2.attention.output.dense.weight     of shape (768, 768)
2023-12-07 16:48:49,592.592 8155:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.attention.self.key.bias           will be loaded from textual.transformer.encoder.layer.2.attention.self.key.bias           of shape (768,)
2023-12-07 16:48:49,592.592 8155:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.attention.self.key.weight         will be loaded from textual.transformer.encoder.layer.2.attention.self.key.weight         of shape (768, 768)
2023-12-07 16:48:49,592.592 8155:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.attention.self.query.bias         will be loaded from textual.transformer.encoder.layer.2.attention.self.query.bias         of shape (768,)
2023-12-07 16:48:49,592.592 8155:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.attention.self.query.weight       will be loaded from textual.transformer.encoder.layer.2.attention.self.query.weight       of shape (768, 768)
2023-12-07 16:48:49,592.592 8155:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.attention.self.value.bias         will be loaded from textual.transformer.encoder.layer.2.attention.self.value.bias         of shape (768,)
2023-12-07 16:48:49,592.592 8155:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.attention.self.value.weight       will be loaded from textual.transformer.encoder.layer.2.attention.self.value.weight       of shape (768, 768)
2023-12-07 16:48:49,592.592 8155:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.intermediate.dense.bias           will be loaded from textual.transformer.encoder.layer.2.intermediate.dense.bias           of shape (3072,)
2023-12-07 16:48:49,592.592 8155:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.intermediate.dense.weight         will be loaded from textual.transformer.encoder.layer.2.intermediate.dense.weight         of shape (3072, 768)
2023-12-07 16:48:49,592.592 8155:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.output.LayerNorm.bias             will be loaded from textual.transformer.encoder.layer.2.output.LayerNorm.bias             of shape (768,)
2023-12-07 16:48:49,592.592 8155:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.output.LayerNorm.weight           will be loaded from textual.transformer.encoder.layer.2.output.LayerNorm.weight           of shape (768,)
2023-12-07 16:48:49,592.592 8155:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.output.dense.bias                 will be loaded from textual.transformer.encoder.layer.2.output.dense.bias                 of shape (768,)
2023-12-07 16:48:49,592.592 8155:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.output.dense.weight               will be loaded from textual.transformer.encoder.layer.2.output.dense.weight               of shape (768, 3072)
2023-12-07 16:48:49,592.592 8155:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.attention.output.LayerNorm.bias   will be loaded from textual.transformer.encoder.layer.3.attention.output.LayerNorm.bias   of shape (768,)
2023-12-07 16:48:49,592.592 8155:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.attention.output.LayerNorm.weight will be loaded from textual.transformer.encoder.layer.3.attention.output.LayerNorm.weight of shape (768,)
2023-12-07 16:48:49,592.592 8155:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.attention.output.dense.bias       will be loaded from textual.transformer.encoder.layer.3.attention.output.dense.bias       of shape (768,)
2023-12-07 16:48:49,592.592 8155:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.attention.output.dense.weight     will be loaded from textual.transformer.encoder.layer.3.attention.output.dense.weight     of shape (768, 768)
2023-12-07 16:48:49,592.592 8155:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.attention.self.key.bias           will be loaded from textual.transformer.encoder.layer.3.attention.self.key.bias           of shape (768,)
2023-12-07 16:48:49,593.593 8155:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.attention.self.key.weight         will be loaded from textual.transformer.encoder.layer.3.attention.self.key.weight         of shape (768, 768)
2023-12-07 16:48:49,593.593 8155:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.attention.self.query.bias         will be loaded from textual.transformer.encoder.layer.3.attention.self.query.bias         of shape (768,)
2023-12-07 16:48:49,593.593 8155:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.attention.self.query.weight       will be loaded from textual.transformer.encoder.layer.3.attention.self.query.weight       of shape (768, 768)
2023-12-07 16:48:49,593.593 8155:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.attention.self.value.bias         will be loaded from textual.transformer.encoder.layer.3.attention.self.value.bias         of shape (768,)
2023-12-07 16:48:49,593.593 8155:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.attention.self.value.weight       will be loaded from textual.transformer.encoder.layer.3.attention.self.value.weight       of shape (768, 768)
2023-12-07 16:48:49,593.593 8155:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.intermediate.dense.bias           will be loaded from textual.transformer.encoder.layer.3.intermediate.dense.bias           of shape (3072,)
2023-12-07 16:48:49,593.593 8155:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.intermediate.dense.weight         will be loaded from textual.transformer.encoder.layer.3.intermediate.dense.weight         of shape (3072, 768)
2023-12-07 16:48:49,593.593 8155:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.output.LayerNorm.bias             will be loaded from textual.transformer.encoder.layer.3.output.LayerNorm.bias             of shape (768,)
2023-12-07 16:48:49,593.593 8155:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.output.LayerNorm.weight           will be loaded from textual.transformer.encoder.layer.3.output.LayerNorm.weight           of shape (768,)
2023-12-07 16:48:49,593.593 8155:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.output.dense.bias                 will be loaded from textual.transformer.encoder.layer.3.output.dense.bias                 of shape (768,)
2023-12-07 16:48:49,593.593 8155:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.output.dense.weight               will be loaded from textual.transformer.encoder.layer.3.output.dense.weight               of shape (768, 3072)
2023-12-07 16:48:49,593.593 8155:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.attention.output.LayerNorm.bias   will be loaded from textual.transformer.encoder.layer.4.attention.output.LayerNorm.bias   of shape (768,)
2023-12-07 16:48:49,593.593 8155:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.attention.output.LayerNorm.weight will be loaded from textual.transformer.encoder.layer.4.attention.output.LayerNorm.weight of shape (768,)
2023-12-07 16:48:49,593.593 8155:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.attention.output.dense.bias       will be loaded from textual.transformer.encoder.layer.4.attention.output.dense.bias       of shape (768,)
2023-12-07 16:48:49,593.593 8155:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.attention.output.dense.weight     will be loaded from textual.transformer.encoder.layer.4.attention.output.dense.weight     of shape (768, 768)
2023-12-07 16:48:49,593.593 8155:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.attention.self.key.bias           will be loaded from textual.transformer.encoder.layer.4.attention.self.key.bias           of shape (768,)
2023-12-07 16:48:49,593.593 8155:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.attention.self.key.weight         will be loaded from textual.transformer.encoder.layer.4.attention.self.key.weight         of shape (768, 768)
2023-12-07 16:48:49,593.593 8155:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.attention.self.query.bias         will be loaded from textual.transformer.encoder.layer.4.attention.self.query.bias         of shape (768,)
2023-12-07 16:48:49,593.593 8155:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.attention.self.query.weight       will be loaded from textual.transformer.encoder.layer.4.attention.self.query.weight       of shape (768, 768)
2023-12-07 16:48:49,593.593 8155:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.attention.self.value.bias         will be loaded from textual.transformer.encoder.layer.4.attention.self.value.bias         of shape (768,)
2023-12-07 16:48:49,593.593 8155:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.attention.self.value.weight       will be loaded from textual.transformer.encoder.layer.4.attention.self.value.weight       of shape (768, 768)
2023-12-07 16:48:49,593.593 8155:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.intermediate.dense.bias           will be loaded from textual.transformer.encoder.layer.4.intermediate.dense.bias           of shape (3072,)
2023-12-07 16:48:49,593.593 8155:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.intermediate.dense.weight         will be loaded from textual.transformer.encoder.layer.4.intermediate.dense.weight         of shape (3072, 768)
2023-12-07 16:48:49,593.593 8155:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.output.LayerNorm.bias             will be loaded from textual.transformer.encoder.layer.4.output.LayerNorm.bias             of shape (768,)
2023-12-07 16:48:49,593.593 8155:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.output.LayerNorm.weight           will be loaded from textual.transformer.encoder.layer.4.output.LayerNorm.weight           of shape (768,)
2023-12-07 16:48:49,594.594 8155:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.output.dense.bias                 will be loaded from textual.transformer.encoder.layer.4.output.dense.bias                 of shape (768,)
2023-12-07 16:48:49,594.594 8155:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.output.dense.weight               will be loaded from textual.transformer.encoder.layer.4.output.dense.weight               of shape (768, 3072)
2023-12-07 16:48:49,594.594 8155:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.attention.output.LayerNorm.bias   will be loaded from textual.transformer.encoder.layer.5.attention.output.LayerNorm.bias   of shape (768,)
2023-12-07 16:48:49,594.594 8155:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.attention.output.LayerNorm.weight will be loaded from textual.transformer.encoder.layer.5.attention.output.LayerNorm.weight of shape (768,)
2023-12-07 16:48:49,594.594 8155:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.attention.output.dense.bias       will be loaded from textual.transformer.encoder.layer.5.attention.output.dense.bias       of shape (768,)
2023-12-07 16:48:49,594.594 8155:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.attention.output.dense.weight     will be loaded from textual.transformer.encoder.layer.5.attention.output.dense.weight     of shape (768, 768)
2023-12-07 16:48:49,594.594 8155:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.attention.self.key.bias           will be loaded from textual.transformer.encoder.layer.5.attention.self.key.bias           of shape (768,)
2023-12-07 16:48:49,594.594 8155:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.attention.self.key.weight         will be loaded from textual.transformer.encoder.layer.5.attention.self.key.weight         of shape (768, 768)
2023-12-07 16:48:49,594.594 8155:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.attention.self.query.bias         will be loaded from textual.transformer.encoder.layer.5.attention.self.query.bias         of shape (768,)
2023-12-07 16:48:49,594.594 8155:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.attention.self.query.weight       will be loaded from textual.transformer.encoder.layer.5.attention.self.query.weight       of shape (768, 768)
2023-12-07 16:48:49,594.594 8155:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.attention.self.value.bias         will be loaded from textual.transformer.encoder.layer.5.attention.self.value.bias         of shape (768,)
2023-12-07 16:48:49,594.594 8155:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.attention.self.value.weight       will be loaded from textual.transformer.encoder.layer.5.attention.self.value.weight       of shape (768, 768)
2023-12-07 16:48:49,594.594 8155:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.intermediate.dense.bias           will be loaded from textual.transformer.encoder.layer.5.intermediate.dense.bias           of shape (3072,)
2023-12-07 16:48:49,594.594 8155:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.intermediate.dense.weight         will be loaded from textual.transformer.encoder.layer.5.intermediate.dense.weight         of shape (3072, 768)
2023-12-07 16:48:49,594.594 8155:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.output.LayerNorm.bias             will be loaded from textual.transformer.encoder.layer.5.output.LayerNorm.bias             of shape (768,)
2023-12-07 16:48:49,594.594 8155:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.output.LayerNorm.weight           will be loaded from textual.transformer.encoder.layer.5.output.LayerNorm.weight           of shape (768,)
2023-12-07 16:48:49,594.594 8155:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.output.dense.bias                 will be loaded from textual.transformer.encoder.layer.5.output.dense.bias                 of shape (768,)
2023-12-07 16:48:49,594.594 8155:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.output.dense.weight               will be loaded from textual.transformer.encoder.layer.5.output.dense.weight               of shape (768, 3072)
2023-12-07 16:48:49,594.594 8155:torch_common.py:128 align_and_update_state_dicts(): textual.visual_projection.0.bias                                      will be loaded from textual.visual_projection.0.bias                                      of shape (768,)
2023-12-07 16:48:49,594.594 8155:torch_common.py:128 align_and_update_state_dicts(): textual.visual_projection.0.weight                                    will be loaded from textual.visual_projection.0.weight                                    of shape (768, 768)
2023-12-07 16:48:49,594.594 8155:torch_common.py:128 align_and_update_state_dicts(): textual.visual_projection.1.bias                                      will be loaded from textual.visual_projection.1.bias                                      of shape (768,)
2023-12-07 16:48:49,594.594 8155:torch_common.py:128 align_and_update_state_dicts(): textual.visual_projection.1.weight                                    will be loaded from textual.visual_projection.1.weight                                    of shape (768,)
2023-12-07 16:48:49,594.594 8155:torch_common.py:137 align_and_update_state_dicts(): target model param = 258; name matched = 258; loaded = 258
2023-12-07 16:48:49,594.594 8155:torch_common.py:140 align_and_update_state_dicts(): from loaded; ignore = []
2023-12-07 16:48:49,596.596 8155:torch_common.py:82 load_model_state_ignore_mismatch(): unique keys in init dict = ['textual.output.weight']; total = 1
2023-12-07 16:48:49,686.686 8155:torch_common.py:86 load_model_state_ignore_mismatch(): unique key (not initialized) in current model = ['textual.output.weight']
/home/paperspace/Desktop/GenerativeImage2Text/generativeimage2text/layers/decoder.py:1198: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  beam_id = idx // vocab_size
2023-12-07 16:48:56,960.960 8155:inference.py:109 test_git_inference_single_image(): output: the water is calm
]0;paperspace@psybffhvzv5t: ~/Desktop/GenerativeImage2Text[01;32mpaperspace@psybffhvzv5t[00m:[01;34m~/Desktop/GenerativeImage2Text[00m$ AZFUSE_TSV_USE_FUSE=1 python -m generativeimage2text.inference -p "{'type': 'test_git_inference_single_image', \
> 'image_path': 'aux_data/images/1.jpg', \
> 'model_name': 'GIT_BASE_VQAv2', \
> 'prefix': 'what is it?', \
> }"
2023-12-07 16:49:50,901.901 8386:inference.py:318   <module>(): param:
{'image_path': 'aux_data/images/1.jpg',
 'model_name': 'GIT_BASE_VQAv2',
 'prefix': 'what is it?',
 'type': 'test_git_inference_single_image'}
/home/paperspace/Desktop/GenerativeImage2Text/generativeimage2text/inference.py:63: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.
  image = F.resize(img, size, interpolation=PIL.Image.BICUBIC)
/home/paperspace/.local/lib/python3.9/site-packages/torchvision/transforms/functional.py:417: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.
  warnings.warn(
/home/paperspace/.local/lib/python3.9/site-packages/torchvision/transforms/transforms.py:332: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.
  warnings.warn(
2023-12-07 16:49:59,186.186 8386:cloud_storage.py:1150 az_download_once(): if sync, no need to save to temp first
2023-12-07 16:49:59,186.186 8386:common.py:324    cmd_run(): start to cmd run: azcopy cp https://publicgit.blob.core.windows.net/data/output/GIT_BASE_VQAv2/snapshot/model.pt /tmp/publicgit/output/GIT_BASE_VQAv2/snapshot/model.pt
INFO: Scanning...
INFO: Autologin not specified.
INFO: Any empty folders will not be processed, because source and/or destination doesn't have full folder support

Job 03f184bb-3aea-4e4f-783a-e7909c341991 has started
Log file is located at: /home/paperspace/.azcopy/03f184bb-3aea-4e4f-783a-e7909c341991.log

47.4 %, 0 Done, 0 Failed, 1 Pending, 0 Skipped, 1 Total, 2-sec Throughput (Mb/s): 1647.690398.2 %, 0 Done, 0 Failed, 1 Pending, 0 Skipped, 1 Total, 2-sec Throughput (Mb/s): 201.2587 100.0 %, 1 Done, 0 Failed, 0 Pending, 0 Skipped, 1 Total,                                 


Job 03f184bb-3aea-4e4f-783a-e7909c341991 summary
Elapsed Time (Minutes): 0.1
Number of File Transfers: 1
Number of Folder Property Transfers: 0
Number of Symlink Transfers: 0
Total Number of Transfers: 1
Number of File Transfers Completed: 1
Number of Folder Transfers Completed: 0
Number of File Transfers Failed: 0
Number of Folder Transfers Failed: 0
Number of File Transfers Skipped: 0
Number of Folder Transfers Skipped: 0
TotalBytesTransferred: 462409367
Final Job Status: Completed

2023-12-07 16:50:06,317.317 8386:cloud_storage.py:543 open_to_read(): download output/GIT_BASE_VQAv2/snapshot/model.pt to /tmp/publicgit/output/GIT_BASE_VQAv2/snapshot/model.pt: 10.469920635223389; open cache: 3.7670135498046875e-05
2023-12-07 16:50:07,005.005 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.class_embedding                                         will be loaded from image_encoder.class_embedding                                         of shape (768,)
2023-12-07 16:50:07,005.005 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.conv1.weight                                            will be loaded from image_encoder.conv1.weight                                            of shape (768, 3, 16, 16)
2023-12-07 16:50:07,006.006 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.ln_post.bias                                            will be loaded from image_encoder.ln_post.bias                                            of shape (768,)
2023-12-07 16:50:07,006.006 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.ln_post.weight                                          will be loaded from image_encoder.ln_post.weight                                          of shape (768,)
2023-12-07 16:50:07,006.006 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.ln_pre.bias                                             will be loaded from image_encoder.ln_pre.bias                                             of shape (768,)
2023-12-07 16:50:07,006.006 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.ln_pre.weight                                           will be loaded from image_encoder.ln_pre.weight                                           of shape (768,)
2023-12-07 16:50:07,006.006 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.positional_embedding                                    will be loaded from image_encoder.positional_embedding                                    of shape (901, 768)
2023-12-07 16:50:07,006.006 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.proj                                                    will be loaded from image_encoder.proj                                                    of shape (768, 512)
2023-12-07 16:50:07,006.006 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.0.attn.in_proj_bias               will be loaded from image_encoder.transformer.resblocks.0.attn.in_proj_bias               of shape (2304,)
2023-12-07 16:50:07,006.006 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.0.attn.in_proj_weight             will be loaded from image_encoder.transformer.resblocks.0.attn.in_proj_weight             of shape (2304, 768)
2023-12-07 16:50:07,006.006 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.0.attn.out_proj.bias              will be loaded from image_encoder.transformer.resblocks.0.attn.out_proj.bias              of shape (768,)
2023-12-07 16:50:07,006.006 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.0.attn.out_proj.weight            will be loaded from image_encoder.transformer.resblocks.0.attn.out_proj.weight            of shape (768, 768)
2023-12-07 16:50:07,006.006 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.0.ln_1.bias                       will be loaded from image_encoder.transformer.resblocks.0.ln_1.bias                       of shape (768,)
2023-12-07 16:50:07,006.006 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.0.ln_1.weight                     will be loaded from image_encoder.transformer.resblocks.0.ln_1.weight                     of shape (768,)
2023-12-07 16:50:07,006.006 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.0.ln_2.bias                       will be loaded from image_encoder.transformer.resblocks.0.ln_2.bias                       of shape (768,)
2023-12-07 16:50:07,006.006 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.0.ln_2.weight                     will be loaded from image_encoder.transformer.resblocks.0.ln_2.weight                     of shape (768,)
2023-12-07 16:50:07,006.006 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.0.mlp.c_fc.bias                   will be loaded from image_encoder.transformer.resblocks.0.mlp.c_fc.bias                   of shape (3072,)
2023-12-07 16:50:07,006.006 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.0.mlp.c_fc.weight                 will be loaded from image_encoder.transformer.resblocks.0.mlp.c_fc.weight                 of shape (3072, 768)
2023-12-07 16:50:07,007.007 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.0.mlp.c_proj.bias                 will be loaded from image_encoder.transformer.resblocks.0.mlp.c_proj.bias                 of shape (768,)
2023-12-07 16:50:07,007.007 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.0.mlp.c_proj.weight               will be loaded from image_encoder.transformer.resblocks.0.mlp.c_proj.weight               of shape (768, 3072)
2023-12-07 16:50:07,007.007 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.1.attn.in_proj_bias               will be loaded from image_encoder.transformer.resblocks.1.attn.in_proj_bias               of shape (2304,)
2023-12-07 16:50:07,007.007 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.1.attn.in_proj_weight             will be loaded from image_encoder.transformer.resblocks.1.attn.in_proj_weight             of shape (2304, 768)
2023-12-07 16:50:07,007.007 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.1.attn.out_proj.bias              will be loaded from image_encoder.transformer.resblocks.1.attn.out_proj.bias              of shape (768,)
2023-12-07 16:50:07,007.007 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.1.attn.out_proj.weight            will be loaded from image_encoder.transformer.resblocks.1.attn.out_proj.weight            of shape (768, 768)
2023-12-07 16:50:07,007.007 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.1.ln_1.bias                       will be loaded from image_encoder.transformer.resblocks.1.ln_1.bias                       of shape (768,)
2023-12-07 16:50:07,007.007 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.1.ln_1.weight                     will be loaded from image_encoder.transformer.resblocks.1.ln_1.weight                     of shape (768,)
2023-12-07 16:50:07,007.007 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.1.ln_2.bias                       will be loaded from image_encoder.transformer.resblocks.1.ln_2.bias                       of shape (768,)
2023-12-07 16:50:07,007.007 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.1.ln_2.weight                     will be loaded from image_encoder.transformer.resblocks.1.ln_2.weight                     of shape (768,)
2023-12-07 16:50:07,007.007 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.1.mlp.c_fc.bias                   will be loaded from image_encoder.transformer.resblocks.1.mlp.c_fc.bias                   of shape (3072,)
2023-12-07 16:50:07,007.007 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.1.mlp.c_fc.weight                 will be loaded from image_encoder.transformer.resblocks.1.mlp.c_fc.weight                 of shape (3072, 768)
2023-12-07 16:50:07,007.007 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.1.mlp.c_proj.bias                 will be loaded from image_encoder.transformer.resblocks.1.mlp.c_proj.bias                 of shape (768,)
2023-12-07 16:50:07,007.007 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.1.mlp.c_proj.weight               will be loaded from image_encoder.transformer.resblocks.1.mlp.c_proj.weight               of shape (768, 3072)
2023-12-07 16:50:07,007.007 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.10.attn.in_proj_bias              will be loaded from image_encoder.transformer.resblocks.10.attn.in_proj_bias              of shape (2304,)
2023-12-07 16:50:07,007.007 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.10.attn.in_proj_weight            will be loaded from image_encoder.transformer.resblocks.10.attn.in_proj_weight            of shape (2304, 768)
2023-12-07 16:50:07,007.007 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.10.attn.out_proj.bias             will be loaded from image_encoder.transformer.resblocks.10.attn.out_proj.bias             of shape (768,)
2023-12-07 16:50:07,007.007 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.10.attn.out_proj.weight           will be loaded from image_encoder.transformer.resblocks.10.attn.out_proj.weight           of shape (768, 768)
2023-12-07 16:50:07,007.007 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.10.ln_1.bias                      will be loaded from image_encoder.transformer.resblocks.10.ln_1.bias                      of shape (768,)
2023-12-07 16:50:07,007.007 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.10.ln_1.weight                    will be loaded from image_encoder.transformer.resblocks.10.ln_1.weight                    of shape (768,)
2023-12-07 16:50:07,008.008 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.10.ln_2.bias                      will be loaded from image_encoder.transformer.resblocks.10.ln_2.bias                      of shape (768,)
2023-12-07 16:50:07,008.008 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.10.ln_2.weight                    will be loaded from image_encoder.transformer.resblocks.10.ln_2.weight                    of shape (768,)
2023-12-07 16:50:07,008.008 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.10.mlp.c_fc.bias                  will be loaded from image_encoder.transformer.resblocks.10.mlp.c_fc.bias                  of shape (3072,)
2023-12-07 16:50:07,008.008 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.10.mlp.c_fc.weight                will be loaded from image_encoder.transformer.resblocks.10.mlp.c_fc.weight                of shape (3072, 768)
2023-12-07 16:50:07,008.008 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.10.mlp.c_proj.bias                will be loaded from image_encoder.transformer.resblocks.10.mlp.c_proj.bias                of shape (768,)
2023-12-07 16:50:07,008.008 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.10.mlp.c_proj.weight              will be loaded from image_encoder.transformer.resblocks.10.mlp.c_proj.weight              of shape (768, 3072)
2023-12-07 16:50:07,008.008 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.11.attn.in_proj_bias              will be loaded from image_encoder.transformer.resblocks.11.attn.in_proj_bias              of shape (2304,)
2023-12-07 16:50:07,008.008 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.11.attn.in_proj_weight            will be loaded from image_encoder.transformer.resblocks.11.attn.in_proj_weight            of shape (2304, 768)
2023-12-07 16:50:07,008.008 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.11.attn.out_proj.bias             will be loaded from image_encoder.transformer.resblocks.11.attn.out_proj.bias             of shape (768,)
2023-12-07 16:50:07,008.008 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.11.attn.out_proj.weight           will be loaded from image_encoder.transformer.resblocks.11.attn.out_proj.weight           of shape (768, 768)
2023-12-07 16:50:07,008.008 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.11.ln_1.bias                      will be loaded from image_encoder.transformer.resblocks.11.ln_1.bias                      of shape (768,)
2023-12-07 16:50:07,008.008 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.11.ln_1.weight                    will be loaded from image_encoder.transformer.resblocks.11.ln_1.weight                    of shape (768,)
2023-12-07 16:50:07,008.008 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.11.ln_2.bias                      will be loaded from image_encoder.transformer.resblocks.11.ln_2.bias                      of shape (768,)
2023-12-07 16:50:07,008.008 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.11.ln_2.weight                    will be loaded from image_encoder.transformer.resblocks.11.ln_2.weight                    of shape (768,)
2023-12-07 16:50:07,008.008 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.11.mlp.c_fc.bias                  will be loaded from image_encoder.transformer.resblocks.11.mlp.c_fc.bias                  of shape (3072,)
2023-12-07 16:50:07,008.008 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.11.mlp.c_fc.weight                will be loaded from image_encoder.transformer.resblocks.11.mlp.c_fc.weight                of shape (3072, 768)
2023-12-07 16:50:07,008.008 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.11.mlp.c_proj.bias                will be loaded from image_encoder.transformer.resblocks.11.mlp.c_proj.bias                of shape (768,)
2023-12-07 16:50:07,008.008 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.11.mlp.c_proj.weight              will be loaded from image_encoder.transformer.resblocks.11.mlp.c_proj.weight              of shape (768, 3072)
2023-12-07 16:50:07,008.008 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.2.attn.in_proj_bias               will be loaded from image_encoder.transformer.resblocks.2.attn.in_proj_bias               of shape (2304,)
2023-12-07 16:50:07,008.008 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.2.attn.in_proj_weight             will be loaded from image_encoder.transformer.resblocks.2.attn.in_proj_weight             of shape (2304, 768)
2023-12-07 16:50:07,008.008 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.2.attn.out_proj.bias              will be loaded from image_encoder.transformer.resblocks.2.attn.out_proj.bias              of shape (768,)
2023-12-07 16:50:07,009.009 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.2.attn.out_proj.weight            will be loaded from image_encoder.transformer.resblocks.2.attn.out_proj.weight            of shape (768, 768)
2023-12-07 16:50:07,009.009 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.2.ln_1.bias                       will be loaded from image_encoder.transformer.resblocks.2.ln_1.bias                       of shape (768,)
2023-12-07 16:50:07,009.009 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.2.ln_1.weight                     will be loaded from image_encoder.transformer.resblocks.2.ln_1.weight                     of shape (768,)
2023-12-07 16:50:07,009.009 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.2.ln_2.bias                       will be loaded from image_encoder.transformer.resblocks.2.ln_2.bias                       of shape (768,)
2023-12-07 16:50:07,009.009 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.2.ln_2.weight                     will be loaded from image_encoder.transformer.resblocks.2.ln_2.weight                     of shape (768,)
2023-12-07 16:50:07,009.009 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.2.mlp.c_fc.bias                   will be loaded from image_encoder.transformer.resblocks.2.mlp.c_fc.bias                   of shape (3072,)
2023-12-07 16:50:07,009.009 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.2.mlp.c_fc.weight                 will be loaded from image_encoder.transformer.resblocks.2.mlp.c_fc.weight                 of shape (3072, 768)
2023-12-07 16:50:07,009.009 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.2.mlp.c_proj.bias                 will be loaded from image_encoder.transformer.resblocks.2.mlp.c_proj.bias                 of shape (768,)
2023-12-07 16:50:07,009.009 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.2.mlp.c_proj.weight               will be loaded from image_encoder.transformer.resblocks.2.mlp.c_proj.weight               of shape (768, 3072)
2023-12-07 16:50:07,009.009 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.3.attn.in_proj_bias               will be loaded from image_encoder.transformer.resblocks.3.attn.in_proj_bias               of shape (2304,)
2023-12-07 16:50:07,009.009 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.3.attn.in_proj_weight             will be loaded from image_encoder.transformer.resblocks.3.attn.in_proj_weight             of shape (2304, 768)
2023-12-07 16:50:07,009.009 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.3.attn.out_proj.bias              will be loaded from image_encoder.transformer.resblocks.3.attn.out_proj.bias              of shape (768,)
2023-12-07 16:50:07,009.009 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.3.attn.out_proj.weight            will be loaded from image_encoder.transformer.resblocks.3.attn.out_proj.weight            of shape (768, 768)
2023-12-07 16:50:07,009.009 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.3.ln_1.bias                       will be loaded from image_encoder.transformer.resblocks.3.ln_1.bias                       of shape (768,)
2023-12-07 16:50:07,009.009 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.3.ln_1.weight                     will be loaded from image_encoder.transformer.resblocks.3.ln_1.weight                     of shape (768,)
2023-12-07 16:50:07,009.009 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.3.ln_2.bias                       will be loaded from image_encoder.transformer.resblocks.3.ln_2.bias                       of shape (768,)
2023-12-07 16:50:07,009.009 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.3.ln_2.weight                     will be loaded from image_encoder.transformer.resblocks.3.ln_2.weight                     of shape (768,)
2023-12-07 16:50:07,009.009 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.3.mlp.c_fc.bias                   will be loaded from image_encoder.transformer.resblocks.3.mlp.c_fc.bias                   of shape (3072,)
2023-12-07 16:50:07,009.009 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.3.mlp.c_fc.weight                 will be loaded from image_encoder.transformer.resblocks.3.mlp.c_fc.weight                 of shape (3072, 768)
2023-12-07 16:50:07,009.009 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.3.mlp.c_proj.bias                 will be loaded from image_encoder.transformer.resblocks.3.mlp.c_proj.bias                 of shape (768,)
2023-12-07 16:50:07,010.010 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.3.mlp.c_proj.weight               will be loaded from image_encoder.transformer.resblocks.3.mlp.c_proj.weight               of shape (768, 3072)
2023-12-07 16:50:07,010.010 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.4.attn.in_proj_bias               will be loaded from image_encoder.transformer.resblocks.4.attn.in_proj_bias               of shape (2304,)
2023-12-07 16:50:07,010.010 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.4.attn.in_proj_weight             will be loaded from image_encoder.transformer.resblocks.4.attn.in_proj_weight             of shape (2304, 768)
2023-12-07 16:50:07,010.010 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.4.attn.out_proj.bias              will be loaded from image_encoder.transformer.resblocks.4.attn.out_proj.bias              of shape (768,)
2023-12-07 16:50:07,010.010 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.4.attn.out_proj.weight            will be loaded from image_encoder.transformer.resblocks.4.attn.out_proj.weight            of shape (768, 768)
2023-12-07 16:50:07,010.010 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.4.ln_1.bias                       will be loaded from image_encoder.transformer.resblocks.4.ln_1.bias                       of shape (768,)
2023-12-07 16:50:07,010.010 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.4.ln_1.weight                     will be loaded from image_encoder.transformer.resblocks.4.ln_1.weight                     of shape (768,)
2023-12-07 16:50:07,010.010 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.4.ln_2.bias                       will be loaded from image_encoder.transformer.resblocks.4.ln_2.bias                       of shape (768,)
2023-12-07 16:50:07,010.010 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.4.ln_2.weight                     will be loaded from image_encoder.transformer.resblocks.4.ln_2.weight                     of shape (768,)
2023-12-07 16:50:07,010.010 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.4.mlp.c_fc.bias                   will be loaded from image_encoder.transformer.resblocks.4.mlp.c_fc.bias                   of shape (3072,)
2023-12-07 16:50:07,010.010 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.4.mlp.c_fc.weight                 will be loaded from image_encoder.transformer.resblocks.4.mlp.c_fc.weight                 of shape (3072, 768)
2023-12-07 16:50:07,010.010 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.4.mlp.c_proj.bias                 will be loaded from image_encoder.transformer.resblocks.4.mlp.c_proj.bias                 of shape (768,)
2023-12-07 16:50:07,010.010 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.4.mlp.c_proj.weight               will be loaded from image_encoder.transformer.resblocks.4.mlp.c_proj.weight               of shape (768, 3072)
2023-12-07 16:50:07,010.010 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.5.attn.in_proj_bias               will be loaded from image_encoder.transformer.resblocks.5.attn.in_proj_bias               of shape (2304,)
2023-12-07 16:50:07,010.010 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.5.attn.in_proj_weight             will be loaded from image_encoder.transformer.resblocks.5.attn.in_proj_weight             of shape (2304, 768)
2023-12-07 16:50:07,010.010 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.5.attn.out_proj.bias              will be loaded from image_encoder.transformer.resblocks.5.attn.out_proj.bias              of shape (768,)
2023-12-07 16:50:07,010.010 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.5.attn.out_proj.weight            will be loaded from image_encoder.transformer.resblocks.5.attn.out_proj.weight            of shape (768, 768)
2023-12-07 16:50:07,010.010 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.5.ln_1.bias                       will be loaded from image_encoder.transformer.resblocks.5.ln_1.bias                       of shape (768,)
2023-12-07 16:50:07,010.010 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.5.ln_1.weight                     will be loaded from image_encoder.transformer.resblocks.5.ln_1.weight                     of shape (768,)
2023-12-07 16:50:07,010.010 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.5.ln_2.bias                       will be loaded from image_encoder.transformer.resblocks.5.ln_2.bias                       of shape (768,)
2023-12-07 16:50:07,010.010 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.5.ln_2.weight                     will be loaded from image_encoder.transformer.resblocks.5.ln_2.weight                     of shape (768,)
2023-12-07 16:50:07,010.010 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.5.mlp.c_fc.bias                   will be loaded from image_encoder.transformer.resblocks.5.mlp.c_fc.bias                   of shape (3072,)
2023-12-07 16:50:07,010.010 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.5.mlp.c_fc.weight                 will be loaded from image_encoder.transformer.resblocks.5.mlp.c_fc.weight                 of shape (3072, 768)
2023-12-07 16:50:07,011.011 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.5.mlp.c_proj.bias                 will be loaded from image_encoder.transformer.resblocks.5.mlp.c_proj.bias                 of shape (768,)
2023-12-07 16:50:07,011.011 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.5.mlp.c_proj.weight               will be loaded from image_encoder.transformer.resblocks.5.mlp.c_proj.weight               of shape (768, 3072)
2023-12-07 16:50:07,011.011 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.6.attn.in_proj_bias               will be loaded from image_encoder.transformer.resblocks.6.attn.in_proj_bias               of shape (2304,)
2023-12-07 16:50:07,011.011 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.6.attn.in_proj_weight             will be loaded from image_encoder.transformer.resblocks.6.attn.in_proj_weight             of shape (2304, 768)
2023-12-07 16:50:07,011.011 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.6.attn.out_proj.bias              will be loaded from image_encoder.transformer.resblocks.6.attn.out_proj.bias              of shape (768,)
2023-12-07 16:50:07,011.011 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.6.attn.out_proj.weight            will be loaded from image_encoder.transformer.resblocks.6.attn.out_proj.weight            of shape (768, 768)
2023-12-07 16:50:07,011.011 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.6.ln_1.bias                       will be loaded from image_encoder.transformer.resblocks.6.ln_1.bias                       of shape (768,)
2023-12-07 16:50:07,011.011 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.6.ln_1.weight                     will be loaded from image_encoder.transformer.resblocks.6.ln_1.weight                     of shape (768,)
2023-12-07 16:50:07,011.011 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.6.ln_2.bias                       will be loaded from image_encoder.transformer.resblocks.6.ln_2.bias                       of shape (768,)
2023-12-07 16:50:07,011.011 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.6.ln_2.weight                     will be loaded from image_encoder.transformer.resblocks.6.ln_2.weight                     of shape (768,)
2023-12-07 16:50:07,011.011 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.6.mlp.c_fc.bias                   will be loaded from image_encoder.transformer.resblocks.6.mlp.c_fc.bias                   of shape (3072,)
2023-12-07 16:50:07,011.011 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.6.mlp.c_fc.weight                 will be loaded from image_encoder.transformer.resblocks.6.mlp.c_fc.weight                 of shape (3072, 768)
2023-12-07 16:50:07,011.011 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.6.mlp.c_proj.bias                 will be loaded from image_encoder.transformer.resblocks.6.mlp.c_proj.bias                 of shape (768,)
2023-12-07 16:50:07,011.011 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.6.mlp.c_proj.weight               will be loaded from image_encoder.transformer.resblocks.6.mlp.c_proj.weight               of shape (768, 3072)
2023-12-07 16:50:07,011.011 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.7.attn.in_proj_bias               will be loaded from image_encoder.transformer.resblocks.7.attn.in_proj_bias               of shape (2304,)
2023-12-07 16:50:07,011.011 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.7.attn.in_proj_weight             will be loaded from image_encoder.transformer.resblocks.7.attn.in_proj_weight             of shape (2304, 768)
2023-12-07 16:50:07,011.011 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.7.attn.out_proj.bias              will be loaded from image_encoder.transformer.resblocks.7.attn.out_proj.bias              of shape (768,)
2023-12-07 16:50:07,011.011 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.7.attn.out_proj.weight            will be loaded from image_encoder.transformer.resblocks.7.attn.out_proj.weight            of shape (768, 768)
2023-12-07 16:50:07,011.011 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.7.ln_1.bias                       will be loaded from image_encoder.transformer.resblocks.7.ln_1.bias                       of shape (768,)
2023-12-07 16:50:07,011.011 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.7.ln_1.weight                     will be loaded from image_encoder.transformer.resblocks.7.ln_1.weight                     of shape (768,)
2023-12-07 16:50:07,011.011 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.7.ln_2.bias                       will be loaded from image_encoder.transformer.resblocks.7.ln_2.bias                       of shape (768,)
2023-12-07 16:50:07,011.011 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.7.ln_2.weight                     will be loaded from image_encoder.transformer.resblocks.7.ln_2.weight                     of shape (768,)
2023-12-07 16:50:07,012.012 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.7.mlp.c_fc.bias                   will be loaded from image_encoder.transformer.resblocks.7.mlp.c_fc.bias                   of shape (3072,)
2023-12-07 16:50:07,012.012 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.7.mlp.c_fc.weight                 will be loaded from image_encoder.transformer.resblocks.7.mlp.c_fc.weight                 of shape (3072, 768)
2023-12-07 16:50:07,012.012 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.7.mlp.c_proj.bias                 will be loaded from image_encoder.transformer.resblocks.7.mlp.c_proj.bias                 of shape (768,)
2023-12-07 16:50:07,012.012 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.7.mlp.c_proj.weight               will be loaded from image_encoder.transformer.resblocks.7.mlp.c_proj.weight               of shape (768, 3072)
2023-12-07 16:50:07,012.012 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.8.attn.in_proj_bias               will be loaded from image_encoder.transformer.resblocks.8.attn.in_proj_bias               of shape (2304,)
2023-12-07 16:50:07,012.012 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.8.attn.in_proj_weight             will be loaded from image_encoder.transformer.resblocks.8.attn.in_proj_weight             of shape (2304, 768)
2023-12-07 16:50:07,012.012 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.8.attn.out_proj.bias              will be loaded from image_encoder.transformer.resblocks.8.attn.out_proj.bias              of shape (768,)
2023-12-07 16:50:07,012.012 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.8.attn.out_proj.weight            will be loaded from image_encoder.transformer.resblocks.8.attn.out_proj.weight            of shape (768, 768)
2023-12-07 16:50:07,012.012 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.8.ln_1.bias                       will be loaded from image_encoder.transformer.resblocks.8.ln_1.bias                       of shape (768,)
2023-12-07 16:50:07,012.012 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.8.ln_1.weight                     will be loaded from image_encoder.transformer.resblocks.8.ln_1.weight                     of shape (768,)
2023-12-07 16:50:07,012.012 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.8.ln_2.bias                       will be loaded from image_encoder.transformer.resblocks.8.ln_2.bias                       of shape (768,)
2023-12-07 16:50:07,012.012 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.8.ln_2.weight                     will be loaded from image_encoder.transformer.resblocks.8.ln_2.weight                     of shape (768,)
2023-12-07 16:50:07,012.012 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.8.mlp.c_fc.bias                   will be loaded from image_encoder.transformer.resblocks.8.mlp.c_fc.bias                   of shape (3072,)
2023-12-07 16:50:07,012.012 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.8.mlp.c_fc.weight                 will be loaded from image_encoder.transformer.resblocks.8.mlp.c_fc.weight                 of shape (3072, 768)
2023-12-07 16:50:07,012.012 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.8.mlp.c_proj.bias                 will be loaded from image_encoder.transformer.resblocks.8.mlp.c_proj.bias                 of shape (768,)
2023-12-07 16:50:07,012.012 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.8.mlp.c_proj.weight               will be loaded from image_encoder.transformer.resblocks.8.mlp.c_proj.weight               of shape (768, 3072)
2023-12-07 16:50:07,012.012 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.9.attn.in_proj_bias               will be loaded from image_encoder.transformer.resblocks.9.attn.in_proj_bias               of shape (2304,)
2023-12-07 16:50:07,012.012 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.9.attn.in_proj_weight             will be loaded from image_encoder.transformer.resblocks.9.attn.in_proj_weight             of shape (2304, 768)
2023-12-07 16:50:07,012.012 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.9.attn.out_proj.bias              will be loaded from image_encoder.transformer.resblocks.9.attn.out_proj.bias              of shape (768,)
2023-12-07 16:50:07,012.012 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.9.attn.out_proj.weight            will be loaded from image_encoder.transformer.resblocks.9.attn.out_proj.weight            of shape (768, 768)
2023-12-07 16:50:07,013.013 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.9.ln_1.bias                       will be loaded from image_encoder.transformer.resblocks.9.ln_1.bias                       of shape (768,)
2023-12-07 16:50:07,013.013 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.9.ln_1.weight                     will be loaded from image_encoder.transformer.resblocks.9.ln_1.weight                     of shape (768,)
2023-12-07 16:50:07,013.013 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.9.ln_2.bias                       will be loaded from image_encoder.transformer.resblocks.9.ln_2.bias                       of shape (768,)
2023-12-07 16:50:07,013.013 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.9.ln_2.weight                     will be loaded from image_encoder.transformer.resblocks.9.ln_2.weight                     of shape (768,)
2023-12-07 16:50:07,013.013 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.9.mlp.c_fc.bias                   will be loaded from image_encoder.transformer.resblocks.9.mlp.c_fc.bias                   of shape (3072,)
2023-12-07 16:50:07,013.013 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.9.mlp.c_fc.weight                 will be loaded from image_encoder.transformer.resblocks.9.mlp.c_fc.weight                 of shape (3072, 768)
2023-12-07 16:50:07,013.013 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.9.mlp.c_proj.bias                 will be loaded from image_encoder.transformer.resblocks.9.mlp.c_proj.bias                 of shape (768,)
2023-12-07 16:50:07,013.013 8386:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.9.mlp.c_proj.weight               will be loaded from image_encoder.transformer.resblocks.9.mlp.c_proj.weight               of shape (768, 3072)
2023-12-07 16:50:07,013.013 8386:torch_common.py:128 align_and_update_state_dicts(): textual.embedding.layer_norm.bias                                     will be loaded from textual.embedding.layer_norm.bias                                     of shape (768,)
2023-12-07 16:50:07,013.013 8386:torch_common.py:128 align_and_update_state_dicts(): textual.embedding.layer_norm.weight                                   will be loaded from textual.embedding.layer_norm.weight                                   of shape (768,)
2023-12-07 16:50:07,013.013 8386:torch_common.py:128 align_and_update_state_dicts(): textual.embedding.positions.weight                                    will be loaded from textual.embedding.positions.weight                                    of shape (1024, 768)
2023-12-07 16:50:07,013.013 8386:torch_common.py:128 align_and_update_state_dicts(): textual.embedding.words.weight                                        will be loaded from textual.embedding.words.weight                                        of shape (30522, 768)
2023-12-07 16:50:07,013.013 8386:torch_common.py:128 align_and_update_state_dicts(): textual.output.bias                                                   will be loaded from textual.output.bias                                                   of shape (30522,)
2023-12-07 16:50:07,013.013 8386:torch_common.py:128 align_and_update_state_dicts(): textual.output.weight                                                 will be loaded from textual.output.weight                                                 of shape (30522, 768)
2023-12-07 16:50:07,013.013 8386:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.attention.output.LayerNorm.bias   will be loaded from textual.transformer.encoder.layer.0.attention.output.LayerNorm.bias   of shape (768,)
2023-12-07 16:50:07,013.013 8386:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.attention.output.LayerNorm.weight will be loaded from textual.transformer.encoder.layer.0.attention.output.LayerNorm.weight of shape (768,)
2023-12-07 16:50:07,013.013 8386:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.attention.output.dense.bias       will be loaded from textual.transformer.encoder.layer.0.attention.output.dense.bias       of shape (768,)
2023-12-07 16:50:07,013.013 8386:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.attention.output.dense.weight     will be loaded from textual.transformer.encoder.layer.0.attention.output.dense.weight     of shape (768, 768)
2023-12-07 16:50:07,013.013 8386:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.attention.self.key.bias           will be loaded from textual.transformer.encoder.layer.0.attention.self.key.bias           of shape (768,)
2023-12-07 16:50:07,013.013 8386:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.attention.self.key.weight         will be loaded from textual.transformer.encoder.layer.0.attention.self.key.weight         of shape (768, 768)
2023-12-07 16:50:07,013.013 8386:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.attention.self.query.bias         will be loaded from textual.transformer.encoder.layer.0.attention.self.query.bias         of shape (768,)
2023-12-07 16:50:07,013.013 8386:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.attention.self.query.weight       will be loaded from textual.transformer.encoder.layer.0.attention.self.query.weight       of shape (768, 768)
2023-12-07 16:50:07,014.014 8386:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.attention.self.value.bias         will be loaded from textual.transformer.encoder.layer.0.attention.self.value.bias         of shape (768,)
2023-12-07 16:50:07,014.014 8386:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.attention.self.value.weight       will be loaded from textual.transformer.encoder.layer.0.attention.self.value.weight       of shape (768, 768)
2023-12-07 16:50:07,014.014 8386:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.intermediate.dense.bias           will be loaded from textual.transformer.encoder.layer.0.intermediate.dense.bias           of shape (3072,)
2023-12-07 16:50:07,014.014 8386:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.intermediate.dense.weight         will be loaded from textual.transformer.encoder.layer.0.intermediate.dense.weight         of shape (3072, 768)
2023-12-07 16:50:07,014.014 8386:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.output.LayerNorm.bias             will be loaded from textual.transformer.encoder.layer.0.output.LayerNorm.bias             of shape (768,)
2023-12-07 16:50:07,014.014 8386:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.output.LayerNorm.weight           will be loaded from textual.transformer.encoder.layer.0.output.LayerNorm.weight           of shape (768,)
2023-12-07 16:50:07,014.014 8386:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.output.dense.bias                 will be loaded from textual.transformer.encoder.layer.0.output.dense.bias                 of shape (768,)
2023-12-07 16:50:07,014.014 8386:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.output.dense.weight               will be loaded from textual.transformer.encoder.layer.0.output.dense.weight               of shape (768, 3072)
2023-12-07 16:50:07,014.014 8386:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.attention.output.LayerNorm.bias   will be loaded from textual.transformer.encoder.layer.1.attention.output.LayerNorm.bias   of shape (768,)
2023-12-07 16:50:07,014.014 8386:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.attention.output.LayerNorm.weight will be loaded from textual.transformer.encoder.layer.1.attention.output.LayerNorm.weight of shape (768,)
2023-12-07 16:50:07,014.014 8386:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.attention.output.dense.bias       will be loaded from textual.transformer.encoder.layer.1.attention.output.dense.bias       of shape (768,)
2023-12-07 16:50:07,014.014 8386:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.attention.output.dense.weight     will be loaded from textual.transformer.encoder.layer.1.attention.output.dense.weight     of shape (768, 768)
2023-12-07 16:50:07,014.014 8386:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.attention.self.key.bias           will be loaded from textual.transformer.encoder.layer.1.attention.self.key.bias           of shape (768,)
2023-12-07 16:50:07,014.014 8386:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.attention.self.key.weight         will be loaded from textual.transformer.encoder.layer.1.attention.self.key.weight         of shape (768, 768)
2023-12-07 16:50:07,014.014 8386:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.attention.self.query.bias         will be loaded from textual.transformer.encoder.layer.1.attention.self.query.bias         of shape (768,)
2023-12-07 16:50:07,014.014 8386:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.attention.self.query.weight       will be loaded from textual.transformer.encoder.layer.1.attention.self.query.weight       of shape (768, 768)
2023-12-07 16:50:07,014.014 8386:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.attention.self.value.bias         will be loaded from textual.transformer.encoder.layer.1.attention.self.value.bias         of shape (768,)
2023-12-07 16:50:07,014.014 8386:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.attention.self.value.weight       will be loaded from textual.transformer.encoder.layer.1.attention.self.value.weight       of shape (768, 768)
2023-12-07 16:50:07,014.014 8386:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.intermediate.dense.bias           will be loaded from textual.transformer.encoder.layer.1.intermediate.dense.bias           of shape (3072,)
2023-12-07 16:50:07,014.014 8386:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.intermediate.dense.weight         will be loaded from textual.transformer.encoder.layer.1.intermediate.dense.weight         of shape (3072, 768)
2023-12-07 16:50:07,015.015 8386:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.output.LayerNorm.bias             will be loaded from textual.transformer.encoder.layer.1.output.LayerNorm.bias             of shape (768,)
2023-12-07 16:50:07,015.015 8386:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.output.LayerNorm.weight           will be loaded from textual.transformer.encoder.layer.1.output.LayerNorm.weight           of shape (768,)
2023-12-07 16:50:07,015.015 8386:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.output.dense.bias                 will be loaded from textual.transformer.encoder.layer.1.output.dense.bias                 of shape (768,)
2023-12-07 16:50:07,015.015 8386:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.output.dense.weight               will be loaded from textual.transformer.encoder.layer.1.output.dense.weight               of shape (768, 3072)
2023-12-07 16:50:07,015.015 8386:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.attention.output.LayerNorm.bias   will be loaded from textual.transformer.encoder.layer.2.attention.output.LayerNorm.bias   of shape (768,)
2023-12-07 16:50:07,015.015 8386:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.attention.output.LayerNorm.weight will be loaded from textual.transformer.encoder.layer.2.attention.output.LayerNorm.weight of shape (768,)
2023-12-07 16:50:07,015.015 8386:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.attention.output.dense.bias       will be loaded from textual.transformer.encoder.layer.2.attention.output.dense.bias       of shape (768,)
2023-12-07 16:50:07,015.015 8386:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.attention.output.dense.weight     will be loaded from textual.transformer.encoder.layer.2.attention.output.dense.weight     of shape (768, 768)
2023-12-07 16:50:07,015.015 8386:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.attention.self.key.bias           will be loaded from textual.transformer.encoder.layer.2.attention.self.key.bias           of shape (768,)
2023-12-07 16:50:07,015.015 8386:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.attention.self.key.weight         will be loaded from textual.transformer.encoder.layer.2.attention.self.key.weight         of shape (768, 768)
2023-12-07 16:50:07,015.015 8386:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.attention.self.query.bias         will be loaded from textual.transformer.encoder.layer.2.attention.self.query.bias         of shape (768,)
2023-12-07 16:50:07,015.015 8386:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.attention.self.query.weight       will be loaded from textual.transformer.encoder.layer.2.attention.self.query.weight       of shape (768, 768)
2023-12-07 16:50:07,015.015 8386:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.attention.self.value.bias         will be loaded from textual.transformer.encoder.layer.2.attention.self.value.bias         of shape (768,)
2023-12-07 16:50:07,015.015 8386:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.attention.self.value.weight       will be loaded from textual.transformer.encoder.layer.2.attention.self.value.weight       of shape (768, 768)
2023-12-07 16:50:07,015.015 8386:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.intermediate.dense.bias           will be loaded from textual.transformer.encoder.layer.2.intermediate.dense.bias           of shape (3072,)
2023-12-07 16:50:07,015.015 8386:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.intermediate.dense.weight         will be loaded from textual.transformer.encoder.layer.2.intermediate.dense.weight         of shape (3072, 768)
2023-12-07 16:50:07,015.015 8386:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.output.LayerNorm.bias             will be loaded from textual.transformer.encoder.layer.2.output.LayerNorm.bias             of shape (768,)
2023-12-07 16:50:07,015.015 8386:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.output.LayerNorm.weight           will be loaded from textual.transformer.encoder.layer.2.output.LayerNorm.weight           of shape (768,)
2023-12-07 16:50:07,015.015 8386:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.output.dense.bias                 will be loaded from textual.transformer.encoder.layer.2.output.dense.bias                 of shape (768,)
2023-12-07 16:50:07,015.015 8386:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.output.dense.weight               will be loaded from textual.transformer.encoder.layer.2.output.dense.weight               of shape (768, 3072)
2023-12-07 16:50:07,015.015 8386:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.attention.output.LayerNorm.bias   will be loaded from textual.transformer.encoder.layer.3.attention.output.LayerNorm.bias   of shape (768,)
2023-12-07 16:50:07,016.016 8386:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.attention.output.LayerNorm.weight will be loaded from textual.transformer.encoder.layer.3.attention.output.LayerNorm.weight of shape (768,)
2023-12-07 16:50:07,016.016 8386:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.attention.output.dense.bias       will be loaded from textual.transformer.encoder.layer.3.attention.output.dense.bias       of shape (768,)
2023-12-07 16:50:07,016.016 8386:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.attention.output.dense.weight     will be loaded from textual.transformer.encoder.layer.3.attention.output.dense.weight     of shape (768, 768)
2023-12-07 16:50:07,016.016 8386:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.attention.self.key.bias           will be loaded from textual.transformer.encoder.layer.3.attention.self.key.bias           of shape (768,)
2023-12-07 16:50:07,016.016 8386:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.attention.self.key.weight         will be loaded from textual.transformer.encoder.layer.3.attention.self.key.weight         of shape (768, 768)
2023-12-07 16:50:07,016.016 8386:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.attention.self.query.bias         will be loaded from textual.transformer.encoder.layer.3.attention.self.query.bias         of shape (768,)
2023-12-07 16:50:07,016.016 8386:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.attention.self.query.weight       will be loaded from textual.transformer.encoder.layer.3.attention.self.query.weight       of shape (768, 768)
2023-12-07 16:50:07,016.016 8386:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.attention.self.value.bias         will be loaded from textual.transformer.encoder.layer.3.attention.self.value.bias         of shape (768,)
2023-12-07 16:50:07,016.016 8386:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.attention.self.value.weight       will be loaded from textual.transformer.encoder.layer.3.attention.self.value.weight       of shape (768, 768)
2023-12-07 16:50:07,016.016 8386:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.intermediate.dense.bias           will be loaded from textual.transformer.encoder.layer.3.intermediate.dense.bias           of shape (3072,)
2023-12-07 16:50:07,016.016 8386:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.intermediate.dense.weight         will be loaded from textual.transformer.encoder.layer.3.intermediate.dense.weight         of shape (3072, 768)
2023-12-07 16:50:07,016.016 8386:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.output.LayerNorm.bias             will be loaded from textual.transformer.encoder.layer.3.output.LayerNorm.bias             of shape (768,)
2023-12-07 16:50:07,016.016 8386:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.output.LayerNorm.weight           will be loaded from textual.transformer.encoder.layer.3.output.LayerNorm.weight           of shape (768,)
2023-12-07 16:50:07,016.016 8386:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.output.dense.bias                 will be loaded from textual.transformer.encoder.layer.3.output.dense.bias                 of shape (768,)
2023-12-07 16:50:07,016.016 8386:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.output.dense.weight               will be loaded from textual.transformer.encoder.layer.3.output.dense.weight               of shape (768, 3072)
2023-12-07 16:50:07,016.016 8386:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.attention.output.LayerNorm.bias   will be loaded from textual.transformer.encoder.layer.4.attention.output.LayerNorm.bias   of shape (768,)
2023-12-07 16:50:07,016.016 8386:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.attention.output.LayerNorm.weight will be loaded from textual.transformer.encoder.layer.4.attention.output.LayerNorm.weight of shape (768,)
2023-12-07 16:50:07,016.016 8386:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.attention.output.dense.bias       will be loaded from textual.transformer.encoder.layer.4.attention.output.dense.bias       of shape (768,)
2023-12-07 16:50:07,016.016 8386:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.attention.output.dense.weight     will be loaded from textual.transformer.encoder.layer.4.attention.output.dense.weight     of shape (768, 768)
2023-12-07 16:50:07,016.016 8386:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.attention.self.key.bias           will be loaded from textual.transformer.encoder.layer.4.attention.self.key.bias           of shape (768,)
2023-12-07 16:50:07,016.016 8386:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.attention.self.key.weight         will be loaded from textual.transformer.encoder.layer.4.attention.self.key.weight         of shape (768, 768)
2023-12-07 16:50:07,016.016 8386:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.attention.self.query.bias         will be loaded from textual.transformer.encoder.layer.4.attention.self.query.bias         of shape (768,)
2023-12-07 16:50:07,017.017 8386:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.attention.self.query.weight       will be loaded from textual.transformer.encoder.layer.4.attention.self.query.weight       of shape (768, 768)
2023-12-07 16:50:07,017.017 8386:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.attention.self.value.bias         will be loaded from textual.transformer.encoder.layer.4.attention.self.value.bias         of shape (768,)
2023-12-07 16:50:07,017.017 8386:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.attention.self.value.weight       will be loaded from textual.transformer.encoder.layer.4.attention.self.value.weight       of shape (768, 768)
2023-12-07 16:50:07,017.017 8386:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.intermediate.dense.bias           will be loaded from textual.transformer.encoder.layer.4.intermediate.dense.bias           of shape (3072,)
2023-12-07 16:50:07,017.017 8386:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.intermediate.dense.weight         will be loaded from textual.transformer.encoder.layer.4.intermediate.dense.weight         of shape (3072, 768)
2023-12-07 16:50:07,017.017 8386:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.output.LayerNorm.bias             will be loaded from textual.transformer.encoder.layer.4.output.LayerNorm.bias             of shape (768,)
2023-12-07 16:50:07,017.017 8386:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.output.LayerNorm.weight           will be loaded from textual.transformer.encoder.layer.4.output.LayerNorm.weight           of shape (768,)
2023-12-07 16:50:07,017.017 8386:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.output.dense.bias                 will be loaded from textual.transformer.encoder.layer.4.output.dense.bias                 of shape (768,)
2023-12-07 16:50:07,017.017 8386:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.output.dense.weight               will be loaded from textual.transformer.encoder.layer.4.output.dense.weight               of shape (768, 3072)
2023-12-07 16:50:07,017.017 8386:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.attention.output.LayerNorm.bias   will be loaded from textual.transformer.encoder.layer.5.attention.output.LayerNorm.bias   of shape (768,)
2023-12-07 16:50:07,017.017 8386:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.attention.output.LayerNorm.weight will be loaded from textual.transformer.encoder.layer.5.attention.output.LayerNorm.weight of shape (768,)
2023-12-07 16:50:07,017.017 8386:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.attention.output.dense.bias       will be loaded from textual.transformer.encoder.layer.5.attention.output.dense.bias       of shape (768,)
2023-12-07 16:50:07,017.017 8386:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.attention.output.dense.weight     will be loaded from textual.transformer.encoder.layer.5.attention.output.dense.weight     of shape (768, 768)
2023-12-07 16:50:07,017.017 8386:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.attention.self.key.bias           will be loaded from textual.transformer.encoder.layer.5.attention.self.key.bias           of shape (768,)
2023-12-07 16:50:07,017.017 8386:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.attention.self.key.weight         will be loaded from textual.transformer.encoder.layer.5.attention.self.key.weight         of shape (768, 768)
2023-12-07 16:50:07,017.017 8386:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.attention.self.query.bias         will be loaded from textual.transformer.encoder.layer.5.attention.self.query.bias         of shape (768,)
2023-12-07 16:50:07,017.017 8386:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.attention.self.query.weight       will be loaded from textual.transformer.encoder.layer.5.attention.self.query.weight       of shape (768, 768)
2023-12-07 16:50:07,017.017 8386:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.attention.self.value.bias         will be loaded from textual.transformer.encoder.layer.5.attention.self.value.bias         of shape (768,)
2023-12-07 16:50:07,017.017 8386:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.attention.self.value.weight       will be loaded from textual.transformer.encoder.layer.5.attention.self.value.weight       of shape (768, 768)
2023-12-07 16:50:07,017.017 8386:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.intermediate.dense.bias           will be loaded from textual.transformer.encoder.layer.5.intermediate.dense.bias           of shape (3072,)
2023-12-07 16:50:07,018.018 8386:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.intermediate.dense.weight         will be loaded from textual.transformer.encoder.layer.5.intermediate.dense.weight         of shape (3072, 768)
2023-12-07 16:50:07,018.018 8386:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.output.LayerNorm.bias             will be loaded from textual.transformer.encoder.layer.5.output.LayerNorm.bias             of shape (768,)
2023-12-07 16:50:07,018.018 8386:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.output.LayerNorm.weight           will be loaded from textual.transformer.encoder.layer.5.output.LayerNorm.weight           of shape (768,)
2023-12-07 16:50:07,018.018 8386:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.output.dense.bias                 will be loaded from textual.transformer.encoder.layer.5.output.dense.bias                 of shape (768,)
2023-12-07 16:50:07,018.018 8386:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.output.dense.weight               will be loaded from textual.transformer.encoder.layer.5.output.dense.weight               of shape (768, 3072)
2023-12-07 16:50:07,018.018 8386:torch_common.py:128 align_and_update_state_dicts(): textual.visual_projection.0.bias                                      will be loaded from textual.visual_projection.0.bias                                      of shape (768,)
2023-12-07 16:50:07,018.018 8386:torch_common.py:128 align_and_update_state_dicts(): textual.visual_projection.0.weight                                    will be loaded from textual.visual_projection.0.weight                                    of shape (768, 768)
2023-12-07 16:50:07,018.018 8386:torch_common.py:128 align_and_update_state_dicts(): textual.visual_projection.1.bias                                      will be loaded from textual.visual_projection.1.bias                                      of shape (768,)
2023-12-07 16:50:07,018.018 8386:torch_common.py:128 align_and_update_state_dicts(): textual.visual_projection.1.weight                                    will be loaded from textual.visual_projection.1.weight                                    of shape (768,)
2023-12-07 16:50:07,018.018 8386:torch_common.py:137 align_and_update_state_dicts(): target model param = 258; name matched = 258; loaded = 258
2023-12-07 16:50:07,018.018 8386:torch_common.py:140 align_and_update_state_dicts(): from loaded; ignore = []
2023-12-07 16:50:07,020.020 8386:torch_common.py:82 load_model_state_ignore_mismatch(): unique keys in init dict = ['textual.output.weight']; total = 1
2023-12-07 16:50:07,113.113 8386:torch_common.py:86 load_model_state_ignore_mismatch(): unique key (not initialized) in current model = ['textual.output.weight']
/home/paperspace/Desktop/GenerativeImage2Text/generativeimage2text/layers/decoder.py:1198: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  beam_id = idx // vocab_size
2023-12-07 16:50:17,092.092 8386:inference.py:109 test_git_inference_single_image(): output: boats
]0;paperspace@psybffhvzv5t: ~/Desktop/GenerativeImage2Text[01;32mpaperspace@psybffhvzv5t[00m:[01;34m~/Desktop/GenerativeImage2Text[00m$ 
]0;paperspace@psybffhvzv5t: ~/Desktop/GenerativeImage2Text[01;32mpaperspace@psybffhvzv5t[00m:[01;34m~/Desktop/GenerativeImage2Text[00m$ 
]0;paperspace@psybffhvzv5t: ~/Desktop/GenerativeImage2Text[01;32mpaperspace@psybffhvzv5t[00m:[01;34m~/Desktop/GenerativeImage2Text[00m$ 
]0;paperspace@psybffhvzv5t: ~/Desktop/GenerativeImage2Text[01;32mpaperspace@psybffhvzv5t[00m:[01;34m~/Desktop/GenerativeImage2Text[00m$ AZFUSE_TSV_USE_FUSE=1 python -m generativeimage2text.inference -p "{'type': 'test_git_inference_single_image', \
> 'image_path': ['aux_data/images/1.jpg', 'aux_data/images/1.jpg', 'aux_data/images/1.jpg', 'aux_data/images/1.jpg', 'aux_data/images/1.jpg', 'aux_data/images/1.jpg'], \
> 'model_name': 'GIT_BASE_VATEX', \
> 'prefix': '', \
> }"
2023-12-07 16:51:50,922.922 9027:inference.py:318   <module>(): param:
{'image_path': ['aux_data/images/1.jpg',
                'aux_data/images/1.jpg',
                'aux_data/images/1.jpg',
                'aux_data/images/1.jpg',
                'aux_data/images/1.jpg',
                'aux_data/images/1.jpg'],
 'model_name': 'GIT_BASE_VATEX',
 'prefix': '',
 'type': 'test_git_inference_single_image'}
/home/paperspace/Desktop/GenerativeImage2Text/generativeimage2text/inference.py:119: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.
  Resize(crop_size, interpolation=Image.BICUBIC),
/home/paperspace/.local/lib/python3.9/site-packages/torchvision/transforms/transforms.py:332: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.
  warnings.warn(
2023-12-07 16:51:56,103.103 9027:decoder.py:832   __init__(): creating temperal embedding
2023-12-07 16:51:56,896.896 9027:cloud_storage.py:1150 az_download_once(): if sync, no need to save to temp first
2023-12-07 16:51:56,897.897 9027:common.py:324    cmd_run(): start to cmd run: azcopy cp https://publicgit.blob.core.windows.net/data/output/GIT_BASE_VATEX/snapshot/model.pt /tmp/publicgit/output/GIT_BASE_VATEX/snapshot/model.pt
INFO: Scanning...
INFO: Autologin not specified.
INFO: Any empty folders will not be processed, because source and/or destination doesn't have full folder support

Job 022ce130-1b5b-034a-7e0c-62d62582067d has started
Log file is located at: /home/paperspace/.azcopy/022ce130-1b5b-034a-7e0c-62d62582067d.log

71.0 %, 0 Done, 0 Failed, 1 Pending, 0 Skipped, 1 Total, 2-sec Throughput (Mb/s): 1228.2525100.0 %, 1 Done, 0 Failed, 0 Pending, 0 Skipped, 1 Total,                                  


Job 022ce130-1b5b-034a-7e0c-62d62582067d summary
Elapsed Time (Minutes): 0.0667
Number of File Transfers: 1
Number of Folder Property Transfers: 0
Number of Symlink Transfers: 0
Total Number of Transfers: 1
Number of File Transfers Completed: 1
Number of Folder Transfers Completed: 0
Number of File Transfers Failed: 0
Number of Folder Transfers Failed: 0
Number of File Transfers Skipped: 0
Number of Folder Transfers Skipped: 0
TotalBytesTransferred: 307204583
Final Job Status: Completed

2023-12-07 16:52:02,422.422 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.class_embedding                                         will be loaded from image_encoder.class_embedding                                         of shape (768,)
2023-12-07 16:52:02,422.422 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.conv1.weight                                            will be loaded from image_encoder.conv1.weight                                            of shape (768, 3, 16, 16)
2023-12-07 16:52:02,422.422 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.ln_post.bias                                            will be loaded from image_encoder.ln_post.bias                                            of shape (768,)
2023-12-07 16:52:02,422.422 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.ln_post.weight                                          will be loaded from image_encoder.ln_post.weight                                          of shape (768,)
2023-12-07 16:52:02,422.422 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.ln_pre.bias                                             will be loaded from image_encoder.ln_pre.bias                                             of shape (768,)
2023-12-07 16:52:02,422.422 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.ln_pre.weight                                           will be loaded from image_encoder.ln_pre.weight                                           of shape (768,)
2023-12-07 16:52:02,422.422 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.positional_embedding                                    will be loaded from image_encoder.positional_embedding                                    of shape (197, 768)
2023-12-07 16:52:02,422.422 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.proj                                                    will be loaded from image_encoder.proj                                                    of shape (768, 512)
2023-12-07 16:52:02,423.423 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.0.attn.in_proj_bias               will be loaded from image_encoder.transformer.resblocks.0.attn.in_proj_bias               of shape (2304,)
2023-12-07 16:52:02,423.423 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.0.attn.in_proj_weight             will be loaded from image_encoder.transformer.resblocks.0.attn.in_proj_weight             of shape (2304, 768)
2023-12-07 16:52:02,423.423 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.0.attn.out_proj.bias              will be loaded from image_encoder.transformer.resblocks.0.attn.out_proj.bias              of shape (768,)
2023-12-07 16:52:02,423.423 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.0.attn.out_proj.weight            will be loaded from image_encoder.transformer.resblocks.0.attn.out_proj.weight            of shape (768, 768)
2023-12-07 16:52:02,423.423 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.0.ln_1.bias                       will be loaded from image_encoder.transformer.resblocks.0.ln_1.bias                       of shape (768,)
2023-12-07 16:52:02,423.423 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.0.ln_1.weight                     will be loaded from image_encoder.transformer.resblocks.0.ln_1.weight                     of shape (768,)
2023-12-07 16:52:02,423.423 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.0.ln_2.bias                       will be loaded from image_encoder.transformer.resblocks.0.ln_2.bias                       of shape (768,)
2023-12-07 16:52:02,423.423 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.0.ln_2.weight                     will be loaded from image_encoder.transformer.resblocks.0.ln_2.weight                     of shape (768,)
2023-12-07 16:52:02,423.423 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.0.mlp.c_fc.bias                   will be loaded from image_encoder.transformer.resblocks.0.mlp.c_fc.bias                   of shape (3072,)
2023-12-07 16:52:02,423.423 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.0.mlp.c_fc.weight                 will be loaded from image_encoder.transformer.resblocks.0.mlp.c_fc.weight                 of shape (3072, 768)
2023-12-07 16:52:02,423.423 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.0.mlp.c_proj.bias                 will be loaded from image_encoder.transformer.resblocks.0.mlp.c_proj.bias                 of shape (768,)
2023-12-07 16:52:02,423.423 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.0.mlp.c_proj.weight               will be loaded from image_encoder.transformer.resblocks.0.mlp.c_proj.weight               of shape (768, 3072)
2023-12-07 16:52:02,423.423 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.1.attn.in_proj_bias               will be loaded from image_encoder.transformer.resblocks.1.attn.in_proj_bias               of shape (2304,)
2023-12-07 16:52:02,423.423 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.1.attn.in_proj_weight             will be loaded from image_encoder.transformer.resblocks.1.attn.in_proj_weight             of shape (2304, 768)
2023-12-07 16:52:02,423.423 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.1.attn.out_proj.bias              will be loaded from image_encoder.transformer.resblocks.1.attn.out_proj.bias              of shape (768,)
2023-12-07 16:52:02,423.423 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.1.attn.out_proj.weight            will be loaded from image_encoder.transformer.resblocks.1.attn.out_proj.weight            of shape (768, 768)
2023-12-07 16:52:02,423.423 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.1.ln_1.bias                       will be loaded from image_encoder.transformer.resblocks.1.ln_1.bias                       of shape (768,)
2023-12-07 16:52:02,423.423 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.1.ln_1.weight                     will be loaded from image_encoder.transformer.resblocks.1.ln_1.weight                     of shape (768,)
2023-12-07 16:52:02,423.423 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.1.ln_2.bias                       will be loaded from image_encoder.transformer.resblocks.1.ln_2.bias                       of shape (768,)
2023-12-07 16:52:02,423.423 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.1.ln_2.weight                     will be loaded from image_encoder.transformer.resblocks.1.ln_2.weight                     of shape (768,)
2023-12-07 16:52:02,423.423 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.1.mlp.c_fc.bias                   will be loaded from image_encoder.transformer.resblocks.1.mlp.c_fc.bias                   of shape (3072,)
2023-12-07 16:52:02,423.423 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.1.mlp.c_fc.weight                 will be loaded from image_encoder.transformer.resblocks.1.mlp.c_fc.weight                 of shape (3072, 768)
2023-12-07 16:52:02,424.424 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.1.mlp.c_proj.bias                 will be loaded from image_encoder.transformer.resblocks.1.mlp.c_proj.bias                 of shape (768,)
2023-12-07 16:52:02,424.424 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.1.mlp.c_proj.weight               will be loaded from image_encoder.transformer.resblocks.1.mlp.c_proj.weight               of shape (768, 3072)
2023-12-07 16:52:02,424.424 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.10.attn.in_proj_bias              will be loaded from image_encoder.transformer.resblocks.10.attn.in_proj_bias              of shape (2304,)
2023-12-07 16:52:02,424.424 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.10.attn.in_proj_weight            will be loaded from image_encoder.transformer.resblocks.10.attn.in_proj_weight            of shape (2304, 768)
2023-12-07 16:52:02,424.424 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.10.attn.out_proj.bias             will be loaded from image_encoder.transformer.resblocks.10.attn.out_proj.bias             of shape (768,)
2023-12-07 16:52:02,424.424 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.10.attn.out_proj.weight           will be loaded from image_encoder.transformer.resblocks.10.attn.out_proj.weight           of shape (768, 768)
2023-12-07 16:52:02,424.424 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.10.ln_1.bias                      will be loaded from image_encoder.transformer.resblocks.10.ln_1.bias                      of shape (768,)
2023-12-07 16:52:02,424.424 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.10.ln_1.weight                    will be loaded from image_encoder.transformer.resblocks.10.ln_1.weight                    of shape (768,)
2023-12-07 16:52:02,424.424 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.10.ln_2.bias                      will be loaded from image_encoder.transformer.resblocks.10.ln_2.bias                      of shape (768,)
2023-12-07 16:52:02,424.424 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.10.ln_2.weight                    will be loaded from image_encoder.transformer.resblocks.10.ln_2.weight                    of shape (768,)
2023-12-07 16:52:02,424.424 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.10.mlp.c_fc.bias                  will be loaded from image_encoder.transformer.resblocks.10.mlp.c_fc.bias                  of shape (3072,)
2023-12-07 16:52:02,424.424 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.10.mlp.c_fc.weight                will be loaded from image_encoder.transformer.resblocks.10.mlp.c_fc.weight                of shape (3072, 768)
2023-12-07 16:52:02,424.424 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.10.mlp.c_proj.bias                will be loaded from image_encoder.transformer.resblocks.10.mlp.c_proj.bias                of shape (768,)
2023-12-07 16:52:02,424.424 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.10.mlp.c_proj.weight              will be loaded from image_encoder.transformer.resblocks.10.mlp.c_proj.weight              of shape (768, 3072)
2023-12-07 16:52:02,424.424 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.11.attn.in_proj_bias              will be loaded from image_encoder.transformer.resblocks.11.attn.in_proj_bias              of shape (2304,)
2023-12-07 16:52:02,424.424 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.11.attn.in_proj_weight            will be loaded from image_encoder.transformer.resblocks.11.attn.in_proj_weight            of shape (2304, 768)
2023-12-07 16:52:02,424.424 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.11.attn.out_proj.bias             will be loaded from image_encoder.transformer.resblocks.11.attn.out_proj.bias             of shape (768,)
2023-12-07 16:52:02,424.424 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.11.attn.out_proj.weight           will be loaded from image_encoder.transformer.resblocks.11.attn.out_proj.weight           of shape (768, 768)
2023-12-07 16:52:02,424.424 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.11.ln_1.bias                      will be loaded from image_encoder.transformer.resblocks.11.ln_1.bias                      of shape (768,)
2023-12-07 16:52:02,424.424 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.11.ln_1.weight                    will be loaded from image_encoder.transformer.resblocks.11.ln_1.weight                    of shape (768,)
2023-12-07 16:52:02,424.424 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.11.ln_2.bias                      will be loaded from image_encoder.transformer.resblocks.11.ln_2.bias                      of shape (768,)
2023-12-07 16:52:02,424.424 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.11.ln_2.weight                    will be loaded from image_encoder.transformer.resblocks.11.ln_2.weight                    of shape (768,)
2023-12-07 16:52:02,425.425 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.11.mlp.c_fc.bias                  will be loaded from image_encoder.transformer.resblocks.11.mlp.c_fc.bias                  of shape (3072,)
2023-12-07 16:52:02,425.425 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.11.mlp.c_fc.weight                will be loaded from image_encoder.transformer.resblocks.11.mlp.c_fc.weight                of shape (3072, 768)
2023-12-07 16:52:02,425.425 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.11.mlp.c_proj.bias                will be loaded from image_encoder.transformer.resblocks.11.mlp.c_proj.bias                of shape (768,)
2023-12-07 16:52:02,425.425 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.11.mlp.c_proj.weight              will be loaded from image_encoder.transformer.resblocks.11.mlp.c_proj.weight              of shape (768, 3072)
2023-12-07 16:52:02,425.425 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.2.attn.in_proj_bias               will be loaded from image_encoder.transformer.resblocks.2.attn.in_proj_bias               of shape (2304,)
2023-12-07 16:52:02,425.425 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.2.attn.in_proj_weight             will be loaded from image_encoder.transformer.resblocks.2.attn.in_proj_weight             of shape (2304, 768)
2023-12-07 16:52:02,425.425 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.2.attn.out_proj.bias              will be loaded from image_encoder.transformer.resblocks.2.attn.out_proj.bias              of shape (768,)
2023-12-07 16:52:02,425.425 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.2.attn.out_proj.weight            will be loaded from image_encoder.transformer.resblocks.2.attn.out_proj.weight            of shape (768, 768)
2023-12-07 16:52:02,425.425 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.2.ln_1.bias                       will be loaded from image_encoder.transformer.resblocks.2.ln_1.bias                       of shape (768,)
2023-12-07 16:52:02,425.425 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.2.ln_1.weight                     will be loaded from image_encoder.transformer.resblocks.2.ln_1.weight                     of shape (768,)
2023-12-07 16:52:02,425.425 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.2.ln_2.bias                       will be loaded from image_encoder.transformer.resblocks.2.ln_2.bias                       of shape (768,)
2023-12-07 16:52:02,425.425 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.2.ln_2.weight                     will be loaded from image_encoder.transformer.resblocks.2.ln_2.weight                     of shape (768,)
2023-12-07 16:52:02,425.425 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.2.mlp.c_fc.bias                   will be loaded from image_encoder.transformer.resblocks.2.mlp.c_fc.bias                   of shape (3072,)
2023-12-07 16:52:02,425.425 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.2.mlp.c_fc.weight                 will be loaded from image_encoder.transformer.resblocks.2.mlp.c_fc.weight                 of shape (3072, 768)
2023-12-07 16:52:02,425.425 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.2.mlp.c_proj.bias                 will be loaded from image_encoder.transformer.resblocks.2.mlp.c_proj.bias                 of shape (768,)
2023-12-07 16:52:02,425.425 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.2.mlp.c_proj.weight               will be loaded from image_encoder.transformer.resblocks.2.mlp.c_proj.weight               of shape (768, 3072)
2023-12-07 16:52:02,425.425 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.3.attn.in_proj_bias               will be loaded from image_encoder.transformer.resblocks.3.attn.in_proj_bias               of shape (2304,)
2023-12-07 16:52:02,425.425 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.3.attn.in_proj_weight             will be loaded from image_encoder.transformer.resblocks.3.attn.in_proj_weight             of shape (2304, 768)
2023-12-07 16:52:02,425.425 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.3.attn.out_proj.bias              will be loaded from image_encoder.transformer.resblocks.3.attn.out_proj.bias              of shape (768,)
2023-12-07 16:52:02,425.425 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.3.attn.out_proj.weight            will be loaded from image_encoder.transformer.resblocks.3.attn.out_proj.weight            of shape (768, 768)
2023-12-07 16:52:02,425.425 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.3.ln_1.bias                       will be loaded from image_encoder.transformer.resblocks.3.ln_1.bias                       of shape (768,)
2023-12-07 16:52:02,425.425 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.3.ln_1.weight                     will be loaded from image_encoder.transformer.resblocks.3.ln_1.weight                     of shape (768,)
2023-12-07 16:52:02,426.426 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.3.ln_2.bias                       will be loaded from image_encoder.transformer.resblocks.3.ln_2.bias                       of shape (768,)
2023-12-07 16:52:02,426.426 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.3.ln_2.weight                     will be loaded from image_encoder.transformer.resblocks.3.ln_2.weight                     of shape (768,)
2023-12-07 16:52:02,426.426 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.3.mlp.c_fc.bias                   will be loaded from image_encoder.transformer.resblocks.3.mlp.c_fc.bias                   of shape (3072,)
2023-12-07 16:52:02,426.426 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.3.mlp.c_fc.weight                 will be loaded from image_encoder.transformer.resblocks.3.mlp.c_fc.weight                 of shape (3072, 768)
2023-12-07 16:52:02,426.426 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.3.mlp.c_proj.bias                 will be loaded from image_encoder.transformer.resblocks.3.mlp.c_proj.bias                 of shape (768,)
2023-12-07 16:52:02,426.426 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.3.mlp.c_proj.weight               will be loaded from image_encoder.transformer.resblocks.3.mlp.c_proj.weight               of shape (768, 3072)
2023-12-07 16:52:02,426.426 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.4.attn.in_proj_bias               will be loaded from image_encoder.transformer.resblocks.4.attn.in_proj_bias               of shape (2304,)
2023-12-07 16:52:02,426.426 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.4.attn.in_proj_weight             will be loaded from image_encoder.transformer.resblocks.4.attn.in_proj_weight             of shape (2304, 768)
2023-12-07 16:52:02,426.426 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.4.attn.out_proj.bias              will be loaded from image_encoder.transformer.resblocks.4.attn.out_proj.bias              of shape (768,)
2023-12-07 16:52:02,426.426 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.4.attn.out_proj.weight            will be loaded from image_encoder.transformer.resblocks.4.attn.out_proj.weight            of shape (768, 768)
2023-12-07 16:52:02,426.426 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.4.ln_1.bias                       will be loaded from image_encoder.transformer.resblocks.4.ln_1.bias                       of shape (768,)
2023-12-07 16:52:02,426.426 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.4.ln_1.weight                     will be loaded from image_encoder.transformer.resblocks.4.ln_1.weight                     of shape (768,)
2023-12-07 16:52:02,426.426 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.4.ln_2.bias                       will be loaded from image_encoder.transformer.resblocks.4.ln_2.bias                       of shape (768,)
2023-12-07 16:52:02,426.426 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.4.ln_2.weight                     will be loaded from image_encoder.transformer.resblocks.4.ln_2.weight                     of shape (768,)
2023-12-07 16:52:02,426.426 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.4.mlp.c_fc.bias                   will be loaded from image_encoder.transformer.resblocks.4.mlp.c_fc.bias                   of shape (3072,)
2023-12-07 16:52:02,426.426 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.4.mlp.c_fc.weight                 will be loaded from image_encoder.transformer.resblocks.4.mlp.c_fc.weight                 of shape (3072, 768)
2023-12-07 16:52:02,426.426 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.4.mlp.c_proj.bias                 will be loaded from image_encoder.transformer.resblocks.4.mlp.c_proj.bias                 of shape (768,)
2023-12-07 16:52:02,426.426 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.4.mlp.c_proj.weight               will be loaded from image_encoder.transformer.resblocks.4.mlp.c_proj.weight               of shape (768, 3072)
2023-12-07 16:52:02,426.426 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.5.attn.in_proj_bias               will be loaded from image_encoder.transformer.resblocks.5.attn.in_proj_bias               of shape (2304,)
2023-12-07 16:52:02,426.426 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.5.attn.in_proj_weight             will be loaded from image_encoder.transformer.resblocks.5.attn.in_proj_weight             of shape (2304, 768)
2023-12-07 16:52:02,426.426 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.5.attn.out_proj.bias              will be loaded from image_encoder.transformer.resblocks.5.attn.out_proj.bias              of shape (768,)
2023-12-07 16:52:02,427.427 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.5.attn.out_proj.weight            will be loaded from image_encoder.transformer.resblocks.5.attn.out_proj.weight            of shape (768, 768)
2023-12-07 16:52:02,427.427 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.5.ln_1.bias                       will be loaded from image_encoder.transformer.resblocks.5.ln_1.bias                       of shape (768,)
2023-12-07 16:52:02,427.427 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.5.ln_1.weight                     will be loaded from image_encoder.transformer.resblocks.5.ln_1.weight                     of shape (768,)
2023-12-07 16:52:02,427.427 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.5.ln_2.bias                       will be loaded from image_encoder.transformer.resblocks.5.ln_2.bias                       of shape (768,)
2023-12-07 16:52:02,427.427 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.5.ln_2.weight                     will be loaded from image_encoder.transformer.resblocks.5.ln_2.weight                     of shape (768,)
2023-12-07 16:52:02,427.427 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.5.mlp.c_fc.bias                   will be loaded from image_encoder.transformer.resblocks.5.mlp.c_fc.bias                   of shape (3072,)
2023-12-07 16:52:02,427.427 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.5.mlp.c_fc.weight                 will be loaded from image_encoder.transformer.resblocks.5.mlp.c_fc.weight                 of shape (3072, 768)
2023-12-07 16:52:02,427.427 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.5.mlp.c_proj.bias                 will be loaded from image_encoder.transformer.resblocks.5.mlp.c_proj.bias                 of shape (768,)
2023-12-07 16:52:02,427.427 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.5.mlp.c_proj.weight               will be loaded from image_encoder.transformer.resblocks.5.mlp.c_proj.weight               of shape (768, 3072)
2023-12-07 16:52:02,427.427 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.6.attn.in_proj_bias               will be loaded from image_encoder.transformer.resblocks.6.attn.in_proj_bias               of shape (2304,)
2023-12-07 16:52:02,427.427 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.6.attn.in_proj_weight             will be loaded from image_encoder.transformer.resblocks.6.attn.in_proj_weight             of shape (2304, 768)
2023-12-07 16:52:02,427.427 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.6.attn.out_proj.bias              will be loaded from image_encoder.transformer.resblocks.6.attn.out_proj.bias              of shape (768,)
2023-12-07 16:52:02,427.427 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.6.attn.out_proj.weight            will be loaded from image_encoder.transformer.resblocks.6.attn.out_proj.weight            of shape (768, 768)
2023-12-07 16:52:02,427.427 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.6.ln_1.bias                       will be loaded from image_encoder.transformer.resblocks.6.ln_1.bias                       of shape (768,)
2023-12-07 16:52:02,427.427 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.6.ln_1.weight                     will be loaded from image_encoder.transformer.resblocks.6.ln_1.weight                     of shape (768,)
2023-12-07 16:52:02,427.427 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.6.ln_2.bias                       will be loaded from image_encoder.transformer.resblocks.6.ln_2.bias                       of shape (768,)
2023-12-07 16:52:02,427.427 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.6.ln_2.weight                     will be loaded from image_encoder.transformer.resblocks.6.ln_2.weight                     of shape (768,)
2023-12-07 16:52:02,427.427 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.6.mlp.c_fc.bias                   will be loaded from image_encoder.transformer.resblocks.6.mlp.c_fc.bias                   of shape (3072,)
2023-12-07 16:52:02,427.427 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.6.mlp.c_fc.weight                 will be loaded from image_encoder.transformer.resblocks.6.mlp.c_fc.weight                 of shape (3072, 768)
2023-12-07 16:52:02,427.427 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.6.mlp.c_proj.bias                 will be loaded from image_encoder.transformer.resblocks.6.mlp.c_proj.bias                 of shape (768,)
2023-12-07 16:52:02,427.427 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.6.mlp.c_proj.weight               will be loaded from image_encoder.transformer.resblocks.6.mlp.c_proj.weight               of shape (768, 3072)
2023-12-07 16:52:02,427.427 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.7.attn.in_proj_bias               will be loaded from image_encoder.transformer.resblocks.7.attn.in_proj_bias               of shape (2304,)
2023-12-07 16:52:02,428.428 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.7.attn.in_proj_weight             will be loaded from image_encoder.transformer.resblocks.7.attn.in_proj_weight             of shape (2304, 768)
2023-12-07 16:52:02,428.428 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.7.attn.out_proj.bias              will be loaded from image_encoder.transformer.resblocks.7.attn.out_proj.bias              of shape (768,)
2023-12-07 16:52:02,428.428 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.7.attn.out_proj.weight            will be loaded from image_encoder.transformer.resblocks.7.attn.out_proj.weight            of shape (768, 768)
2023-12-07 16:52:02,428.428 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.7.ln_1.bias                       will be loaded from image_encoder.transformer.resblocks.7.ln_1.bias                       of shape (768,)
2023-12-07 16:52:02,428.428 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.7.ln_1.weight                     will be loaded from image_encoder.transformer.resblocks.7.ln_1.weight                     of shape (768,)
2023-12-07 16:52:02,428.428 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.7.ln_2.bias                       will be loaded from image_encoder.transformer.resblocks.7.ln_2.bias                       of shape (768,)
2023-12-07 16:52:02,428.428 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.7.ln_2.weight                     will be loaded from image_encoder.transformer.resblocks.7.ln_2.weight                     of shape (768,)
2023-12-07 16:52:02,428.428 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.7.mlp.c_fc.bias                   will be loaded from image_encoder.transformer.resblocks.7.mlp.c_fc.bias                   of shape (3072,)
2023-12-07 16:52:02,428.428 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.7.mlp.c_fc.weight                 will be loaded from image_encoder.transformer.resblocks.7.mlp.c_fc.weight                 of shape (3072, 768)
2023-12-07 16:52:02,428.428 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.7.mlp.c_proj.bias                 will be loaded from image_encoder.transformer.resblocks.7.mlp.c_proj.bias                 of shape (768,)
2023-12-07 16:52:02,428.428 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.7.mlp.c_proj.weight               will be loaded from image_encoder.transformer.resblocks.7.mlp.c_proj.weight               of shape (768, 3072)
2023-12-07 16:52:02,428.428 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.8.attn.in_proj_bias               will be loaded from image_encoder.transformer.resblocks.8.attn.in_proj_bias               of shape (2304,)
2023-12-07 16:52:02,428.428 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.8.attn.in_proj_weight             will be loaded from image_encoder.transformer.resblocks.8.attn.in_proj_weight             of shape (2304, 768)
2023-12-07 16:52:02,428.428 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.8.attn.out_proj.bias              will be loaded from image_encoder.transformer.resblocks.8.attn.out_proj.bias              of shape (768,)
2023-12-07 16:52:02,428.428 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.8.attn.out_proj.weight            will be loaded from image_encoder.transformer.resblocks.8.attn.out_proj.weight            of shape (768, 768)
2023-12-07 16:52:02,428.428 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.8.ln_1.bias                       will be loaded from image_encoder.transformer.resblocks.8.ln_1.bias                       of shape (768,)
2023-12-07 16:52:02,428.428 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.8.ln_1.weight                     will be loaded from image_encoder.transformer.resblocks.8.ln_1.weight                     of shape (768,)
2023-12-07 16:52:02,428.428 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.8.ln_2.bias                       will be loaded from image_encoder.transformer.resblocks.8.ln_2.bias                       of shape (768,)
2023-12-07 16:52:02,428.428 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.8.ln_2.weight                     will be loaded from image_encoder.transformer.resblocks.8.ln_2.weight                     of shape (768,)
2023-12-07 16:52:02,428.428 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.8.mlp.c_fc.bias                   will be loaded from image_encoder.transformer.resblocks.8.mlp.c_fc.bias                   of shape (3072,)
2023-12-07 16:52:02,428.428 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.8.mlp.c_fc.weight                 will be loaded from image_encoder.transformer.resblocks.8.mlp.c_fc.weight                 of shape (3072, 768)
2023-12-07 16:52:02,428.428 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.8.mlp.c_proj.bias                 will be loaded from image_encoder.transformer.resblocks.8.mlp.c_proj.bias                 of shape (768,)
2023-12-07 16:52:02,429.429 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.8.mlp.c_proj.weight               will be loaded from image_encoder.transformer.resblocks.8.mlp.c_proj.weight               of shape (768, 3072)
2023-12-07 16:52:02,429.429 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.9.attn.in_proj_bias               will be loaded from image_encoder.transformer.resblocks.9.attn.in_proj_bias               of shape (2304,)
2023-12-07 16:52:02,429.429 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.9.attn.in_proj_weight             will be loaded from image_encoder.transformer.resblocks.9.attn.in_proj_weight             of shape (2304, 768)
2023-12-07 16:52:02,429.429 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.9.attn.out_proj.bias              will be loaded from image_encoder.transformer.resblocks.9.attn.out_proj.bias              of shape (768,)
2023-12-07 16:52:02,429.429 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.9.attn.out_proj.weight            will be loaded from image_encoder.transformer.resblocks.9.attn.out_proj.weight            of shape (768, 768)
2023-12-07 16:52:02,429.429 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.9.ln_1.bias                       will be loaded from image_encoder.transformer.resblocks.9.ln_1.bias                       of shape (768,)
2023-12-07 16:52:02,429.429 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.9.ln_1.weight                     will be loaded from image_encoder.transformer.resblocks.9.ln_1.weight                     of shape (768,)
2023-12-07 16:52:02,429.429 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.9.ln_2.bias                       will be loaded from image_encoder.transformer.resblocks.9.ln_2.bias                       of shape (768,)
2023-12-07 16:52:02,429.429 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.9.ln_2.weight                     will be loaded from image_encoder.transformer.resblocks.9.ln_2.weight                     of shape (768,)
2023-12-07 16:52:02,429.429 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.9.mlp.c_fc.bias                   will be loaded from image_encoder.transformer.resblocks.9.mlp.c_fc.bias                   of shape (3072,)
2023-12-07 16:52:02,429.429 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.9.mlp.c_fc.weight                 will be loaded from image_encoder.transformer.resblocks.9.mlp.c_fc.weight                 of shape (3072, 768)
2023-12-07 16:52:02,429.429 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.9.mlp.c_proj.bias                 will be loaded from image_encoder.transformer.resblocks.9.mlp.c_proj.bias                 of shape (768,)
2023-12-07 16:52:02,429.429 9027:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.9.mlp.c_proj.weight               will be loaded from image_encoder.transformer.resblocks.9.mlp.c_proj.weight               of shape (768, 3072)
2023-12-07 16:52:02,429.429 9027:torch_common.py:128 align_and_update_state_dicts(): img_temperal_embedding.0                                              will be loaded from img_temperal_embedding.0                                              of shape (1, 1, 768)
2023-12-07 16:52:02,429.429 9027:torch_common.py:128 align_and_update_state_dicts(): img_temperal_embedding.1                                              will be loaded from img_temperal_embedding.1                                              of shape (1, 1, 768)
2023-12-07 16:52:02,429.429 9027:torch_common.py:128 align_and_update_state_dicts(): img_temperal_embedding.2                                              will be loaded from img_temperal_embedding.2                                              of shape (1, 1, 768)
2023-12-07 16:52:02,429.429 9027:torch_common.py:128 align_and_update_state_dicts(): img_temperal_embedding.3                                              will be loaded from img_temperal_embedding.3                                              of shape (1, 1, 768)
2023-12-07 16:52:02,429.429 9027:torch_common.py:128 align_and_update_state_dicts(): img_temperal_embedding.4                                              will be loaded from img_temperal_embedding.4                                              of shape (1, 1, 768)
2023-12-07 16:52:02,429.429 9027:torch_common.py:128 align_and_update_state_dicts(): img_temperal_embedding.5                                              will be loaded from img_temperal_embedding.5                                              of shape (1, 1, 768)
2023-12-07 16:52:02,429.429 9027:torch_common.py:128 align_and_update_state_dicts(): textual.embedding.layer_norm.bias                                     will be loaded from textual.embedding.layer_norm.bias                                     of shape (768,)
2023-12-07 16:52:02,429.429 9027:torch_common.py:128 align_and_update_state_dicts(): textual.embedding.layer_norm.weight                                   will be loaded from textual.embedding.layer_norm.weight                                   of shape (768,)
2023-12-07 16:52:02,430.430 9027:torch_common.py:128 align_and_update_state_dicts(): textual.embedding.positions.weight                                    will be loaded from textual.embedding.positions.weight                                    of shape (1024, 768)
2023-12-07 16:52:02,430.430 9027:torch_common.py:128 align_and_update_state_dicts(): textual.embedding.words.weight                                        will be loaded from textual.embedding.words.weight                                        of shape (30522, 768)
2023-12-07 16:52:02,430.430 9027:torch_common.py:128 align_and_update_state_dicts(): textual.output.bias                                                   will be loaded from textual.output.bias                                                   of shape (30522,)
2023-12-07 16:52:02,430.430 9027:torch_common.py:128 align_and_update_state_dicts(): textual.output.weight                                                 will be loaded from textual.output.weight                                                 of shape (30522, 768)
2023-12-07 16:52:02,430.430 9027:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.attention.output.LayerNorm.bias   will be loaded from textual.transformer.encoder.layer.0.attention.output.LayerNorm.bias   of shape (768,)
2023-12-07 16:52:02,430.430 9027:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.attention.output.LayerNorm.weight will be loaded from textual.transformer.encoder.layer.0.attention.output.LayerNorm.weight of shape (768,)
2023-12-07 16:52:02,430.430 9027:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.attention.output.dense.bias       will be loaded from textual.transformer.encoder.layer.0.attention.output.dense.bias       of shape (768,)
2023-12-07 16:52:02,430.430 9027:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.attention.output.dense.weight     will be loaded from textual.transformer.encoder.layer.0.attention.output.dense.weight     of shape (768, 768)
2023-12-07 16:52:02,430.430 9027:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.attention.self.key.bias           will be loaded from textual.transformer.encoder.layer.0.attention.self.key.bias           of shape (768,)
2023-12-07 16:52:02,430.430 9027:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.attention.self.key.weight         will be loaded from textual.transformer.encoder.layer.0.attention.self.key.weight         of shape (768, 768)
2023-12-07 16:52:02,430.430 9027:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.attention.self.query.bias         will be loaded from textual.transformer.encoder.layer.0.attention.self.query.bias         of shape (768,)
2023-12-07 16:52:02,430.430 9027:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.attention.self.query.weight       will be loaded from textual.transformer.encoder.layer.0.attention.self.query.weight       of shape (768, 768)
2023-12-07 16:52:02,430.430 9027:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.attention.self.value.bias         will be loaded from textual.transformer.encoder.layer.0.attention.self.value.bias         of shape (768,)
2023-12-07 16:52:02,430.430 9027:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.attention.self.value.weight       will be loaded from textual.transformer.encoder.layer.0.attention.self.value.weight       of shape (768, 768)
2023-12-07 16:52:02,430.430 9027:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.intermediate.dense.bias           will be loaded from textual.transformer.encoder.layer.0.intermediate.dense.bias           of shape (3072,)
2023-12-07 16:52:02,430.430 9027:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.intermediate.dense.weight         will be loaded from textual.transformer.encoder.layer.0.intermediate.dense.weight         of shape (3072, 768)
2023-12-07 16:52:02,430.430 9027:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.output.LayerNorm.bias             will be loaded from textual.transformer.encoder.layer.0.output.LayerNorm.bias             of shape (768,)
2023-12-07 16:52:02,430.430 9027:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.output.LayerNorm.weight           will be loaded from textual.transformer.encoder.layer.0.output.LayerNorm.weight           of shape (768,)
2023-12-07 16:52:02,430.430 9027:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.output.dense.bias                 will be loaded from textual.transformer.encoder.layer.0.output.dense.bias                 of shape (768,)
2023-12-07 16:52:02,430.430 9027:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.output.dense.weight               will be loaded from textual.transformer.encoder.layer.0.output.dense.weight               of shape (768, 3072)
2023-12-07 16:52:02,430.430 9027:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.attention.output.LayerNorm.bias   will be loaded from textual.transformer.encoder.layer.1.attention.output.LayerNorm.bias   of shape (768,)
2023-12-07 16:52:02,431.431 9027:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.attention.output.LayerNorm.weight will be loaded from textual.transformer.encoder.layer.1.attention.output.LayerNorm.weight of shape (768,)
2023-12-07 16:52:02,431.431 9027:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.attention.output.dense.bias       will be loaded from textual.transformer.encoder.layer.1.attention.output.dense.bias       of shape (768,)
2023-12-07 16:52:02,431.431 9027:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.attention.output.dense.weight     will be loaded from textual.transformer.encoder.layer.1.attention.output.dense.weight     of shape (768, 768)
2023-12-07 16:52:02,431.431 9027:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.attention.self.key.bias           will be loaded from textual.transformer.encoder.layer.1.attention.self.key.bias           of shape (768,)
2023-12-07 16:52:02,431.431 9027:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.attention.self.key.weight         will be loaded from textual.transformer.encoder.layer.1.attention.self.key.weight         of shape (768, 768)
2023-12-07 16:52:02,431.431 9027:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.attention.self.query.bias         will be loaded from textual.transformer.encoder.layer.1.attention.self.query.bias         of shape (768,)
2023-12-07 16:52:02,431.431 9027:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.attention.self.query.weight       will be loaded from textual.transformer.encoder.layer.1.attention.self.query.weight       of shape (768, 768)
2023-12-07 16:52:02,431.431 9027:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.attention.self.value.bias         will be loaded from textual.transformer.encoder.layer.1.attention.self.value.bias         of shape (768,)
2023-12-07 16:52:02,431.431 9027:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.attention.self.value.weight       will be loaded from textual.transformer.encoder.layer.1.attention.self.value.weight       of shape (768, 768)
2023-12-07 16:52:02,431.431 9027:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.intermediate.dense.bias           will be loaded from textual.transformer.encoder.layer.1.intermediate.dense.bias           of shape (3072,)
2023-12-07 16:52:02,431.431 9027:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.intermediate.dense.weight         will be loaded from textual.transformer.encoder.layer.1.intermediate.dense.weight         of shape (3072, 768)
2023-12-07 16:52:02,431.431 9027:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.output.LayerNorm.bias             will be loaded from textual.transformer.encoder.layer.1.output.LayerNorm.bias             of shape (768,)
2023-12-07 16:52:02,431.431 9027:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.output.LayerNorm.weight           will be loaded from textual.transformer.encoder.layer.1.output.LayerNorm.weight           of shape (768,)
2023-12-07 16:52:02,431.431 9027:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.output.dense.bias                 will be loaded from textual.transformer.encoder.layer.1.output.dense.bias                 of shape (768,)
2023-12-07 16:52:02,431.431 9027:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.output.dense.weight               will be loaded from textual.transformer.encoder.layer.1.output.dense.weight               of shape (768, 3072)
2023-12-07 16:52:02,431.431 9027:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.attention.output.LayerNorm.bias   will be loaded from textual.transformer.encoder.layer.2.attention.output.LayerNorm.bias   of shape (768,)
2023-12-07 16:52:02,431.431 9027:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.attention.output.LayerNorm.weight will be loaded from textual.transformer.encoder.layer.2.attention.output.LayerNorm.weight of shape (768,)
2023-12-07 16:52:02,431.431 9027:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.attention.output.dense.bias       will be loaded from textual.transformer.encoder.layer.2.attention.output.dense.bias       of shape (768,)
2023-12-07 16:52:02,431.431 9027:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.attention.output.dense.weight     will be loaded from textual.transformer.encoder.layer.2.attention.output.dense.weight     of shape (768, 768)
2023-12-07 16:52:02,431.431 9027:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.attention.self.key.bias           will be loaded from textual.transformer.encoder.layer.2.attention.self.key.bias           of shape (768,)
2023-12-07 16:52:02,431.431 9027:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.attention.self.key.weight         will be loaded from textual.transformer.encoder.layer.2.attention.self.key.weight         of shape (768, 768)
2023-12-07 16:52:02,431.431 9027:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.attention.self.query.bias         will be loaded from textual.transformer.encoder.layer.2.attention.self.query.bias         of shape (768,)
2023-12-07 16:52:02,432.432 9027:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.attention.self.query.weight       will be loaded from textual.transformer.encoder.layer.2.attention.self.query.weight       of shape (768, 768)
2023-12-07 16:52:02,432.432 9027:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.attention.self.value.bias         will be loaded from textual.transformer.encoder.layer.2.attention.self.value.bias         of shape (768,)
2023-12-07 16:52:02,432.432 9027:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.attention.self.value.weight       will be loaded from textual.transformer.encoder.layer.2.attention.self.value.weight       of shape (768, 768)
2023-12-07 16:52:02,432.432 9027:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.intermediate.dense.bias           will be loaded from textual.transformer.encoder.layer.2.intermediate.dense.bias           of shape (3072,)
2023-12-07 16:52:02,432.432 9027:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.intermediate.dense.weight         will be loaded from textual.transformer.encoder.layer.2.intermediate.dense.weight         of shape (3072, 768)
2023-12-07 16:52:02,432.432 9027:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.output.LayerNorm.bias             will be loaded from textual.transformer.encoder.layer.2.output.LayerNorm.bias             of shape (768,)
2023-12-07 16:52:02,432.432 9027:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.output.LayerNorm.weight           will be loaded from textual.transformer.encoder.layer.2.output.LayerNorm.weight           of shape (768,)
2023-12-07 16:52:02,432.432 9027:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.output.dense.bias                 will be loaded from textual.transformer.encoder.layer.2.output.dense.bias                 of shape (768,)
2023-12-07 16:52:02,432.432 9027:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.output.dense.weight               will be loaded from textual.transformer.encoder.layer.2.output.dense.weight               of shape (768, 3072)
2023-12-07 16:52:02,432.432 9027:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.attention.output.LayerNorm.bias   will be loaded from textual.transformer.encoder.layer.3.attention.output.LayerNorm.bias   of shape (768,)
2023-12-07 16:52:02,432.432 9027:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.attention.output.LayerNorm.weight will be loaded from textual.transformer.encoder.layer.3.attention.output.LayerNorm.weight of shape (768,)
2023-12-07 16:52:02,432.432 9027:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.attention.output.dense.bias       will be loaded from textual.transformer.encoder.layer.3.attention.output.dense.bias       of shape (768,)
2023-12-07 16:52:02,432.432 9027:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.attention.output.dense.weight     will be loaded from textual.transformer.encoder.layer.3.attention.output.dense.weight     of shape (768, 768)
2023-12-07 16:52:02,433.433 9027:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.attention.self.key.bias           will be loaded from textual.transformer.encoder.layer.3.attention.self.key.bias           of shape (768,)
2023-12-07 16:52:02,433.433 9027:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.attention.self.key.weight         will be loaded from textual.transformer.encoder.layer.3.attention.self.key.weight         of shape (768, 768)
2023-12-07 16:52:02,433.433 9027:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.attention.self.query.bias         will be loaded from textual.transformer.encoder.layer.3.attention.self.query.bias         of shape (768,)
2023-12-07 16:52:02,433.433 9027:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.attention.self.query.weight       will be loaded from textual.transformer.encoder.layer.3.attention.self.query.weight       of shape (768, 768)
2023-12-07 16:52:02,433.433 9027:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.attention.self.value.bias         will be loaded from textual.transformer.encoder.layer.3.attention.self.value.bias         of shape (768,)
2023-12-07 16:52:02,433.433 9027:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.attention.self.value.weight       will be loaded from textual.transformer.encoder.layer.3.attention.self.value.weight       of shape (768, 768)
2023-12-07 16:52:02,433.433 9027:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.intermediate.dense.bias           will be loaded from textual.transformer.encoder.layer.3.intermediate.dense.bias           of shape (3072,)
2023-12-07 16:52:02,433.433 9027:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.intermediate.dense.weight         will be loaded from textual.transformer.encoder.layer.3.intermediate.dense.weight         of shape (3072, 768)
2023-12-07 16:52:02,433.433 9027:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.output.LayerNorm.bias             will be loaded from textual.transformer.encoder.layer.3.output.LayerNorm.bias             of shape (768,)
2023-12-07 16:52:02,434.434 9027:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.output.LayerNorm.weight           will be loaded from textual.transformer.encoder.layer.3.output.LayerNorm.weight           of shape (768,)
2023-12-07 16:52:02,434.434 9027:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.output.dense.bias                 will be loaded from textual.transformer.encoder.layer.3.output.dense.bias                 of shape (768,)
2023-12-07 16:52:02,434.434 9027:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.output.dense.weight               will be loaded from textual.transformer.encoder.layer.3.output.dense.weight               of shape (768, 3072)
2023-12-07 16:52:02,434.434 9027:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.attention.output.LayerNorm.bias   will be loaded from textual.transformer.encoder.layer.4.attention.output.LayerNorm.bias   of shape (768,)
2023-12-07 16:52:02,434.434 9027:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.attention.output.LayerNorm.weight will be loaded from textual.transformer.encoder.layer.4.attention.output.LayerNorm.weight of shape (768,)
2023-12-07 16:52:02,434.434 9027:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.attention.output.dense.bias       will be loaded from textual.transformer.encoder.layer.4.attention.output.dense.bias       of shape (768,)
2023-12-07 16:52:02,434.434 9027:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.attention.output.dense.weight     will be loaded from textual.transformer.encoder.layer.4.attention.output.dense.weight     of shape (768, 768)
2023-12-07 16:52:02,434.434 9027:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.attention.self.key.bias           will be loaded from textual.transformer.encoder.layer.4.attention.self.key.bias           of shape (768,)
2023-12-07 16:52:02,434.434 9027:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.attention.self.key.weight         will be loaded from textual.transformer.encoder.layer.4.attention.self.key.weight         of shape (768, 768)
2023-12-07 16:52:02,434.434 9027:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.attention.self.query.bias         will be loaded from textual.transformer.encoder.layer.4.attention.self.query.bias         of shape (768,)
2023-12-07 16:52:02,434.434 9027:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.attention.self.query.weight       will be loaded from textual.transformer.encoder.layer.4.attention.self.query.weight       of shape (768, 768)
2023-12-07 16:52:02,434.434 9027:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.attention.self.value.bias         will be loaded from textual.transformer.encoder.layer.4.attention.self.value.bias         of shape (768,)
2023-12-07 16:52:02,434.434 9027:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.attention.self.value.weight       will be loaded from textual.transformer.encoder.layer.4.attention.self.value.weight       of shape (768, 768)
2023-12-07 16:52:02,434.434 9027:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.intermediate.dense.bias           will be loaded from textual.transformer.encoder.layer.4.intermediate.dense.bias           of shape (3072,)
2023-12-07 16:52:02,434.434 9027:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.intermediate.dense.weight         will be loaded from textual.transformer.encoder.layer.4.intermediate.dense.weight         of shape (3072, 768)
2023-12-07 16:52:02,434.434 9027:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.output.LayerNorm.bias             will be loaded from textual.transformer.encoder.layer.4.output.LayerNorm.bias             of shape (768,)
2023-12-07 16:52:02,434.434 9027:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.output.LayerNorm.weight           will be loaded from textual.transformer.encoder.layer.4.output.LayerNorm.weight           of shape (768,)
2023-12-07 16:52:02,434.434 9027:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.output.dense.bias                 will be loaded from textual.transformer.encoder.layer.4.output.dense.bias                 of shape (768,)
2023-12-07 16:52:02,434.434 9027:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.output.dense.weight               will be loaded from textual.transformer.encoder.layer.4.output.dense.weight               of shape (768, 3072)
2023-12-07 16:52:02,434.434 9027:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.attention.output.LayerNorm.bias   will be loaded from textual.transformer.encoder.layer.5.attention.output.LayerNorm.bias   of shape (768,)
2023-12-07 16:52:02,434.434 9027:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.attention.output.LayerNorm.weight will be loaded from textual.transformer.encoder.layer.5.attention.output.LayerNorm.weight of shape (768,)
2023-12-07 16:52:02,434.434 9027:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.attention.output.dense.bias       will be loaded from textual.transformer.encoder.layer.5.attention.output.dense.bias       of shape (768,)
2023-12-07 16:52:02,435.435 9027:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.attention.output.dense.weight     will be loaded from textual.transformer.encoder.layer.5.attention.output.dense.weight     of shape (768, 768)
2023-12-07 16:52:02,435.435 9027:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.attention.self.key.bias           will be loaded from textual.transformer.encoder.layer.5.attention.self.key.bias           of shape (768,)
2023-12-07 16:52:02,435.435 9027:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.attention.self.key.weight         will be loaded from textual.transformer.encoder.layer.5.attention.self.key.weight         of shape (768, 768)
2023-12-07 16:52:02,435.435 9027:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.attention.self.query.bias         will be loaded from textual.transformer.encoder.layer.5.attention.self.query.bias         of shape (768,)
2023-12-07 16:52:02,435.435 9027:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.attention.self.query.weight       will be loaded from textual.transformer.encoder.layer.5.attention.self.query.weight       of shape (768, 768)
2023-12-07 16:52:02,435.435 9027:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.attention.self.value.bias         will be loaded from textual.transformer.encoder.layer.5.attention.self.value.bias         of shape (768,)
2023-12-07 16:52:02,435.435 9027:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.attention.self.value.weight       will be loaded from textual.transformer.encoder.layer.5.attention.self.value.weight       of shape (768, 768)
2023-12-07 16:52:02,435.435 9027:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.intermediate.dense.bias           will be loaded from textual.transformer.encoder.layer.5.intermediate.dense.bias           of shape (3072,)
2023-12-07 16:52:02,435.435 9027:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.intermediate.dense.weight         will be loaded from textual.transformer.encoder.layer.5.intermediate.dense.weight         of shape (3072, 768)
2023-12-07 16:52:02,435.435 9027:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.output.LayerNorm.bias             will be loaded from textual.transformer.encoder.layer.5.output.LayerNorm.bias             of shape (768,)
2023-12-07 16:52:02,435.435 9027:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.output.LayerNorm.weight           will be loaded from textual.transformer.encoder.layer.5.output.LayerNorm.weight           of shape (768,)
2023-12-07 16:52:02,435.435 9027:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.output.dense.bias                 will be loaded from textual.transformer.encoder.layer.5.output.dense.bias                 of shape (768,)
2023-12-07 16:52:02,435.435 9027:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.output.dense.weight               will be loaded from textual.transformer.encoder.layer.5.output.dense.weight               of shape (768, 3072)
2023-12-07 16:52:02,435.435 9027:torch_common.py:128 align_and_update_state_dicts(): textual.visual_projection.0.bias                                      will be loaded from textual.visual_projection.0.bias                                      of shape (768,)
2023-12-07 16:52:02,435.435 9027:torch_common.py:128 align_and_update_state_dicts(): textual.visual_projection.0.weight                                    will be loaded from textual.visual_projection.0.weight                                    of shape (768, 768)
2023-12-07 16:52:02,435.435 9027:torch_common.py:128 align_and_update_state_dicts(): textual.visual_projection.1.bias                                      will be loaded from textual.visual_projection.1.bias                                      of shape (768,)
2023-12-07 16:52:02,435.435 9027:torch_common.py:128 align_and_update_state_dicts(): textual.visual_projection.1.weight                                    will be loaded from textual.visual_projection.1.weight                                    of shape (768,)
2023-12-07 16:52:02,435.435 9027:torch_common.py:137 align_and_update_state_dicts(): target model param = 264; name matched = 264; loaded = 264
2023-12-07 16:52:02,435.435 9027:torch_common.py:140 align_and_update_state_dicts(): from loaded; ignore = []
2023-12-07 16:52:02,437.437 9027:torch_common.py:82 load_model_state_ignore_mismatch(): unique keys in init dict = ['textual.output.weight']; total = 1
2023-12-07 16:52:02,516.516 9027:torch_common.py:86 load_model_state_ignore_mismatch(): unique key (not initialized) in current model = ['textual.output.weight']
/home/paperspace/Desktop/GenerativeImage2Text/generativeimage2text/layers/decoder.py:1198: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  beam_id = idx // vocab_size
2023-12-07 16:52:29,708.708 9027:inference.py:109 test_git_inference_single_image(): output: a large body of water
]0;paperspace@psybffhvzv5t: ~/Desktop/GenerativeImage2Text[01;32mpaperspace@psybffhvzv5t[00m:[01;34m~/Desktop/GenerativeImage2Text[00m$ AZFUSE_TSV_USE_FUSE=1 python -m generativeimage2text.inference -p "{'type': 'test_git_inference_single_image', \
> 'image_path': ['aux_data/images/1.jpg', 'aux_data/images/1.jpg', 'aux_data/images/1.jpg', 'aux_data/images/1.jpg', 'aux_data/images/1.jpg', 'aux_data/images/1.jpg'], \
> 'model_name': 'GIT_BASE_MSRVTT_QA', \
> 'prefix': 'what is it?', \
> }"
2023-12-07 16:53:08,879.879 9392:inference.py:318   <module>(): param:
{'image_path': ['aux_data/images/1.jpg',
                'aux_data/images/1.jpg',
                'aux_data/images/1.jpg',
                'aux_data/images/1.jpg',
                'aux_data/images/1.jpg',
                'aux_data/images/1.jpg'],
 'model_name': 'GIT_BASE_MSRVTT_QA',
 'prefix': 'what is it?',
 'type': 'test_git_inference_single_image'}
/home/paperspace/Desktop/GenerativeImage2Text/generativeimage2text/inference.py:119: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.
  Resize(crop_size, interpolation=Image.BICUBIC),
/home/paperspace/.local/lib/python3.9/site-packages/torchvision/transforms/transforms.py:332: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.
  warnings.warn(
2023-12-07 16:53:13,992.992 9392:decoder.py:832   __init__(): creating temperal embedding
2023-12-07 16:53:14,759.759 9392:cloud_storage.py:1150 az_download_once(): if sync, no need to save to temp first
2023-12-07 16:53:14,759.759 9392:common.py:324    cmd_run(): start to cmd run: azcopy cp https://publicgit.blob.core.windows.net/data/output/GIT_BASE_MSRVTT_QA/snapshot/model.pt /tmp/publicgit/output/GIT_BASE_MSRVTT_QA/snapshot/model.pt
INFO: Scanning...
INFO: Autologin not specified.
INFO: Any empty folders will not be processed, because source and/or destination doesn't have full folder support

Job 0a53508e-099c-7c43-61a5-a72bc13ea6fc has started
Log file is located at: /home/paperspace/.azcopy/0a53508e-099c-7c43-61a5-a72bc13ea6fc.log

59.0 %, 0 Done, 0 Failed, 1 Pending, 0 Skipped, 1 Total, 2-sec Throughput (Mb/s): 1228.3673100.0 %, 1 Done, 0 Failed, 0 Pending, 0 Skipped, 1 Total,                                  


Job 0a53508e-099c-7c43-61a5-a72bc13ea6fc summary
Elapsed Time (Minutes): 0.0667
Number of File Transfers: 1
Number of Folder Property Transfers: 0
Number of Symlink Transfers: 0
Total Number of Transfers: 1
Number of File Transfers Completed: 1
Number of Folder Transfers Completed: 0
Number of File Transfers Failed: 0
Number of Folder Transfers Failed: 0
Number of File Transfers Skipped: 0
Number of Folder Transfers Skipped: 0
TotalBytesTransferred: 307204455
Final Job Status: Completed

2023-12-07 16:53:20,266.266 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.class_embedding                                         will be loaded from image_encoder.class_embedding                                         of shape (768,)
2023-12-07 16:53:20,266.266 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.conv1.weight                                            will be loaded from image_encoder.conv1.weight                                            of shape (768, 3, 16, 16)
2023-12-07 16:53:20,266.266 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.ln_post.bias                                            will be loaded from image_encoder.ln_post.bias                                            of shape (768,)
2023-12-07 16:53:20,267.267 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.ln_post.weight                                          will be loaded from image_encoder.ln_post.weight                                          of shape (768,)
2023-12-07 16:53:20,267.267 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.ln_pre.bias                                             will be loaded from image_encoder.ln_pre.bias                                             of shape (768,)
2023-12-07 16:53:20,267.267 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.ln_pre.weight                                           will be loaded from image_encoder.ln_pre.weight                                           of shape (768,)
2023-12-07 16:53:20,267.267 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.positional_embedding                                    will be loaded from image_encoder.positional_embedding                                    of shape (197, 768)
2023-12-07 16:53:20,267.267 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.proj                                                    will be loaded from image_encoder.proj                                                    of shape (768, 512)
2023-12-07 16:53:20,267.267 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.0.attn.in_proj_bias               will be loaded from image_encoder.transformer.resblocks.0.attn.in_proj_bias               of shape (2304,)
2023-12-07 16:53:20,267.267 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.0.attn.in_proj_weight             will be loaded from image_encoder.transformer.resblocks.0.attn.in_proj_weight             of shape (2304, 768)
2023-12-07 16:53:20,267.267 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.0.attn.out_proj.bias              will be loaded from image_encoder.transformer.resblocks.0.attn.out_proj.bias              of shape (768,)
2023-12-07 16:53:20,267.267 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.0.attn.out_proj.weight            will be loaded from image_encoder.transformer.resblocks.0.attn.out_proj.weight            of shape (768, 768)
2023-12-07 16:53:20,267.267 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.0.ln_1.bias                       will be loaded from image_encoder.transformer.resblocks.0.ln_1.bias                       of shape (768,)
2023-12-07 16:53:20,267.267 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.0.ln_1.weight                     will be loaded from image_encoder.transformer.resblocks.0.ln_1.weight                     of shape (768,)
2023-12-07 16:53:20,267.267 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.0.ln_2.bias                       will be loaded from image_encoder.transformer.resblocks.0.ln_2.bias                       of shape (768,)
2023-12-07 16:53:20,268.268 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.0.ln_2.weight                     will be loaded from image_encoder.transformer.resblocks.0.ln_2.weight                     of shape (768,)
2023-12-07 16:53:20,268.268 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.0.mlp.c_fc.bias                   will be loaded from image_encoder.transformer.resblocks.0.mlp.c_fc.bias                   of shape (3072,)
2023-12-07 16:53:20,268.268 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.0.mlp.c_fc.weight                 will be loaded from image_encoder.transformer.resblocks.0.mlp.c_fc.weight                 of shape (3072, 768)
2023-12-07 16:53:20,268.268 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.0.mlp.c_proj.bias                 will be loaded from image_encoder.transformer.resblocks.0.mlp.c_proj.bias                 of shape (768,)
2023-12-07 16:53:20,268.268 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.0.mlp.c_proj.weight               will be loaded from image_encoder.transformer.resblocks.0.mlp.c_proj.weight               of shape (768, 3072)
2023-12-07 16:53:20,268.268 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.1.attn.in_proj_bias               will be loaded from image_encoder.transformer.resblocks.1.attn.in_proj_bias               of shape (2304,)
2023-12-07 16:53:20,268.268 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.1.attn.in_proj_weight             will be loaded from image_encoder.transformer.resblocks.1.attn.in_proj_weight             of shape (2304, 768)
2023-12-07 16:53:20,268.268 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.1.attn.out_proj.bias              will be loaded from image_encoder.transformer.resblocks.1.attn.out_proj.bias              of shape (768,)
2023-12-07 16:53:20,268.268 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.1.attn.out_proj.weight            will be loaded from image_encoder.transformer.resblocks.1.attn.out_proj.weight            of shape (768, 768)
2023-12-07 16:53:20,268.268 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.1.ln_1.bias                       will be loaded from image_encoder.transformer.resblocks.1.ln_1.bias                       of shape (768,)
2023-12-07 16:53:20,268.268 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.1.ln_1.weight                     will be loaded from image_encoder.transformer.resblocks.1.ln_1.weight                     of shape (768,)
2023-12-07 16:53:20,269.269 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.1.ln_2.bias                       will be loaded from image_encoder.transformer.resblocks.1.ln_2.bias                       of shape (768,)
2023-12-07 16:53:20,269.269 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.1.ln_2.weight                     will be loaded from image_encoder.transformer.resblocks.1.ln_2.weight                     of shape (768,)
2023-12-07 16:53:20,269.269 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.1.mlp.c_fc.bias                   will be loaded from image_encoder.transformer.resblocks.1.mlp.c_fc.bias                   of shape (3072,)
2023-12-07 16:53:20,269.269 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.1.mlp.c_fc.weight                 will be loaded from image_encoder.transformer.resblocks.1.mlp.c_fc.weight                 of shape (3072, 768)
2023-12-07 16:53:20,269.269 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.1.mlp.c_proj.bias                 will be loaded from image_encoder.transformer.resblocks.1.mlp.c_proj.bias                 of shape (768,)
2023-12-07 16:53:20,269.269 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.1.mlp.c_proj.weight               will be loaded from image_encoder.transformer.resblocks.1.mlp.c_proj.weight               of shape (768, 3072)
2023-12-07 16:53:20,269.269 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.10.attn.in_proj_bias              will be loaded from image_encoder.transformer.resblocks.10.attn.in_proj_bias              of shape (2304,)
2023-12-07 16:53:20,269.269 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.10.attn.in_proj_weight            will be loaded from image_encoder.transformer.resblocks.10.attn.in_proj_weight            of shape (2304, 768)
2023-12-07 16:53:20,269.269 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.10.attn.out_proj.bias             will be loaded from image_encoder.transformer.resblocks.10.attn.out_proj.bias             of shape (768,)
2023-12-07 16:53:20,269.269 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.10.attn.out_proj.weight           will be loaded from image_encoder.transformer.resblocks.10.attn.out_proj.weight           of shape (768, 768)
2023-12-07 16:53:20,269.269 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.10.ln_1.bias                      will be loaded from image_encoder.transformer.resblocks.10.ln_1.bias                      of shape (768,)
2023-12-07 16:53:20,269.269 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.10.ln_1.weight                    will be loaded from image_encoder.transformer.resblocks.10.ln_1.weight                    of shape (768,)
2023-12-07 16:53:20,269.269 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.10.ln_2.bias                      will be loaded from image_encoder.transformer.resblocks.10.ln_2.bias                      of shape (768,)
2023-12-07 16:53:20,269.269 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.10.ln_2.weight                    will be loaded from image_encoder.transformer.resblocks.10.ln_2.weight                    of shape (768,)
2023-12-07 16:53:20,269.269 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.10.mlp.c_fc.bias                  will be loaded from image_encoder.transformer.resblocks.10.mlp.c_fc.bias                  of shape (3072,)
2023-12-07 16:53:20,269.269 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.10.mlp.c_fc.weight                will be loaded from image_encoder.transformer.resblocks.10.mlp.c_fc.weight                of shape (3072, 768)
2023-12-07 16:53:20,269.269 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.10.mlp.c_proj.bias                will be loaded from image_encoder.transformer.resblocks.10.mlp.c_proj.bias                of shape (768,)
2023-12-07 16:53:20,269.269 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.10.mlp.c_proj.weight              will be loaded from image_encoder.transformer.resblocks.10.mlp.c_proj.weight              of shape (768, 3072)
2023-12-07 16:53:20,269.269 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.11.attn.in_proj_bias              will be loaded from image_encoder.transformer.resblocks.11.attn.in_proj_bias              of shape (2304,)
2023-12-07 16:53:20,269.269 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.11.attn.in_proj_weight            will be loaded from image_encoder.transformer.resblocks.11.attn.in_proj_weight            of shape (2304, 768)
2023-12-07 16:53:20,269.269 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.11.attn.out_proj.bias             will be loaded from image_encoder.transformer.resblocks.11.attn.out_proj.bias             of shape (768,)
2023-12-07 16:53:20,269.269 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.11.attn.out_proj.weight           will be loaded from image_encoder.transformer.resblocks.11.attn.out_proj.weight           of shape (768, 768)
2023-12-07 16:53:20,269.269 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.11.ln_1.bias                      will be loaded from image_encoder.transformer.resblocks.11.ln_1.bias                      of shape (768,)
2023-12-07 16:53:20,270.270 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.11.ln_1.weight                    will be loaded from image_encoder.transformer.resblocks.11.ln_1.weight                    of shape (768,)
2023-12-07 16:53:20,270.270 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.11.ln_2.bias                      will be loaded from image_encoder.transformer.resblocks.11.ln_2.bias                      of shape (768,)
2023-12-07 16:53:20,270.270 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.11.ln_2.weight                    will be loaded from image_encoder.transformer.resblocks.11.ln_2.weight                    of shape (768,)
2023-12-07 16:53:20,270.270 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.11.mlp.c_fc.bias                  will be loaded from image_encoder.transformer.resblocks.11.mlp.c_fc.bias                  of shape (3072,)
2023-12-07 16:53:20,270.270 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.11.mlp.c_fc.weight                will be loaded from image_encoder.transformer.resblocks.11.mlp.c_fc.weight                of shape (3072, 768)
2023-12-07 16:53:20,270.270 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.11.mlp.c_proj.bias                will be loaded from image_encoder.transformer.resblocks.11.mlp.c_proj.bias                of shape (768,)
2023-12-07 16:53:20,270.270 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.11.mlp.c_proj.weight              will be loaded from image_encoder.transformer.resblocks.11.mlp.c_proj.weight              of shape (768, 3072)
2023-12-07 16:53:20,270.270 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.2.attn.in_proj_bias               will be loaded from image_encoder.transformer.resblocks.2.attn.in_proj_bias               of shape (2304,)
2023-12-07 16:53:20,270.270 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.2.attn.in_proj_weight             will be loaded from image_encoder.transformer.resblocks.2.attn.in_proj_weight             of shape (2304, 768)
2023-12-07 16:53:20,270.270 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.2.attn.out_proj.bias              will be loaded from image_encoder.transformer.resblocks.2.attn.out_proj.bias              of shape (768,)
2023-12-07 16:53:20,270.270 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.2.attn.out_proj.weight            will be loaded from image_encoder.transformer.resblocks.2.attn.out_proj.weight            of shape (768, 768)
2023-12-07 16:53:20,270.270 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.2.ln_1.bias                       will be loaded from image_encoder.transformer.resblocks.2.ln_1.bias                       of shape (768,)
2023-12-07 16:53:20,270.270 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.2.ln_1.weight                     will be loaded from image_encoder.transformer.resblocks.2.ln_1.weight                     of shape (768,)
2023-12-07 16:53:20,270.270 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.2.ln_2.bias                       will be loaded from image_encoder.transformer.resblocks.2.ln_2.bias                       of shape (768,)
2023-12-07 16:53:20,270.270 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.2.ln_2.weight                     will be loaded from image_encoder.transformer.resblocks.2.ln_2.weight                     of shape (768,)
2023-12-07 16:53:20,270.270 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.2.mlp.c_fc.bias                   will be loaded from image_encoder.transformer.resblocks.2.mlp.c_fc.bias                   of shape (3072,)
2023-12-07 16:53:20,270.270 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.2.mlp.c_fc.weight                 will be loaded from image_encoder.transformer.resblocks.2.mlp.c_fc.weight                 of shape (3072, 768)
2023-12-07 16:53:20,270.270 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.2.mlp.c_proj.bias                 will be loaded from image_encoder.transformer.resblocks.2.mlp.c_proj.bias                 of shape (768,)
2023-12-07 16:53:20,270.270 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.2.mlp.c_proj.weight               will be loaded from image_encoder.transformer.resblocks.2.mlp.c_proj.weight               of shape (768, 3072)
2023-12-07 16:53:20,270.270 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.3.attn.in_proj_bias               will be loaded from image_encoder.transformer.resblocks.3.attn.in_proj_bias               of shape (2304,)
2023-12-07 16:53:20,270.270 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.3.attn.in_proj_weight             will be loaded from image_encoder.transformer.resblocks.3.attn.in_proj_weight             of shape (2304, 768)
2023-12-07 16:53:20,270.270 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.3.attn.out_proj.bias              will be loaded from image_encoder.transformer.resblocks.3.attn.out_proj.bias              of shape (768,)
2023-12-07 16:53:20,270.270 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.3.attn.out_proj.weight            will be loaded from image_encoder.transformer.resblocks.3.attn.out_proj.weight            of shape (768, 768)
2023-12-07 16:53:20,270.270 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.3.ln_1.bias                       will be loaded from image_encoder.transformer.resblocks.3.ln_1.bias                       of shape (768,)
2023-12-07 16:53:20,270.270 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.3.ln_1.weight                     will be loaded from image_encoder.transformer.resblocks.3.ln_1.weight                     of shape (768,)
2023-12-07 16:53:20,271.271 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.3.ln_2.bias                       will be loaded from image_encoder.transformer.resblocks.3.ln_2.bias                       of shape (768,)
2023-12-07 16:53:20,271.271 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.3.ln_2.weight                     will be loaded from image_encoder.transformer.resblocks.3.ln_2.weight                     of shape (768,)
2023-12-07 16:53:20,271.271 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.3.mlp.c_fc.bias                   will be loaded from image_encoder.transformer.resblocks.3.mlp.c_fc.bias                   of shape (3072,)
2023-12-07 16:53:20,271.271 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.3.mlp.c_fc.weight                 will be loaded from image_encoder.transformer.resblocks.3.mlp.c_fc.weight                 of shape (3072, 768)
2023-12-07 16:53:20,271.271 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.3.mlp.c_proj.bias                 will be loaded from image_encoder.transformer.resblocks.3.mlp.c_proj.bias                 of shape (768,)
2023-12-07 16:53:20,271.271 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.3.mlp.c_proj.weight               will be loaded from image_encoder.transformer.resblocks.3.mlp.c_proj.weight               of shape (768, 3072)
2023-12-07 16:53:20,271.271 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.4.attn.in_proj_bias               will be loaded from image_encoder.transformer.resblocks.4.attn.in_proj_bias               of shape (2304,)
2023-12-07 16:53:20,271.271 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.4.attn.in_proj_weight             will be loaded from image_encoder.transformer.resblocks.4.attn.in_proj_weight             of shape (2304, 768)
2023-12-07 16:53:20,271.271 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.4.attn.out_proj.bias              will be loaded from image_encoder.transformer.resblocks.4.attn.out_proj.bias              of shape (768,)
2023-12-07 16:53:20,271.271 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.4.attn.out_proj.weight            will be loaded from image_encoder.transformer.resblocks.4.attn.out_proj.weight            of shape (768, 768)
2023-12-07 16:53:20,271.271 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.4.ln_1.bias                       will be loaded from image_encoder.transformer.resblocks.4.ln_1.bias                       of shape (768,)
2023-12-07 16:53:20,271.271 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.4.ln_1.weight                     will be loaded from image_encoder.transformer.resblocks.4.ln_1.weight                     of shape (768,)
2023-12-07 16:53:20,271.271 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.4.ln_2.bias                       will be loaded from image_encoder.transformer.resblocks.4.ln_2.bias                       of shape (768,)
2023-12-07 16:53:20,271.271 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.4.ln_2.weight                     will be loaded from image_encoder.transformer.resblocks.4.ln_2.weight                     of shape (768,)
2023-12-07 16:53:20,271.271 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.4.mlp.c_fc.bias                   will be loaded from image_encoder.transformer.resblocks.4.mlp.c_fc.bias                   of shape (3072,)
2023-12-07 16:53:20,271.271 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.4.mlp.c_fc.weight                 will be loaded from image_encoder.transformer.resblocks.4.mlp.c_fc.weight                 of shape (3072, 768)
2023-12-07 16:53:20,271.271 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.4.mlp.c_proj.bias                 will be loaded from image_encoder.transformer.resblocks.4.mlp.c_proj.bias                 of shape (768,)
2023-12-07 16:53:20,271.271 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.4.mlp.c_proj.weight               will be loaded from image_encoder.transformer.resblocks.4.mlp.c_proj.weight               of shape (768, 3072)
2023-12-07 16:53:20,271.271 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.5.attn.in_proj_bias               will be loaded from image_encoder.transformer.resblocks.5.attn.in_proj_bias               of shape (2304,)
2023-12-07 16:53:20,271.271 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.5.attn.in_proj_weight             will be loaded from image_encoder.transformer.resblocks.5.attn.in_proj_weight             of shape (2304, 768)
2023-12-07 16:53:20,271.271 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.5.attn.out_proj.bias              will be loaded from image_encoder.transformer.resblocks.5.attn.out_proj.bias              of shape (768,)
2023-12-07 16:53:20,271.271 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.5.attn.out_proj.weight            will be loaded from image_encoder.transformer.resblocks.5.attn.out_proj.weight            of shape (768, 768)
2023-12-07 16:53:20,271.271 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.5.ln_1.bias                       will be loaded from image_encoder.transformer.resblocks.5.ln_1.bias                       of shape (768,)
2023-12-07 16:53:20,271.271 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.5.ln_1.weight                     will be loaded from image_encoder.transformer.resblocks.5.ln_1.weight                     of shape (768,)
2023-12-07 16:53:20,271.271 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.5.ln_2.bias                       will be loaded from image_encoder.transformer.resblocks.5.ln_2.bias                       of shape (768,)
2023-12-07 16:53:20,272.272 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.5.ln_2.weight                     will be loaded from image_encoder.transformer.resblocks.5.ln_2.weight                     of shape (768,)
2023-12-07 16:53:20,272.272 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.5.mlp.c_fc.bias                   will be loaded from image_encoder.transformer.resblocks.5.mlp.c_fc.bias                   of shape (3072,)
2023-12-07 16:53:20,272.272 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.5.mlp.c_fc.weight                 will be loaded from image_encoder.transformer.resblocks.5.mlp.c_fc.weight                 of shape (3072, 768)
2023-12-07 16:53:20,272.272 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.5.mlp.c_proj.bias                 will be loaded from image_encoder.transformer.resblocks.5.mlp.c_proj.bias                 of shape (768,)
2023-12-07 16:53:20,272.272 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.5.mlp.c_proj.weight               will be loaded from image_encoder.transformer.resblocks.5.mlp.c_proj.weight               of shape (768, 3072)
2023-12-07 16:53:20,272.272 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.6.attn.in_proj_bias               will be loaded from image_encoder.transformer.resblocks.6.attn.in_proj_bias               of shape (2304,)
2023-12-07 16:53:20,272.272 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.6.attn.in_proj_weight             will be loaded from image_encoder.transformer.resblocks.6.attn.in_proj_weight             of shape (2304, 768)
2023-12-07 16:53:20,272.272 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.6.attn.out_proj.bias              will be loaded from image_encoder.transformer.resblocks.6.attn.out_proj.bias              of shape (768,)
2023-12-07 16:53:20,272.272 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.6.attn.out_proj.weight            will be loaded from image_encoder.transformer.resblocks.6.attn.out_proj.weight            of shape (768, 768)
2023-12-07 16:53:20,272.272 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.6.ln_1.bias                       will be loaded from image_encoder.transformer.resblocks.6.ln_1.bias                       of shape (768,)
2023-12-07 16:53:20,272.272 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.6.ln_1.weight                     will be loaded from image_encoder.transformer.resblocks.6.ln_1.weight                     of shape (768,)
2023-12-07 16:53:20,272.272 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.6.ln_2.bias                       will be loaded from image_encoder.transformer.resblocks.6.ln_2.bias                       of shape (768,)
2023-12-07 16:53:20,272.272 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.6.ln_2.weight                     will be loaded from image_encoder.transformer.resblocks.6.ln_2.weight                     of shape (768,)
2023-12-07 16:53:20,272.272 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.6.mlp.c_fc.bias                   will be loaded from image_encoder.transformer.resblocks.6.mlp.c_fc.bias                   of shape (3072,)
2023-12-07 16:53:20,272.272 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.6.mlp.c_fc.weight                 will be loaded from image_encoder.transformer.resblocks.6.mlp.c_fc.weight                 of shape (3072, 768)
2023-12-07 16:53:20,272.272 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.6.mlp.c_proj.bias                 will be loaded from image_encoder.transformer.resblocks.6.mlp.c_proj.bias                 of shape (768,)
2023-12-07 16:53:20,272.272 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.6.mlp.c_proj.weight               will be loaded from image_encoder.transformer.resblocks.6.mlp.c_proj.weight               of shape (768, 3072)
2023-12-07 16:53:20,272.272 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.7.attn.in_proj_bias               will be loaded from image_encoder.transformer.resblocks.7.attn.in_proj_bias               of shape (2304,)
2023-12-07 16:53:20,272.272 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.7.attn.in_proj_weight             will be loaded from image_encoder.transformer.resblocks.7.attn.in_proj_weight             of shape (2304, 768)
2023-12-07 16:53:20,272.272 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.7.attn.out_proj.bias              will be loaded from image_encoder.transformer.resblocks.7.attn.out_proj.bias              of shape (768,)
2023-12-07 16:53:20,272.272 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.7.attn.out_proj.weight            will be loaded from image_encoder.transformer.resblocks.7.attn.out_proj.weight            of shape (768, 768)
2023-12-07 16:53:20,272.272 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.7.ln_1.bias                       will be loaded from image_encoder.transformer.resblocks.7.ln_1.bias                       of shape (768,)
2023-12-07 16:53:20,272.272 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.7.ln_1.weight                     will be loaded from image_encoder.transformer.resblocks.7.ln_1.weight                     of shape (768,)
2023-12-07 16:53:20,273.273 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.7.ln_2.bias                       will be loaded from image_encoder.transformer.resblocks.7.ln_2.bias                       of shape (768,)
2023-12-07 16:53:20,273.273 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.7.ln_2.weight                     will be loaded from image_encoder.transformer.resblocks.7.ln_2.weight                     of shape (768,)
2023-12-07 16:53:20,273.273 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.7.mlp.c_fc.bias                   will be loaded from image_encoder.transformer.resblocks.7.mlp.c_fc.bias                   of shape (3072,)
2023-12-07 16:53:20,273.273 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.7.mlp.c_fc.weight                 will be loaded from image_encoder.transformer.resblocks.7.mlp.c_fc.weight                 of shape (3072, 768)
2023-12-07 16:53:20,273.273 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.7.mlp.c_proj.bias                 will be loaded from image_encoder.transformer.resblocks.7.mlp.c_proj.bias                 of shape (768,)
2023-12-07 16:53:20,273.273 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.7.mlp.c_proj.weight               will be loaded from image_encoder.transformer.resblocks.7.mlp.c_proj.weight               of shape (768, 3072)
2023-12-07 16:53:20,273.273 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.8.attn.in_proj_bias               will be loaded from image_encoder.transformer.resblocks.8.attn.in_proj_bias               of shape (2304,)
2023-12-07 16:53:20,273.273 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.8.attn.in_proj_weight             will be loaded from image_encoder.transformer.resblocks.8.attn.in_proj_weight             of shape (2304, 768)
2023-12-07 16:53:20,273.273 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.8.attn.out_proj.bias              will be loaded from image_encoder.transformer.resblocks.8.attn.out_proj.bias              of shape (768,)
2023-12-07 16:53:20,273.273 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.8.attn.out_proj.weight            will be loaded from image_encoder.transformer.resblocks.8.attn.out_proj.weight            of shape (768, 768)
2023-12-07 16:53:20,273.273 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.8.ln_1.bias                       will be loaded from image_encoder.transformer.resblocks.8.ln_1.bias                       of shape (768,)
2023-12-07 16:53:20,273.273 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.8.ln_1.weight                     will be loaded from image_encoder.transformer.resblocks.8.ln_1.weight                     of shape (768,)
2023-12-07 16:53:20,273.273 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.8.ln_2.bias                       will be loaded from image_encoder.transformer.resblocks.8.ln_2.bias                       of shape (768,)
2023-12-07 16:53:20,273.273 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.8.ln_2.weight                     will be loaded from image_encoder.transformer.resblocks.8.ln_2.weight                     of shape (768,)
2023-12-07 16:53:20,273.273 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.8.mlp.c_fc.bias                   will be loaded from image_encoder.transformer.resblocks.8.mlp.c_fc.bias                   of shape (3072,)
2023-12-07 16:53:20,273.273 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.8.mlp.c_fc.weight                 will be loaded from image_encoder.transformer.resblocks.8.mlp.c_fc.weight                 of shape (3072, 768)
2023-12-07 16:53:20,273.273 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.8.mlp.c_proj.bias                 will be loaded from image_encoder.transformer.resblocks.8.mlp.c_proj.bias                 of shape (768,)
2023-12-07 16:53:20,273.273 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.8.mlp.c_proj.weight               will be loaded from image_encoder.transformer.resblocks.8.mlp.c_proj.weight               of shape (768, 3072)
2023-12-07 16:53:20,273.273 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.9.attn.in_proj_bias               will be loaded from image_encoder.transformer.resblocks.9.attn.in_proj_bias               of shape (2304,)
2023-12-07 16:53:20,273.273 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.9.attn.in_proj_weight             will be loaded from image_encoder.transformer.resblocks.9.attn.in_proj_weight             of shape (2304, 768)
2023-12-07 16:53:20,273.273 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.9.attn.out_proj.bias              will be loaded from image_encoder.transformer.resblocks.9.attn.out_proj.bias              of shape (768,)
2023-12-07 16:53:20,273.273 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.9.attn.out_proj.weight            will be loaded from image_encoder.transformer.resblocks.9.attn.out_proj.weight            of shape (768, 768)
2023-12-07 16:53:20,274.274 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.9.ln_1.bias                       will be loaded from image_encoder.transformer.resblocks.9.ln_1.bias                       of shape (768,)
2023-12-07 16:53:20,274.274 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.9.ln_1.weight                     will be loaded from image_encoder.transformer.resblocks.9.ln_1.weight                     of shape (768,)
2023-12-07 16:53:20,274.274 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.9.ln_2.bias                       will be loaded from image_encoder.transformer.resblocks.9.ln_2.bias                       of shape (768,)
2023-12-07 16:53:20,274.274 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.9.ln_2.weight                     will be loaded from image_encoder.transformer.resblocks.9.ln_2.weight                     of shape (768,)
2023-12-07 16:53:20,274.274 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.9.mlp.c_fc.bias                   will be loaded from image_encoder.transformer.resblocks.9.mlp.c_fc.bias                   of shape (3072,)
2023-12-07 16:53:20,274.274 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.9.mlp.c_fc.weight                 will be loaded from image_encoder.transformer.resblocks.9.mlp.c_fc.weight                 of shape (3072, 768)
2023-12-07 16:53:20,274.274 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.9.mlp.c_proj.bias                 will be loaded from image_encoder.transformer.resblocks.9.mlp.c_proj.bias                 of shape (768,)
2023-12-07 16:53:20,274.274 9392:torch_common.py:128 align_and_update_state_dicts(): image_encoder.transformer.resblocks.9.mlp.c_proj.weight               will be loaded from image_encoder.transformer.resblocks.9.mlp.c_proj.weight               of shape (768, 3072)
2023-12-07 16:53:20,274.274 9392:torch_common.py:128 align_and_update_state_dicts(): img_temperal_embedding.0                                              will be loaded from img_temperal_embedding.0                                              of shape (1, 1, 768)
2023-12-07 16:53:20,274.274 9392:torch_common.py:128 align_and_update_state_dicts(): img_temperal_embedding.1                                              will be loaded from img_temperal_embedding.1                                              of shape (1, 1, 768)
2023-12-07 16:53:20,274.274 9392:torch_common.py:128 align_and_update_state_dicts(): img_temperal_embedding.2                                              will be loaded from img_temperal_embedding.2                                              of shape (1, 1, 768)
2023-12-07 16:53:20,274.274 9392:torch_common.py:128 align_and_update_state_dicts(): img_temperal_embedding.3                                              will be loaded from img_temperal_embedding.3                                              of shape (1, 1, 768)
2023-12-07 16:53:20,274.274 9392:torch_common.py:128 align_and_update_state_dicts(): img_temperal_embedding.4                                              will be loaded from img_temperal_embedding.4                                              of shape (1, 1, 768)
2023-12-07 16:53:20,274.274 9392:torch_common.py:128 align_and_update_state_dicts(): img_temperal_embedding.5                                              will be loaded from img_temperal_embedding.5                                              of shape (1, 1, 768)
2023-12-07 16:53:20,274.274 9392:torch_common.py:128 align_and_update_state_dicts(): textual.embedding.layer_norm.bias                                     will be loaded from textual.embedding.layer_norm.bias                                     of shape (768,)
2023-12-07 16:53:20,274.274 9392:torch_common.py:128 align_and_update_state_dicts(): textual.embedding.layer_norm.weight                                   will be loaded from textual.embedding.layer_norm.weight                                   of shape (768,)
2023-12-07 16:53:20,274.274 9392:torch_common.py:128 align_and_update_state_dicts(): textual.embedding.positions.weight                                    will be loaded from textual.embedding.positions.weight                                    of shape (1024, 768)
2023-12-07 16:53:20,274.274 9392:torch_common.py:128 align_and_update_state_dicts(): textual.embedding.words.weight                                        will be loaded from textual.embedding.words.weight                                        of shape (30522, 768)
2023-12-07 16:53:20,274.274 9392:torch_common.py:128 align_and_update_state_dicts(): textual.output.bias                                                   will be loaded from textual.output.bias                                                   of shape (30522,)
2023-12-07 16:53:20,274.274 9392:torch_common.py:128 align_and_update_state_dicts(): textual.output.weight                                                 will be loaded from textual.output.weight                                                 of shape (30522, 768)
2023-12-07 16:53:20,274.274 9392:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.attention.output.LayerNorm.bias   will be loaded from textual.transformer.encoder.layer.0.attention.output.LayerNorm.bias   of shape (768,)
2023-12-07 16:53:20,274.274 9392:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.attention.output.LayerNorm.weight will be loaded from textual.transformer.encoder.layer.0.attention.output.LayerNorm.weight of shape (768,)
2023-12-07 16:53:20,274.274 9392:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.attention.output.dense.bias       will be loaded from textual.transformer.encoder.layer.0.attention.output.dense.bias       of shape (768,)
2023-12-07 16:53:20,275.275 9392:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.attention.output.dense.weight     will be loaded from textual.transformer.encoder.layer.0.attention.output.dense.weight     of shape (768, 768)
2023-12-07 16:53:20,275.275 9392:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.attention.self.key.bias           will be loaded from textual.transformer.encoder.layer.0.attention.self.key.bias           of shape (768,)
2023-12-07 16:53:20,275.275 9392:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.attention.self.key.weight         will be loaded from textual.transformer.encoder.layer.0.attention.self.key.weight         of shape (768, 768)
2023-12-07 16:53:20,275.275 9392:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.attention.self.query.bias         will be loaded from textual.transformer.encoder.layer.0.attention.self.query.bias         of shape (768,)
2023-12-07 16:53:20,275.275 9392:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.attention.self.query.weight       will be loaded from textual.transformer.encoder.layer.0.attention.self.query.weight       of shape (768, 768)
2023-12-07 16:53:20,275.275 9392:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.attention.self.value.bias         will be loaded from textual.transformer.encoder.layer.0.attention.self.value.bias         of shape (768,)
2023-12-07 16:53:20,275.275 9392:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.attention.self.value.weight       will be loaded from textual.transformer.encoder.layer.0.attention.self.value.weight       of shape (768, 768)
2023-12-07 16:53:20,275.275 9392:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.intermediate.dense.bias           will be loaded from textual.transformer.encoder.layer.0.intermediate.dense.bias           of shape (3072,)
2023-12-07 16:53:20,275.275 9392:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.intermediate.dense.weight         will be loaded from textual.transformer.encoder.layer.0.intermediate.dense.weight         of shape (3072, 768)
2023-12-07 16:53:20,275.275 9392:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.output.LayerNorm.bias             will be loaded from textual.transformer.encoder.layer.0.output.LayerNorm.bias             of shape (768,)
2023-12-07 16:53:20,275.275 9392:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.output.LayerNorm.weight           will be loaded from textual.transformer.encoder.layer.0.output.LayerNorm.weight           of shape (768,)
2023-12-07 16:53:20,275.275 9392:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.output.dense.bias                 will be loaded from textual.transformer.encoder.layer.0.output.dense.bias                 of shape (768,)
2023-12-07 16:53:20,275.275 9392:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.0.output.dense.weight               will be loaded from textual.transformer.encoder.layer.0.output.dense.weight               of shape (768, 3072)
2023-12-07 16:53:20,275.275 9392:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.attention.output.LayerNorm.bias   will be loaded from textual.transformer.encoder.layer.1.attention.output.LayerNorm.bias   of shape (768,)
2023-12-07 16:53:20,275.275 9392:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.attention.output.LayerNorm.weight will be loaded from textual.transformer.encoder.layer.1.attention.output.LayerNorm.weight of shape (768,)
2023-12-07 16:53:20,275.275 9392:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.attention.output.dense.bias       will be loaded from textual.transformer.encoder.layer.1.attention.output.dense.bias       of shape (768,)
2023-12-07 16:53:20,276.276 9392:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.attention.output.dense.weight     will be loaded from textual.transformer.encoder.layer.1.attention.output.dense.weight     of shape (768, 768)
2023-12-07 16:53:20,276.276 9392:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.attention.self.key.bias           will be loaded from textual.transformer.encoder.layer.1.attention.self.key.bias           of shape (768,)
2023-12-07 16:53:20,276.276 9392:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.attention.self.key.weight         will be loaded from textual.transformer.encoder.layer.1.attention.self.key.weight         of shape (768, 768)
2023-12-07 16:53:20,276.276 9392:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.attention.self.query.bias         will be loaded from textual.transformer.encoder.layer.1.attention.self.query.bias         of shape (768,)
2023-12-07 16:53:20,276.276 9392:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.attention.self.query.weight       will be loaded from textual.transformer.encoder.layer.1.attention.self.query.weight       of shape (768, 768)
2023-12-07 16:53:20,276.276 9392:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.attention.self.value.bias         will be loaded from textual.transformer.encoder.layer.1.attention.self.value.bias         of shape (768,)
2023-12-07 16:53:20,276.276 9392:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.attention.self.value.weight       will be loaded from textual.transformer.encoder.layer.1.attention.self.value.weight       of shape (768, 768)
2023-12-07 16:53:20,276.276 9392:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.intermediate.dense.bias           will be loaded from textual.transformer.encoder.layer.1.intermediate.dense.bias           of shape (3072,)
2023-12-07 16:53:20,276.276 9392:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.intermediate.dense.weight         will be loaded from textual.transformer.encoder.layer.1.intermediate.dense.weight         of shape (3072, 768)
2023-12-07 16:53:20,276.276 9392:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.output.LayerNorm.bias             will be loaded from textual.transformer.encoder.layer.1.output.LayerNorm.bias             of shape (768,)
2023-12-07 16:53:20,276.276 9392:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.output.LayerNorm.weight           will be loaded from textual.transformer.encoder.layer.1.output.LayerNorm.weight           of shape (768,)
2023-12-07 16:53:20,276.276 9392:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.output.dense.bias                 will be loaded from textual.transformer.encoder.layer.1.output.dense.bias                 of shape (768,)
2023-12-07 16:53:20,276.276 9392:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.1.output.dense.weight               will be loaded from textual.transformer.encoder.layer.1.output.dense.weight               of shape (768, 3072)
2023-12-07 16:53:20,276.276 9392:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.attention.output.LayerNorm.bias   will be loaded from textual.transformer.encoder.layer.2.attention.output.LayerNorm.bias   of shape (768,)
2023-12-07 16:53:20,276.276 9392:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.attention.output.LayerNorm.weight will be loaded from textual.transformer.encoder.layer.2.attention.output.LayerNorm.weight of shape (768,)
2023-12-07 16:53:20,276.276 9392:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.attention.output.dense.bias       will be loaded from textual.transformer.encoder.layer.2.attention.output.dense.bias       of shape (768,)
2023-12-07 16:53:20,276.276 9392:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.attention.output.dense.weight     will be loaded from textual.transformer.encoder.layer.2.attention.output.dense.weight     of shape (768, 768)
2023-12-07 16:53:20,276.276 9392:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.attention.self.key.bias           will be loaded from textual.transformer.encoder.layer.2.attention.self.key.bias           of shape (768,)
2023-12-07 16:53:20,276.276 9392:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.attention.self.key.weight         will be loaded from textual.transformer.encoder.layer.2.attention.self.key.weight         of shape (768, 768)
2023-12-07 16:53:20,276.276 9392:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.attention.self.query.bias         will be loaded from textual.transformer.encoder.layer.2.attention.self.query.bias         of shape (768,)
2023-12-07 16:53:20,276.276 9392:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.attention.self.query.weight       will be loaded from textual.transformer.encoder.layer.2.attention.self.query.weight       of shape (768, 768)
2023-12-07 16:53:20,276.276 9392:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.attention.self.value.bias         will be loaded from textual.transformer.encoder.layer.2.attention.self.value.bias         of shape (768,)
2023-12-07 16:53:20,276.276 9392:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.attention.self.value.weight       will be loaded from textual.transformer.encoder.layer.2.attention.self.value.weight       of shape (768, 768)
2023-12-07 16:53:20,277.277 9392:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.intermediate.dense.bias           will be loaded from textual.transformer.encoder.layer.2.intermediate.dense.bias           of shape (3072,)
2023-12-07 16:53:20,277.277 9392:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.intermediate.dense.weight         will be loaded from textual.transformer.encoder.layer.2.intermediate.dense.weight         of shape (3072, 768)
2023-12-07 16:53:20,277.277 9392:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.output.LayerNorm.bias             will be loaded from textual.transformer.encoder.layer.2.output.LayerNorm.bias             of shape (768,)
2023-12-07 16:53:20,277.277 9392:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.output.LayerNorm.weight           will be loaded from textual.transformer.encoder.layer.2.output.LayerNorm.weight           of shape (768,)
2023-12-07 16:53:20,277.277 9392:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.output.dense.bias                 will be loaded from textual.transformer.encoder.layer.2.output.dense.bias                 of shape (768,)
2023-12-07 16:53:20,277.277 9392:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.2.output.dense.weight               will be loaded from textual.transformer.encoder.layer.2.output.dense.weight               of shape (768, 3072)
2023-12-07 16:53:20,277.277 9392:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.attention.output.LayerNorm.bias   will be loaded from textual.transformer.encoder.layer.3.attention.output.LayerNorm.bias   of shape (768,)
2023-12-07 16:53:20,277.277 9392:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.attention.output.LayerNorm.weight will be loaded from textual.transformer.encoder.layer.3.attention.output.LayerNorm.weight of shape (768,)
2023-12-07 16:53:20,277.277 9392:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.attention.output.dense.bias       will be loaded from textual.transformer.encoder.layer.3.attention.output.dense.bias       of shape (768,)
2023-12-07 16:53:20,277.277 9392:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.attention.output.dense.weight     will be loaded from textual.transformer.encoder.layer.3.attention.output.dense.weight     of shape (768, 768)
2023-12-07 16:53:20,277.277 9392:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.attention.self.key.bias           will be loaded from textual.transformer.encoder.layer.3.attention.self.key.bias           of shape (768,)
2023-12-07 16:53:20,277.277 9392:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.attention.self.key.weight         will be loaded from textual.transformer.encoder.layer.3.attention.self.key.weight         of shape (768, 768)
2023-12-07 16:53:20,277.277 9392:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.attention.self.query.bias         will be loaded from textual.transformer.encoder.layer.3.attention.self.query.bias         of shape (768,)
2023-12-07 16:53:20,277.277 9392:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.attention.self.query.weight       will be loaded from textual.transformer.encoder.layer.3.attention.self.query.weight       of shape (768, 768)
2023-12-07 16:53:20,277.277 9392:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.attention.self.value.bias         will be loaded from textual.transformer.encoder.layer.3.attention.self.value.bias         of shape (768,)
2023-12-07 16:53:20,277.277 9392:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.attention.self.value.weight       will be loaded from textual.transformer.encoder.layer.3.attention.self.value.weight       of shape (768, 768)
2023-12-07 16:53:20,277.277 9392:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.intermediate.dense.bias           will be loaded from textual.transformer.encoder.layer.3.intermediate.dense.bias           of shape (3072,)
2023-12-07 16:53:20,277.277 9392:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.intermediate.dense.weight         will be loaded from textual.transformer.encoder.layer.3.intermediate.dense.weight         of shape (3072, 768)
2023-12-07 16:53:20,277.277 9392:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.output.LayerNorm.bias             will be loaded from textual.transformer.encoder.layer.3.output.LayerNorm.bias             of shape (768,)
2023-12-07 16:53:20,277.277 9392:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.output.LayerNorm.weight           will be loaded from textual.transformer.encoder.layer.3.output.LayerNorm.weight           of shape (768,)
2023-12-07 16:53:20,277.277 9392:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.output.dense.bias                 will be loaded from textual.transformer.encoder.layer.3.output.dense.bias                 of shape (768,)
2023-12-07 16:53:20,277.277 9392:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.3.output.dense.weight               will be loaded from textual.transformer.encoder.layer.3.output.dense.weight               of shape (768, 3072)
2023-12-07 16:53:20,277.277 9392:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.attention.output.LayerNorm.bias   will be loaded from textual.transformer.encoder.layer.4.attention.output.LayerNorm.bias   of shape (768,)
2023-12-07 16:53:20,278.278 9392:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.attention.output.LayerNorm.weight will be loaded from textual.transformer.encoder.layer.4.attention.output.LayerNorm.weight of shape (768,)
2023-12-07 16:53:20,278.278 9392:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.attention.output.dense.bias       will be loaded from textual.transformer.encoder.layer.4.attention.output.dense.bias       of shape (768,)
2023-12-07 16:53:20,278.278 9392:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.attention.output.dense.weight     will be loaded from textual.transformer.encoder.layer.4.attention.output.dense.weight     of shape (768, 768)
2023-12-07 16:53:20,278.278 9392:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.attention.self.key.bias           will be loaded from textual.transformer.encoder.layer.4.attention.self.key.bias           of shape (768,)
2023-12-07 16:53:20,278.278 9392:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.attention.self.key.weight         will be loaded from textual.transformer.encoder.layer.4.attention.self.key.weight         of shape (768, 768)
2023-12-07 16:53:20,278.278 9392:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.attention.self.query.bias         will be loaded from textual.transformer.encoder.layer.4.attention.self.query.bias         of shape (768,)
2023-12-07 16:53:20,278.278 9392:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.attention.self.query.weight       will be loaded from textual.transformer.encoder.layer.4.attention.self.query.weight       of shape (768, 768)
2023-12-07 16:53:20,278.278 9392:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.attention.self.value.bias         will be loaded from textual.transformer.encoder.layer.4.attention.self.value.bias         of shape (768,)
2023-12-07 16:53:20,278.278 9392:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.attention.self.value.weight       will be loaded from textual.transformer.encoder.layer.4.attention.self.value.weight       of shape (768, 768)
2023-12-07 16:53:20,278.278 9392:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.intermediate.dense.bias           will be loaded from textual.transformer.encoder.layer.4.intermediate.dense.bias           of shape (3072,)
2023-12-07 16:53:20,278.278 9392:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.intermediate.dense.weight         will be loaded from textual.transformer.encoder.layer.4.intermediate.dense.weight         of shape (3072, 768)
2023-12-07 16:53:20,278.278 9392:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.output.LayerNorm.bias             will be loaded from textual.transformer.encoder.layer.4.output.LayerNorm.bias             of shape (768,)
2023-12-07 16:53:20,278.278 9392:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.output.LayerNorm.weight           will be loaded from textual.transformer.encoder.layer.4.output.LayerNorm.weight           of shape (768,)
2023-12-07 16:53:20,278.278 9392:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.output.dense.bias                 will be loaded from textual.transformer.encoder.layer.4.output.dense.bias                 of shape (768,)
2023-12-07 16:53:20,278.278 9392:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.4.output.dense.weight               will be loaded from textual.transformer.encoder.layer.4.output.dense.weight               of shape (768, 3072)
2023-12-07 16:53:20,278.278 9392:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.attention.output.LayerNorm.bias   will be loaded from textual.transformer.encoder.layer.5.attention.output.LayerNorm.bias   of shape (768,)
2023-12-07 16:53:20,278.278 9392:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.attention.output.LayerNorm.weight will be loaded from textual.transformer.encoder.layer.5.attention.output.LayerNorm.weight of shape (768,)
2023-12-07 16:53:20,278.278 9392:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.attention.output.dense.bias       will be loaded from textual.transformer.encoder.layer.5.attention.output.dense.bias       of shape (768,)
2023-12-07 16:53:20,278.278 9392:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.attention.output.dense.weight     will be loaded from textual.transformer.encoder.layer.5.attention.output.dense.weight     of shape (768, 768)
2023-12-07 16:53:20,278.278 9392:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.attention.self.key.bias           will be loaded from textual.transformer.encoder.layer.5.attention.self.key.bias           of shape (768,)
2023-12-07 16:53:20,278.278 9392:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.attention.self.key.weight         will be loaded from textual.transformer.encoder.layer.5.attention.self.key.weight         of shape (768, 768)
2023-12-07 16:53:20,278.278 9392:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.attention.self.query.bias         will be loaded from textual.transformer.encoder.layer.5.attention.self.query.bias         of shape (768,)
2023-12-07 16:53:20,278.278 9392:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.attention.self.query.weight       will be loaded from textual.transformer.encoder.layer.5.attention.self.query.weight       of shape (768, 768)
2023-12-07 16:53:20,278.278 9392:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.attention.self.value.bias         will be loaded from textual.transformer.encoder.layer.5.attention.self.value.bias         of shape (768,)
2023-12-07 16:53:20,279.279 9392:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.attention.self.value.weight       will be loaded from textual.transformer.encoder.layer.5.attention.self.value.weight       of shape (768, 768)
2023-12-07 16:53:20,279.279 9392:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.intermediate.dense.bias           will be loaded from textual.transformer.encoder.layer.5.intermediate.dense.bias           of shape (3072,)
2023-12-07 16:53:20,279.279 9392:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.intermediate.dense.weight         will be loaded from textual.transformer.encoder.layer.5.intermediate.dense.weight         of shape (3072, 768)
2023-12-07 16:53:20,279.279 9392:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.output.LayerNorm.bias             will be loaded from textual.transformer.encoder.layer.5.output.LayerNorm.bias             of shape (768,)
2023-12-07 16:53:20,279.279 9392:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.output.LayerNorm.weight           will be loaded from textual.transformer.encoder.layer.5.output.LayerNorm.weight           of shape (768,)
2023-12-07 16:53:20,279.279 9392:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.output.dense.bias                 will be loaded from textual.transformer.encoder.layer.5.output.dense.bias                 of shape (768,)
2023-12-07 16:53:20,279.279 9392:torch_common.py:128 align_and_update_state_dicts(): textual.transformer.encoder.layer.5.output.dense.weight               will be loaded from textual.transformer.encoder.layer.5.output.dense.weight               of shape (768, 3072)
2023-12-07 16:53:20,279.279 9392:torch_common.py:128 align_and_update_state_dicts(): textual.visual_projection.0.bias                                      will be loaded from textual.visual_projection.0.bias                                      of shape (768,)
2023-12-07 16:53:20,279.279 9392:torch_common.py:128 align_and_update_state_dicts(): textual.visual_projection.0.weight                                    will be loaded from textual.visual_projection.0.weight                                    of shape (768, 768)
2023-12-07 16:53:20,279.279 9392:torch_common.py:128 align_and_update_state_dicts(): textual.visual_projection.1.bias                                      will be loaded from textual.visual_projection.1.bias                                      of shape (768,)
2023-12-07 16:53:20,279.279 9392:torch_common.py:128 align_and_update_state_dicts(): textual.visual_projection.1.weight                                    will be loaded from textual.visual_projection.1.weight                                    of shape (768,)
2023-12-07 16:53:20,279.279 9392:torch_common.py:137 align_and_update_state_dicts(): target model param = 264; name matched = 264; loaded = 264
2023-12-07 16:53:20,279.279 9392:torch_common.py:140 align_and_update_state_dicts(): from loaded; ignore = []
2023-12-07 16:53:20,280.280 9392:torch_common.py:82 load_model_state_ignore_mismatch(): unique keys in init dict = ['textual.output.weight']; total = 1
2023-12-07 16:53:20,377.377 9392:torch_common.py:86 load_model_state_ignore_mismatch(): unique key (not initialized) in current model = ['textual.output.weight']
/home/paperspace/Desktop/GenerativeImage2Text/generativeimage2text/layers/decoder.py:1198: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  beam_id = idx // vocab_size
2023-12-07 16:53:29,265.265 9392:inference.py:109 test_git_inference_single_image(): output: water
]0;paperspace@psybffhvzv5t: ~/Desktop/GenerativeImage2Text[01;32mpaperspace@psybffhvzv5t[00m:[01;34m~/Desktop/GenerativeImage2Text[00m$ python -m generativeimage2text.train -p "{'type': 'forward_backward_example', \
> 'image_files': ['aux_data/images/1.jpg', 'aux_data/images/2.jpg'], \
> 'captions': ['a couple of boats in a large body of water.', 'a view of a mountain with a tree'], \
> }"
2023-12-07 16:53:54,537.537 9687:train.py:309   <module>(): param:
{'captions': ['a couple of boats in a large body of water.',
              'a view of a mountain with a tree'],
 'image_files': ['aux_data/images/1.jpg', 'aux_data/images/2.jpg'],
 'type': 'forward_backward_example'}
/home/paperspace/.local/lib/python3.9/site-packages/torchvision/transforms/transforms.py:891: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.
  warnings.warn(
2023-12-07 16:53:54,927.927 9687:train.py:234 forward_backward_example(): SelectTransform(ts=[Compose(
    ImageTransform2Dict(image_transform=Compose(
    RandomResizedCrop(size=(160, 160), scale=(0.8, 1.0), ratio=(1.0, 1.0), interpolation=bicubic)
    ToTensor()
    Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))
))
), Compose(
    ImageTransform2Dict(image_transform=Compose(
    RandomResizedCrop(size=(176, 176), scale=(0.8, 1.0), ratio=(1.0, 1.0), interpolation=bicubic)
    ToTensor()
    Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))
))
), Compose(
    ImageTransform2Dict(image_transform=Compose(
    RandomResizedCrop(size=(192, 192), scale=(0.8, 1.0), ratio=(1.0, 1.0), interpolation=bicubic)
    ToTensor()
    Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))
))
), Compose(
    ImageTransform2Dict(image_transform=Compose(
    RandomResizedCrop(size=(208, 208), scale=(0.8, 1.0), ratio=(1.0, 1.0), interpolation=bicubic)
    ToTensor()
    Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))
))
), Compose(
    ImageTransform2Dict(image_transform=Compose(
    RandomResizedCrop(size=(224, 224), scale=(0.8, 1.0), ratio=(1.0, 1.0), interpolation=bicubic)
    ToTensor()
    Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))
))
)], selector=<function get_multi_scale_image_transform.<locals>.<lambda> at 0x7fc2e3af7f70>)
/home/paperspace/.local/lib/python3.9/site-packages/torchvision/transforms/transforms.py:332: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.
  warnings.warn(
2023-12-07 16:54:01,967.967 9687:decoder.py:657    forward(): : iter=1, avg pos = 2.898025377362501e-05, max loss = 0, min loss = 0
2023-12-07 16:54:01,969.969 9687:decoder.py:962 forward_one_ce(): {'num_has_image': 0, 'num_no_image': 0}
2023-12-07 16:54:02,292.292 9687:train.py:244 forward_backward_example(): tensor(9.1845, device='cuda:0', grad_fn=<AddBackward0>)
]0;paperspace@psybffhvzv5t: ~/Desktop/GenerativeImage2Text[01;32mpaperspace@psybffhvzv5t[00m:[01;34m~/Desktop/GenerativeImage2Text[00m$ python -m generativeimage2text.train -p "{'type': 'forward_backward_example', \
> 'image_files': ['aux_data/images/1.jpg', 'aux_data/images/2.jpg'], \
> 'prefixs': ['what is this?', 'how many trees?'], \
> 'captions': ['several boats in a large body of water', '1'], \
> }"
2023-12-07 16:54:52,575.575 9951:train.py:309   <module>(): param:
{'captions': ['several boats in a large body of water', '1'],
 'image_files': ['aux_data/images/1.jpg', 'aux_data/images/2.jpg'],
 'prefixs': ['what is this?', 'how many trees?'],
 'type': 'forward_backward_example'}
/home/paperspace/.local/lib/python3.9/site-packages/torchvision/transforms/transforms.py:891: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.
  warnings.warn(
2023-12-07 16:54:52,958.958 9951:train.py:234 forward_backward_example(): SelectTransform(ts=[Compose(
    ImageTransform2Dict(image_transform=Compose(
    RandomResizedCrop(size=(160, 160), scale=(0.8, 1.0), ratio=(1.0, 1.0), interpolation=bicubic)
    ToTensor()
    Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))
))
), Compose(
    ImageTransform2Dict(image_transform=Compose(
    RandomResizedCrop(size=(176, 176), scale=(0.8, 1.0), ratio=(1.0, 1.0), interpolation=bicubic)
    ToTensor()
    Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))
))
), Compose(
    ImageTransform2Dict(image_transform=Compose(
    RandomResizedCrop(size=(192, 192), scale=(0.8, 1.0), ratio=(1.0, 1.0), interpolation=bicubic)
    ToTensor()
    Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))
))
), Compose(
    ImageTransform2Dict(image_transform=Compose(
    RandomResizedCrop(size=(208, 208), scale=(0.8, 1.0), ratio=(1.0, 1.0), interpolation=bicubic)
    ToTensor()
    Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))
))
), Compose(
    ImageTransform2Dict(image_transform=Compose(
    RandomResizedCrop(size=(224, 224), scale=(0.8, 1.0), ratio=(1.0, 1.0), interpolation=bicubic)
    ToTensor()
    Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))
))
)], selector=<function get_multi_scale_image_transform.<locals>.<lambda> at 0x7efd70b5a1f0>)
/home/paperspace/.local/lib/python3.9/site-packages/torchvision/transforms/transforms.py:332: UserWarning: